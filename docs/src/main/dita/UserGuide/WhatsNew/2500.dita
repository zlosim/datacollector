<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_ddx_bpm_pz">
 <title>What's New in 2.5.0.0</title>
 <conbody>
  <p><indexterm>what's new<indexterm>version 2.5.0.0</indexterm></indexterm><ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            version 2.5.0.0 includes the following new features and enhancements:</p>
        <dl>
            <dlentry>
                <dt>Multithreaded Pipelines</dt>
                <dd>
                    <p>The multithreaded framework includes the following enhancements:<ul
                            id="ul_gs3_3pm_pz">
                            <li><xref
                                    href="../Multithreaded_Pipelines/Origins.dita#concept_wcz_tpd_py"
                                    >Origins for multithreaded pipelines</xref> - You can now use
                                the following origins to create multithreaded pipelines:<ul
                                    id="ul_hx3_mpm_pz">
                                    <li>Elasticsearch origin</li>
                                    <li>JDBC Multitable Consumer origin</li>
                                    <li>Kinesis Consumer origin</li>
                                    <li>WebSocket Server origin</li>
                                </ul></li>
                            <li><xref
                                    href="../Multithreaded_Pipelines/Tuning.dita#concept_fmg_pjd_mz"
                                    >Maximum pipeline runners</xref> - You can now configure a
                                maximum number of pipeline runners to use in a pipeline. Previously,
                                    <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                /> generated pipeline runners based on the number of threads created
                                by the origin. This allows you to tune performance and resource
                                usage. By default, <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                /> still generates runners based on the number of threads that the
                                origin uses. </li>
                            <li><xref
                                    href="../Multithreaded_Pipelines/ProcessorCaching.dita#concept_np1_pkz_ry"
                                    >Record Deduplicator processor enhancement</xref> - The
                                processor can now deduplicate records across all pipeline runners in
                                a multithreaded pipeline.</li>
                            <li>Pipeline validation enhancement - The pipeline now displays
                                duplicate errors generated by using multiple threads as one error
                                message.</li>
                            <li>Log enhancement - Multithreaded pipelines now include the runner ID
                                in log information. </li>
                            <li><xref
                                    href="../Multithreaded_Pipelines/Monitoring.dita#concept_tdn_vwy_ry"
                                    >Monitoring</xref> - Monitoring now displays a histogram of
                                available pipeline runners, replacing the information previously
                                included in the Runtime Statistics list.</li>
                        </ul></p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Pipelines</dt>
                <dd>
                    <ul id="ul_asz_cqm_pz">
                        <li><ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
                            <xref href="../Configuration/Permissions.dita#concept_i1p_hzd_yy"
                                >pipeline permissions</xref> change - With this release, pipeline
                            permissions are no longer enabled by default. To enable pipeline
                            permissions, edit the <codeph>pipeline.access.control.enabled</codeph>
                            <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> configuration property.</li>
                        <li><xref href="../Executors/PipelineFinisher.dita#concept_qzm_l4r_kz">Stop
                                pipeline execution</xref> - You can configure pipelines to transfer
                            data and automatically stop execution based on an event such as reaching
                            the end of a table. The JDBC and Salesforce origins can generate events
                            when they reach the end of available data that the Pipeline Finisher
                            uses to stop the pipeline. Click <xref
                                href="../Event_Handling/CaseStudy-StopPipeline.dita#concept_kff_ykv_lz"
                                >here</xref> for a case study. </li>
                        <li><xref
                                href="../Pipeline_Configuration/RuntimeParameters.dita#concept_rjh_ntz_qr"
                                >Pipeline runtime parameters</xref> - You can now define runtime
                            parameters when you configure a pipeline, and then call the parameters
                            from within that pipeline. When you start the pipeline from the user
                            interface, the command line, or the REST API, you specify the values to
                            use for those parameters. Use pipeline parameters to represent any stage
                            or pipeline property with a value that must change for each pipeline run
                            - such as batch sizes and timeouts, directories, or URI.<p>In previous
                                versions, pipeline runtime parameters were named pipeline constants.
                                You defined the constant values in the pipeline, and could not pass
                                different values when you started the pipeline.</p></li>
                        <li><xref
                                href="../Pipeline_Configuration/ConfiguringAPipeline.dita#task_xlv_jdw_kq"
                                >Pipeline ID enhancement</xref> - <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> now prefixes the pipeline ID with the alphanumeric characters entered
                            for the pipeline title. For example, if you enter “Oracle To HDFS” as
                            the pipeline title, then the pipeline ID has the following value:
                            OracleToHDFStad9f592-5f02-4695-bb10-127b2e41561c.</li>
                        <li>Webhooks for pipeline state changes and alerts - You can now configure
                            pipeline state changes and metric and data alerts to call webhooks in
                            addition to sending email. For example, you can configure an incoming
                            webhook in Slack so that an alert can be posted to a Slack channel. Or,
                            you can configure a webhook to start another pipeline when the pipeline
                            state is changed to Finished or Stopped.</li>
                        <li><xref href="../Administration/CLI-Manager.dita#concept_msh_k2q_yt">Force
                                a pipeline to stop from the command line</xref> - If a pipeline
                            remains in a Stopping state, you can now use the command line to force
                            stop the pipeline immediately.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Stage Libraries</dt>
                <dd>
                    <p><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> now supports the Apache Kudu version 1.3.x. <xref
                            href="../Installation/AvailableStageLibraries.dita#concept_evs_xkm_s5"
                            >stage library</xref>.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Salesforce Stages</dt>
                <dd>
                    <p>The following Salesforce stages include several enhancements:<ul
                            id="ul_gsv_prm_pz">
                            <li><xref href="../Origins/Salesforce.dita#concept_odf_vr3_rx"
                                    >Salesforce origin</xref> and <xref
                                    href="../Processors/SalesforceLookup.dita#concept_k23_3rk_yx"
                                    >Salesforce Lookup processor</xref><ul id="ul_k1d_5rm_pz">
                                    <li>The origin and processor can use a proxy to connect to
                                        Salesforce.</li>
                                    <li>You can now specify <codeph>SELECT * FROM
                                            &lt;object></codeph> in a SOQL query. The origin or
                                        processor expands * to all fields in the Salesforce object
                                        that are accessible to the configured user. </li>
                                    <li>The origin and processor generate Salesforce field
                                        attributes that provide additional information about each
                                        field, such as the data type of the Salesforce field.</li>
                                    <li>The origin and processor can now additionally retrieve
                                        deleted records from the Salesforce recycle bin. </li>
                                    <li>The origin can now generate events when it completes
                                        processing all available data.</li>
                                </ul></li>
                            <li><xref href="../Destinations/Salesforce.dita#concept_rlb_rt3_rx"
                                    >Salesforce destination</xref> - The destination can now use a
                                CRUD operation record header attribute to indicate the operation to
                                perform for each record. You can also configure the destination to
                                use a proxy to connect to Salesforce. </li>
                            <li><xref href="../Destinations/WaveAnalytics.dita#concept_hlx_r53_rx"
                                    >Wave Analytics destination</xref> - You can now configure the
                                authentication endpoint and the API version that the destination
                                uses to connect to Salesforce Wave Analytics. You can also configure
                                the destination to use a proxy to connect to Salesforce.</li>
                        </ul></p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Origins</dt>
                <dd>
                    <ul id="ul_z25_bsm_pz">
                        <li><xref href="../Origins/Elasticsearch.dita#concept_f1q_vpm_2z">New
                                Elasticsearch origin</xref> - An origin that reads data from an
                            Elasticsearch cluster. The origin uses the Elasticsearch scroll API to
                            read documents using a user-defined Elasticsearch query. The origin
                            performs parallel processing and can generate multithreaded pipelines. </li>
                        <li><xref href="../Origins/MQTTSubscriber.dita#concept_ukz_3vt_lz">New MQTT
                                Subscriber origin</xref> - An origin that subscribes to a topic on
                            an MQTT broker to read messages from the broker.</li>
                        <li><xref href="../Origins/WebSocketServer.dita#concept_u2r_gpc_3z">New
                                WebSocket Server origin</xref> - An origin that listens on a
                            WebSocket endpoint and processes the contents of all authorized
                            WebSocket requests. The origin performs parallel processing and can
                            generate multithreaded pipelines. </li>
                        <li><xref href="../Pipeline_Design/DevStages.dita#concept_czx_ktn_ht">Dev
                                Data Generator origin enhancement</xref> - When you configure the
                            origin to generate events to test event handling functionality, you can
                            now specify the event type to use.</li>
                        <li><xref href="../Origins/HTTPClient.dita#concept_wk4_bjz_5r">HTTP Client
                                origin enhancements</xref> - When using pagination, the origin can
                            include all response fields in the resulting record in addition to the
                            fields in the specified result field path. The origin can now also
                            process the following new data formats: Binary, Delimited, Log, and SDC
                            Record.</li>
                        <li><xref
                                href="../Origins/HTTPServer-PrerequisitesAppID.dita#concept_thw_wtd_kz"
                                >HTTP Server origin enhancement</xref> - The origin requires that
                            HTTP clients include the application ID in all requests. You can now
                            configure HTTP clients to send data to a URL that includes the
                            application ID in a query parameter, rather than including the
                            application ID in request headers. </li>
                        <li><xref href="../Origins/MultiTableJDBCConsumer.dita#concept_zp3_wnw_4y"
                                >JDBC Multitable Consumer origin enhancements</xref> - The origin
                            now performs parallel processing and can generate multithreaded
                            pipelines. The origin can generate events when it completes processing
                            all available data. <p>You can also configure the quote character to use
                                around table, schema, and column names in the query. And you can
                                configure the number of times a thread tries to read a batch of data
                                after receiving an SQL error.</p></li>
                        <li><xref href="../Origins/JDBCConsumer.dita#concept_qhf_hjr_bs">JDBC Query
                                Consumer origin enhancements</xref> - The origin can now generate
                            events when it completes processing all available data, and when it
                            successfully completes or fails to complete a query. <p>To handle
                                transient connection or network errors, you can now specify how many
                                times the origin should retry a query before stopping the
                                pipeline.</p></li>
                        <li><xref href="../Origins/KConsumer.dita#concept_msz_wnr_5q">Kinesis
                                Consumer origin enhancement</xref> - The origin now performs
                            parallel processing and can generate multithreaded pipelines. </li>
                        <li><xref href="../Origins/MongoDB-Credentials.dita#concept_gzz_kdr_tz"
                                >MongoDB origin</xref> and <xref
                                href="../Origins/MongoDBOplog-Credentials.dita#concept_ovt_vpt_tz"
                                >MongoDB Oplog origin</xref> enhancements - The origins can now use
                            LDAP authentication in addition to username/password authentication to
                            connect to MongoDB. You can also now include credentials in the MongoDB
                            connection string.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Processors</dt>
                <dd>
                    <ul id="ul_apy_wsm_pz">
                        <li><xref href="../Processors/FieldOrder.dita#concept_krp_5fv_vy">New Field
                                Order processor</xref> - A processor that orders fields in a map or
                            list-map field and outputs the fields into a list-map or list root
                            field.</li>
                        <li>Field Flattener enhancement - You can now flatten a field in place to
                            raise it to the parent level.</li>
                        <li><xref href="../Processors/Groovy.dita#concept_ldh_sct_gv">Groovy</xref>,
                                <xref href="../Processors/JavaScript.dita#concept_n2p_jgf_lr"
                                >JavaScript</xref>, and <xref
                                href="../Processors/Jython.dita#concept_a1h_lkf_lr">Jython
                                Evaluator</xref> processor enhancement - You can now develop an
                            initialization script that the processor runs once when the pipeline
                            starts. Use an initialization script to set up connections or resources
                            required by the processor.<p>You can also develop a destroy script that
                                the processor runs once when the pipeline stops. Use a destroy
                                script to close any connections or resources opened by the
                                processor.</p></li>
                        <li><xref href="../Processors/JDBCLookup-Configuring.dita#task_kbr_2cy_hw"
                                >JDBC Lookup enhancement</xref> - Default value date formats. When
                            the default value data type is Date, use the following format:
                            yyyy/MM/dd . When the default value data type is Datetime, use the
                            following format: yyyy/MM/dd HH:mm:ss.</li>
                        <li><xref href="../Pipeline_Design/DevStages.dita#concept_czx_ktn_ht">Record
                                Deduplicator processor enhancement</xref> - The processor can now
                            deduplicate records across all pipeline runners in a multithreaded
                            pipeline.</li>
                        <li><xref href="../Processors/Spark.dita#concept_cpx_1lm_zx">Spark Evaluator
                                processor enhancements</xref> - The processor is now included in the
                            MapR 5.2 stage library. <p>The processor also now provides beta support
                                of cluster mode pipelines. In a development or test environment, you
                                can use the processor in pipelines that process data from a Kafka or
                                MapR cluster in cluster streaming mode. Do not use the Spark
                                Evaluator processor in cluster mode pipelines in a production
                                environment.</p></li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Destinations</dt>
                <dd>
                        <ul id="ul_x13_lym_pz">
                            <li><xref href="../Destinations/HTTPClient.dita#concept_khl_sg5_lz">New
                                    HTTP Client destination</xref> - A destination that writes to an
                                HTTP endpoint.</li>
                            <li><xref href="../Destinations/MQTTPublisher.dita#concept_odz_txt_lz"
                                    >New MQTT Publisher destination</xref> - A destination that
                                publishes messages to a topic on an MQTT broker.</li>
                            <li><xref href="../Destinations/WebSocketClient.dita#concept_l4d_mjn_lz"
                                    >New WebSocket Client destination</xref> - A destination that
                                writes to a WebSocket endpoint.</li>
                            <li><xref
                                    href="../Destinations/DataLakeStore-IdleTimeout.dita#concept_c2p_wzh_4z"
                                    >Azure Data Lake Store destination enhancement</xref> - You can
                                now configure an idle timeout for output files. </li>
                            <li>Cassandra destination enhancements - The destination now supports
                                the Cassandra uuid and timeuuid data types. And you can now specify
                                the Cassandra batch type to use: Logged or Unlogged. Previously, the
                                destination used the Logged batch type.</li>
                            <li><xref href="../Destinations/JDBCProducer.dita#concept_kvs_3hh_ht"
                                    >JDBC Producer enhancements</xref> - The origin now includes a
                                Schema Name property for entering the schema name. For information
                                about possible upgrade impact, see <xref
                                    href="../Upgrade/PostUpgrade-JDBCProducer.dita#concept_cmh_ryd_pz"
                                    >Configure JDBC Producer Schema Names</xref>.<p>You can also use
                                    the Enclose Object Name property to enclose the database/schema,
                                    table, and column names in quotation marks when writing to the
                                    database. </p></li>
                            <li><xref
                                href="../Destinations/MapRDBJSON-Configuring.dita#task_wq3_wkj_dy"
                                >MapR DB JSON destination enhancement</xref> - You can now enter an
                            expression that evaluates to the name of the MapR DB JSON table to write
                            to.</li>
                        <li><xref href="../Destinations/MongoDB-Credentials.dita#concept_ppl_3qt_tz"
                                >MongoDB destination enhancements</xref> - The destination can now
                            use LDAP authentication in addition to username/password authentication
                            to connect to MongoDB. You can also now include credentials in the
                            MongoDB connection string.</li>
                        <li><xref href="../Destinations/RPCdest-Configuring.dita#task_nbl_r2x_dt"
                                >SDC RPC destination enhancements</xref> - The Back Off Period value
                            that you enter now increases exponentially after each retry, until it
                            reaches the maximum wait time of 5 minutes. Previously, there was no
                            limit to the maximum wait time. The maximum value for the Retries per
                            Batch property is now unlimited - previously it was 10 retries.</li>
                        <li><xref href="../Destinations/Solr.dita#concept_z2g_q1r_wr">Solr
                                destination enhancement</xref> - You can now configure the action
                            that the destination takes when it encounters missing fields in the
                            record. The destination can discard the fields, send the record to
                            error, or stop the pipeline.</li>
                        </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Executors</dt>
                <dd>
                    <ul id="ul_ojc_3zm_pz">
                        <li><xref href="../Executors/Spark.dita#concept_cvy_vxb_1z">New Spark
                                executor</xref> - The executor starts a Spark application on a YARN
                            or Databricks cluster each time it receives an event.</li>
                        <li><xref href="../Executors/PipelineFinisher.dita#concept_qzm_l4r_kz">New
                                Pipeline Finisher executor</xref> - The executor stops the pipeline
                            and transitions it to a Finished state when it receives an event. Can be
                            used with the JDBC Query Consumer, JDBC Multitable Consumer, and
                            Salesforce origins to perform batch processing of available data.</li>
                        <li><xref href="../Executors/HDFSMetadata.dita#concept_wgj_slk_fx">HDFS File
                                Metadata executor enhancement</xref> - The executor can now create
                            an empty file upon receiving an event. The executor can also generate a
                            file-created event when generating events. </li>
                        <li><xref href="../Executors/MapReduce.dita#concept_bj2_zlk_fx">MapReduce
                                executor enhancement</xref> - When starting the provided Avro to
                            Parquet job, the executor can now overwrite any temporary files created
                            from a previous run of the job.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Functions</dt>
                <dd>
                    <ul id="ul_ayq_pzm_pz">
                        <li><xref
                                href="../Expression_Language/StringFunctions.dita#concept_ahp_f4v_1r"
                                >New escape XML functions</xref> - Three new string functions enable
                            you to escape and unescape XML.</li>
                        <li><xref
                                href="../Expression_Language/PipelineFunctions.dita#concept_dvg_nqn_wx"
                                >New pipeline user function</xref> - A new pipeline user function
                            enables you to determine the user who started the pipeline. </li>
                        <li><xref
                                href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                >New function to generate UUIDs</xref> - A new function that enables
                            you generate UUIDs.</li>
                        <li><xref
                                href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                >New function returns the number of available processors</xref> -
                            The runtime:availableProcessors() function returns the number of
                            processors available to the Java virtual machine.</li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>General Enhancements</dt>
                <dd>
                    <ul id="ul_ksn_tzm_pz">
                        <li><xref
                                href="../Configuration/HadoopImpersonationMode.dita#concept_pmr_sy5_nz"
                                >Data Collector Hadoop impersonation enhancement</xref> - You can
                            use the
                                <codeph>stage.conf_hadoop.always.impersonate.current.user</codeph>
                            <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> configuration property to ensure that <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> uses the current <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> user to read from or write to Hadoop systems. <p>When enabled, you
                                cannot configure alternate users in the following Hadoop-related
                                    stages:<ul id="ul_ztn_xzm_pz">
                                    <li>Hadoop FS origin and destination</li>
                                    <li>MapR FS origin and destination</li>
                                    <li>HBase lookup and destination</li>
                                    <li>MapR DB destination</li>
                                    <li>HDFS File Metadata executor</li>
                                    <li>Map Reduce executor</li>
                                </ul></p></li>
                        <li>Stage precondition property enhancement - Records that do not meet all
                            preconditions for a stage are now processed based on error handling
                            configured in the stage. Previously, they were processed based on error
                            handling configured for the pipeline. See <xref
                                href="../Upgrade/PostUpgrade-Preconditions.dita#concept_gk3_s5l_nz"
                                >Evaluate Precondition Error Handling</xref> for information about
                            upgrading.</li>
                        <li>XML parsing enhancements - You can include field XPath expressions and
                            namespaces in the record with the <xref
                                href="../Data_Formats/XMLDF-FieldXPathExp.dita#concept_w3k_1ch_qz"
                                >Include Field XPaths property</xref>. And use the new <xref
                                href="../Data_Formats/XMLDF-OutputFAttributes.dita#concept_jll_4wh_qz"
                                >Output Field Attributes property</xref> to write XML attributes and
                            namespace declarations to field attributes rather than including them in
                            the record as fields. </li>
                        <li><xref
                                href="../Getting_Started/ConfiguringConsoleSettings.dita#task_r3q_fnx_pr"
                                >Wrap long lines in properties</xref> - You can now configure <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> to wrap long lines of text that you enter in properties, instead of
                            displaying the text with a scroll bar.</li>
                    </ul>
                </dd>
            </dlentry>
        </dl>
 </conbody>
</concept>
