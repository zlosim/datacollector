<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_b1l_5lv_jz">
 <title>Cluster Pipelines</title>
    <shortdesc>You can use the Spark Evaluator processor in cluster pipelines that process data from
        a Kafka or MapR cluster in cluster streaming mode. You cannot use the processor in cluster
        pipelines that process data from a MapR cluster in cluster batch mode or from HDFS. </shortdesc>
 <conbody>
        <p><indexterm>Spark Evaluator processor<indexterm>cluster
            pipelines</indexterm></indexterm>In a cluster pipeline, the cluster manager spawns a <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            worker for each topic partition in the Kafka or MapR cluster. Spark Streaming generates
            a batch for the pipeline every few seconds. Then each <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            worker processes the batches from a single partition.</p>
        <p>Resilient Distributed Datasets (RDDs) are generated from data received across all of the
                <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
            /> workers. The RDDs are then passed to the Spark Transformer running on the driver.</p>
        <p>The RDD passed to the Spark Transformer points to data across the cluster. The Spark
            Transformer processes all of the data received across all <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            workers in the cluster, processing multiple batches at the same time. Data moves from
            one node to another only if your custom Spark code shuffles the data. The Spark
            Transformer returns the results and errors back to each Spark Evaluator processor
            running on the <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            workers. The <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            workers process the remaining pipeline stages in parallel.</p>
        <p>The following image shows how the Spark Transformer processes data received across all
            workers in the cluster:</p>
        <p><image href="../Graphics/SparkEvaluatorCluster.png" scale="65" id="image_swx_xwv_jz"
            /></p>
        <p>Use the following rules and guidelines when you write the Spark application and configure
            the processor for a cluster pipeline:<ul id="ul_dg5_nmv_jz">
                <li>Ensure that the Spark version you use to build the application matches the Spark
                    version used by the cluster and the Spark Evaluator stage library.</li>
                <li>To maintain the states of the RDDs that are used more than once in the
                    application, use the <codeph>RDD.checkpoint()</codeph> or
                        <codeph>RDD.cache()</codeph> method in your custom Spark application. When
                    you checkpoint or cache the RDDs, the Spark Transformer can access the RDDs from
                    previous batches.<p>The Spark Evaluator processor cleans up checkpoints when the
                        pipeline restarts or when the Spark application restarts. To maintain the
                        state of each RDD between pipeline runs or between restarts of the Spark
                        application, you must maintain the RDDs in an external store such as HDFS or
                        HBase.</p></li>
                <li>When you configure the processor, do not define a parallelism value for the
                    processor. The number of partitions defined in the Kafka Consumer origin
                    determines the number of partitions used throughout the cluster pipeline.</li>
                <li>When you configure the processor, use constant values for <codeph>init</codeph>
                    method arguments. In a cluster pipeline, the Spark Evaluator processor cannot
                    evaluate <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> expressions in <codeph>init</codeph> method arguments.</li>
                <li product="SDC">When you monitor the running pipeline, the record throughput for
                    intervals less than 15 minutes might show an inaccurate count. To accurately
                    monitor the throughput rate, evaluate the throughput for 15 minutes or
                    more.</li>
            </ul></p>
    </conbody>
    <related-links>
        <link href="../Cluster_Mode/ClusterPipelines.dita#concept_hmh_kfn_1s"/>
    </related-links>
</concept>
