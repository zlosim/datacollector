<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA Task//EN" "task.dtd">
<task id="task_cdw_wxb_1z">
    <title>Configuring a Spark Executor</title>
    <taskbody>
        <context>
            <p><indexterm>Spark executor<indexterm>configuring</indexterm></indexterm>Configure a
                Spark executor to start Spark applications each time the executor receives an event
                record. </p>
        </context>
        <steps>
            <step>
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_yxz_pvs_5x">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/D-1stStep-Name">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/D-1stStep-Desc">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/D-1stStep-Library">
                                    <entry/>
                                </row>
                                <row>
                                    <entry>Produce Events <xref href="Spark-EventGeneration.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_qqz_cxs_5x"/></xref></entry>
                                    <entry
                                        conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/D-1stStep-entry-Events"
                                    />
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/D-1stStep-ReqFields">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/D-1stStep-Precond">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="step-Sparktab">
                <cmd>On the <wintitle>Spark</wintitle> tab, configure the <uicontrol>Cluster
                        Manager</uicontrol> property.</cmd>
                <info>Then, when using a <uicontrol>Databricks</uicontrol> cluster manager,
                    configure the following property and continue to the next step.<table
                        frame="all" rowsep="1" colsep="1" id="table_p1f_ytl_gz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Databricks Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Cluster Base URL</entry>
                                    <entry>The Databricks URL for your company. The URL uses the
                                        following format:
                                        <codeblock>https://&lt;your domain>.cloud.databricks.com</codeblock></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
                <info>Then, when using a <uicontrol>YARN</uicontrol> cluster manager, configure the
                    following properties:<table frame="all" rowsep="1" colsep="1"
                        id="table_f4s_j5l_gz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>YARN Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Deploy Mode</entry>
                                    <entry>Deploy mode for the application:<ul id="ul_swj_n5l_gz">
                                            <li>Client - Runs the application in Spark client mode.
                                                Use only when resources are not a concern.</li>
                                            <li>Cluster - Runs the application in Spark cluster
                                                mode. Cluster mode deploys the application on the
                                                YARN cluster.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Driver Memory</entry>
                                    <entry>Maximum amount of memory the driver can use for the
                                            application.<p>Enter the number and a standard Java unit
                                            of measure without additional spaces. For example, 10m.
                                            </p><p>You can use k or K, m or M, or g or G.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Executor Memory</entry>
                                    <entry>Maximum amount of memory the executor can use. <p>Enter
                                            the number and a standard Java unit of measure without
                                            additional spaces. For example, 100k. </p><p>You can use
                                            k or K, m or M, or g or G. </p></entry>
                                </row>
                                <row>
                                    <entry>Dynamic Allocation</entry>
                                    <entry>Enables the dynamic allocation of executors to start an
                                        applications.</entry>
                                </row>
                                <row>
                                    <entry>Number of Worker Nodes</entry>
                                    <entry>The exact number of worker nodes for Spark to use.
                                        Configure when not using dynamic allocation.</entry>
                                </row>
                                <row>
                                    <entry>Minimum Number of Worker Nodes</entry>
                                    <entry>The minimum number of worker nodes for Spark to use.
                                        Configure when using dynamic allocation.</entry>
                                </row>
                                <row>
                                    <entry>Maximum Number of Worker Nodes</entry>
                                    <entry>The maximum number of worker nodes for Spark to use.
                                        Configure when using dynamic allocation.</entry>
                                </row>
                                <row>
                                    <entry>Proxy User <xref href="Spark-User.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_xqm_fjm_zx"
                                        /></xref></entry>
                                    <entry>Hadoop user to connect to the external system and run the
                                        application. When using this property, make sure the
                                        external system is configured appropriately. <p>By default,
                                            the pipeline uses the Data Collector user. </p></entry>
                                </row>
                                <row>
                                    <entry>Custom Spark Home <xref href="Spark-SparkHome.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_crp_srh_c1b"
                                        /></xref></entry>
                                    <entry>Use to enter a custom Spark home directory. By default,
                                        the origin uses the directory specified in the SPARK_HOME
                                        environment variable on the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> machine. <p>This property overrides the SPARK_HOME
                                            environment variable.</p><p>Required if the environment
                                            variable is not set for the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> machine or if the variable is set for an incorrect
                                            version of Spark.</p><p>For example, to run a job
                                            against Spark 2.1, point this property to the Spark 2.1
                                            directory if the SPARK_HOME environment variable points
                                            to an earlier version of Spark.</p></entry>
                                </row>
                                <row>
                                    <entry>Custom Java Home</entry>
                                    <entry>Use to enter a custom Java home directory. By default,
                                        the origin uses the directory specified in the JAVA_HOME
                                        environment variable on the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> machine. <p>This property overrides the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> environment variable.</p><p>Required if the
                                            environment variable is not set for the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> machine.</p></entry>
                                </row>
                                <row>
                                    <entry>Additional Spark Arguments</entry>
                                    <entry>Additional arguments to pass to Spark. Overrides any
                                        previous configuration for the specified arguments. For a
                                        list of available arguments, see the Spark documentation.
                                    </entry>
                                </row>
                                <row>
                                    <entry>Additional Spark Arguments and Values</entry>
                                    <entry>Additional arguments with values to pass to Spark.
                                        Overrides any previous configuration for the specified
                                        arguments. For a list of available arguments, see the Spark
                                        documentation.</entry>
                                </row>
                                <row>
                                    <entry>Environment Variables</entry>
                                    <entry>Additional environment variables to use. Overrides any
                                        previous configuration for the specified arguments. For a
                                        list of valid environment variables, see the Spark
                                        documentation.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step>
                <cmd>When using a Databricks cluster manager, click the
                        <wintitle>Application</wintitle> tab and configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_uvj_jxm_gz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Databricks Application Details Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Job Type</entry>
                                    <entry>Job type to run: Notebook or JAR.</entry>
                                </row>
                                <row>
                                    <entry>Job ID <xref href="Spark-Databricks-Prereq.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_tw4_jn2_kz"/></xref></entry>
                                    <entry>Job ID generated by Databricks after the job was
                                        submitted.</entry>
                                </row>
                                <row>
                                    <entry>Parameters</entry>
                                    <entry>Parameters to pass to the job. Enter the parameters
                                        exactly as expected, and in the expected order. The executor
                                        does not validate the parameters.<p>You can use the
                                            expression language in job parameters. For example, when
                                            performing post-processing on an Amazon S3 object, you
                                            can use the following expression to retrieve the object
                                            key name from the event record:
                                            <codeblock><codeph>${record:field('/objectKey')}</codeph></codeblock></p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When using a YARN cluster manager, click the <wintitle>Application
                        Details</wintitle> tab, select the <uicontrol>Language</uicontrol> used to
                    write the application, and then configure the following properties:</cmd>
                <info>For applications written in Java or Scala, configure the following
                        properties:<draft-comment author="Loretta">doc note: Make sure any changes
                        to this table are replicated as needed in the Python table
                        below.</draft-comment><table frame="all" rowsep="1" colsep="1"
                        id="table_udj_yyx_jz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Java/Scala Application Properties</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Application Name</entry>
                                    <entry>Name to display in YARN resource manager and logs. Also
                                        displays in Spark server history pages.<p
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/SparkAppName-Tip"
                                        /></entry>
                                </row>
                                <row>
                                    <entry>Application Resource</entry>
                                    <entry>The full path to the JAR that contains the main
                                        class.</entry>
                                </row>
                                <row>
                                    <entry>Main Class</entry>
                                    <entry>The full path to the main class for the Spark
                                        application. </entry>
                                </row>
                                <row>
                                    <entry>Application Arguments</entry>
                                    <entry>You can add additional arguments to pass to the
                                        application. <p>Enter the arguments exactly as expected, and
                                            in the expected order. The executor does not validate
                                            the arguments.</p></entry>
                                </row>
                                <row>
                                    <entry>Additional JARs</entry>
                                    <entry>You can specify additional JARs to use. Enter the full
                                        path to the JAR.</entry>
                                </row>
                                <row>
                                    <entry>Additional Files</entry>
                                    <entry>Additional files to pass to the application using the
                                            <codeph>--files</codeph> protocol. Enter the full path
                                        to the files. <p>For information about the protocol, see the
                                            Spark documentation.</p></entry>
                                </row>
                                <row>
                                    <entry>Enable Verbose Logging</entry>
                                    <entry>Enables logging additional information to the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> log. <p>To avoid filling the log with unnecessary
                                            information, enable this property only when testing the
                                            pipeline. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
                <info>For applications written in Python, configure the following
                        properties:<draft-comment author="Loretta">doc note: Make sure any changes
                        to this table are replicated as needed in the Java/Scala table
                        above.</draft-comment><table frame="all" rowsep="1" colsep="1"
                        id="table_uzz_qr2_kz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Python Application Properties </entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Application Name</entry>
                                    <entry>Name to display in YARN resource manager and logs. Also
                                        displays in Spark server history pages.<p
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/SparkAppName-Tip"
                                        /></entry>
                                </row>
                                <row>
                                    <entry>Application Resource</entry>
                                    <entry>The full path to the Python file to run. </entry>
                                </row>
                                <row>
                                    <entry>Application Arguments</entry>
                                    <entry>You can add additional arguments to pass to the
                                        application. <p>Enter the arguments exactly as expected, and
                                            in the expected order. The executor does not validate
                                            the arguments.</p></entry>
                                </row>
                                <row>
                                    <entry>Dependencies</entry>
                                    <entry>Full path to any files the Python application resource
                                        requires. </entry>
                                </row>
                                <row>
                                    <entry>Additional Files</entry>
                                    <entry>Additional files to pass to the application using the
                                            <codeph>--files</codeph> protocol. Enter the full path
                                        to the files. <p>For information about the protocol, see the
                                            Spark documentation.</p></entry>
                                </row>
                                <row>
                                    <entry>Enable Verbose Logging</entry>
                                    <entry>Enables logging additional information to the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> log. <p>To avoid filling the log with unnecessary
                                            information, enable this property only when testing the
                                            pipeline. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step>
                <cmd>Optionally, click the <wintitle>Credentials</wintitle> tab and configure the
                    following properties:</cmd>
                <info>To enter Databricks credentials, configure the following properties:<table
                        frame="all" rowsep="1" colsep="1" id="table_e4n_jyj_kz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Databricks Credentials</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Username</entry>
                                    <entry>Databricks user name.</entry>
                                </row>
                                <row>
                                    <entry>Password</entry>
                                    <entry>Password for the account.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
                <info>To use Kerberos authentication to access a destination system from YARN,
                    configure the following properties:<table frame="all" rowsep="1" colsep="1"
                        id="table_hxn_syj_kz">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>YARN Kerberos Properties <xref href="Spark-Kerberos.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_hlh_jzj_kz"/></xref></entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Kerberos Principal</entry>
                                    <entry>Kerberos principal for the YARN cluster where the
                                        application runs. </entry>
                                </row>
                                <row>
                                    <entry>Kerberos Keytab</entry>
                                    <entry>Kerberos keytab for the YARN cluster where the
                                        application runs. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step>
                <cmd>To use SSL/TLS with Databricks, click the <wintitle>SSL</wintitle> tab and
                    configure the following properties:</cmd>
                <info>
                    <note
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Passwords"/>
                    <table
                        conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/SSLTLS_table"
                        id="table_b5p_p1k_kz">
                        <tgroup cols="1">
                            <tbody>
                                <row>
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>To use an HTTP proxy, on the <wintitle>Proxy</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table
                        conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/HTTPProxy-table"
                        id="table_vy5_dbk_kz">
                        <tgroup cols="1">
                            <tbody>
                                <row>
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
        </steps>
    </taskbody>
</task>
