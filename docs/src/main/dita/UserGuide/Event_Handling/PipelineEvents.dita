<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_amg_2qr_t1b">
    <title>Pipeline Event Generation</title>
    <conbody>
        <p><indexterm>pipelines<indexterm>event generation</indexterm></indexterm><indexterm>event
                    framework<indexterm>pipeline event generation</indexterm></indexterm>The event
            framework generates pipeline events at specific points in the pipeline lifecycle. You
            can configure the pipeline properties to pass each event to an executor or to another
            pipeline for more complex processing.</p>
        <p>The event framework generates the following pipeline-related events:<dl>
                <dlentry>
                    <dt>Pipeline Start</dt>
                    <dd>The pipeline start event is generated <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ph-StartEvent"
                        />. This can allow time for an executor to perform a task before stages
                        initialize. </dd>
                    <dd>Most executors wait for confirmation that a task completes. As a result, the
                        pipeline waits for the executor to complete the task before continuing with
                        stage initialization. For example, if you configure the JDBC Query executor
                        to truncate a table before the pipeline begins, the pipeline waits until the
                        task is complete before processing any data. </dd>
                    <dd>
                        <p>The MapReduce executor and Spark executor kick off jobs and do not wait
                            for the submitted jobs to complete. When you use one of these executors,
                            the pipeline waits only for successful job submission before continuing
                            with stage initialization.</p>
                        <p>If the executor fails to process the event, for example if a Hive Query
                            Executor fails to execute the specified query or if the query fails,
                            then the initialization phase fails and the pipeline does not start.
                            Instead the pipeline transitions to a failure state.</p>
                    </dd>
                </dlentry>
                <dlentry>
                    <dt>Pipeline Stop</dt>
                    <dd>The pipeline stop event is generated <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/phStopEvent"
                        /> This allows an executor to perform a task after pipeline processing is
                        complete, before the pipeline fully stops.</dd>
                    <dd>
                        <p>Similar to start event consumers, the behavior of the executor that
                            consumes the event determines whether the pipeline waits for the
                            executor task to complete before allowing the pipeline to stop. Also, if
                            the processing of the pipeline stop event fails for any reason, the
                            pipeline transitions to a failed state even though the data processing
                            was successful.</p>
                    </dd>
                </dlentry>
            </dl></p>
        <p>Pipeline events differ from stage events as follows:<ul id="ul_n1m_k5r_t1b">
                <li><b>Virtual processing</b> - Unlike stage events, pipeline events are not
                    processed by stages that you configure in the canvas. They are passed to an
                    event consumer that you configure in the pipeline properties.<p>The event
                        consumer does not display in the pipelineâ€™s canvas. As a result, pipeline
                        events are also not visualized in data preview<ph product="SDC"> or pipeline
                            monitoring</ph>. </p></li>
                <li><b>Single-use events</b> - You can configure only one event consumer for each
                    event type within the pipeline properties: one for the Start event and one for
                    the Stop event. <p>When necessary, you can pass pipeline events to another
                        pipeline. In the event consuming pipeline, you can include as many stages as
                        you need for more complex processing. </p></li>
            </ul></p>
        <p>For a case study that describes a couple ways to use pipeline events, see <xref
                href="CaseStudy-SqoopReplacement.dita#concept_vrh_jrs_bbb"/>.</p>
    </conbody>
</concept>
