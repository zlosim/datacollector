<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_szz_xwm_lx">
    <title>Case Study: Impala Metadata Updates for DDS for Hive</title>
    <conbody>
        <p><indexterm>dataflow trigger case study<indexterm>Drift Synchronization Solution for Hive
                    with Impala</indexterm></indexterm><indexterm>Hive Query executor<indexterm>case
                    study</indexterm></indexterm>You love the <xref
                href="../Hive_Drift_Solution/HiveDrift-Overview.dita#concept_phk_bdf_2w">Drift
                Synchronization Solution for Hive</xref> because it automatically updates the Hive
            metastore when needed. But if you've been using it with Impala, you've been trying to
            time the Invalidate Metadata command after each metadata change and file write.</p>
        <p>Instead of running the command manually, you use the event framework in your Drift
            Synchronization Solution for Hive pipeline to execute the command automatically. </p>
        <p>Enable both the Hive Metastore destination and the Hadoop FS destination to generate
            events. You can connect both event streams to a single Hive Query executor. The executor
            then runs the Invalidate Metadata command each time the Hive Metastore destination
            changes the Hive metastore and each time Hadoop FS writes a file to a Hive table.</p>
        <p>Here's how it works:</p>
        <p>The following Drift Synchronization Solution for Hive pipeline reads files from a
            directory. The Hive Metadata processor evaluates the data for structural changes. It
            passes data to Hadoop FS and metadata records to the Hive Metastore destination. Hive
            Metastore creates and updates tables in Hive based on the metadata records it
            receives:</p>
        <p><image href="../Graphics/Event-HDS-BasicPipe.png" id="image_lz5_414_lx" scale="75"/></p>
        <p>
            <ol id="ol_mtf_tzn_lx">
                <li>Configure the Hive Metastore destination to generate events.<p>On the
                            <wintitle>General</wintitle> tab, select the <uicontrol>Produce
                            Events</uicontrol> property.</p><p>Now, the event output stream becomes
                        available, and Hive Metastore destination generates an event record every
                        time it updates the Hive metastore. The event record contains the name of
                        the table that was created or updated.</p><p><image
                            href="../Graphics/Event-HDS-HMetastore.png" id="image_cht_bc4_lx"
                            scale="75"/></p></li>
                <li>We also need to add an event stream to the Hadoop FS destination so we can run
                    the Invalidate Metadata command each time the destination writes a file to Hive.
                    So in the Hadoop FS destination, on the <wintitle>General</wintitle> tab, select
                        <uicontrol>Produce Events</uicontrol>.<p>With this property selected the
                        event output stream becomes available, and Hadoop FS generates an event
                        record every time it closes a file:</p><p><image
                            href="../Graphics/Event-HDS-HDFS.png" id="image_c35_qns_5x" scale="60"
                        /></p></li>
                <li>The event record generated by the Hadoop FS destination does not include the
                    table name required by the Hive Query executor, but it contains the table name
                    in the file path. So add an Expression Evaluator processor to the event stream.
                    Create a new Table field and use the following
                        expression:<codeblock conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/code-HiveDrift-DB.table"/><p>This
                        expression uses the path in the Filepath field of the event record and
                        performs the following calculations:<ul id="ul_umt_2sm_5x">
                            <li>Extracts the third-to-last section of the path and uses it as the
                                database name. </li>
                            <li>Extracts the second-to-last section of the path and uses it as the
                                table name.</li>
                        </ul></p><p>So when Hadoop FS completes a file, it writes the path of the
                        written file in the filepath field, such as users/logs/server1weblog.txt.
                        And the expression above properly interprets the database and table name as:
                        logs.server1weblog.</p><p><image href="../Graphics/Event-HDS-Expression.png"
                            id="image_pcc_3zm_5x" scale="55"/></p></li>
                <li>Add the Hive Query executor and connect the Hive Metastore destination and the
                    Expression Evaluator to the executor. Then configure the Hive Query executor.<p>
                        <note>If you want to use an Impala JDBC driver, make sure to install the
                            driver as an  external library for the Hive Query executor. For more
                            information, see <xref
                                href="../Executors/HiveQuery-InstallImpalaDriver.dita#concept_rfq_xk4_nbb"
                            />. When using the Apache Hive JDBC driver included with <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            />, no additional steps are required. </note>
                    </p><p>In the Hive Query executor, configure the Hive configuration details on
                        the <wintitle>Hive</wintitle> tab. If you have any trouble configuring the
                        URL, see the Impala driver information in our <xref
                            href="https://ask.streamsets.com/question/7/how-do-you-configure-a-hive-impala-jdbc-driver-for-data-collector/"
                            format="html" scope="external">Ask StreamSets post</xref>. </p><p>Then,
                        on the <wintitle>Query</wintitle> tab, enter the following
                        query:<codeblock>invalidate metadata ${record:value('/table')}</codeblock></p><p>This
                        query refreshes the Impala cache for the specified table. And the table is
                        either the table in the Hive Metastore event record that was just updated or
                        the table where Hadoop FS wrote a file. </p><p>Here's the final
                        pipeline:</p><p><image href="../Graphics/Event-HDS-HiveQueryDeets.png"
                            id="image_ccr_hpr_mx" scale="70"/></p><p>With these new event streams,
                        each time the Hive Metastore destination creates a table, partition or
                        column, and each time the Hadoop FS destination completes writing a file,
                        the destinations generate event records. When the Hive Query executor
                        receives an event record, it runs the Invalidate Metadata command so Impala
                        can update its cache with the new information. Done!</p></li>
            </ol>
        </p>
    </conbody>
</concept>
