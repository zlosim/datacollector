<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_wfr_rnw_yq">
 <title>Reusable Tables of Information</title>
 <shortdesc/>
 <conbody>
  <p>
   <draft-comment author="Loretta">rows from the following table are used in the Data Collector UI
    topics (getting started, all pipelines, monitoring, data preview). The whole table used by the
    configuring topic.</draft-comment>
  </p>
  <p>
   <simpletable frame="all" id="table-configUI" relcolwidth="1.0* 1.62* 5.08*">
    <sthead>
     <stentry>Area / Icon</stentry>
     <stentry>Name</stentry>
     <stentry>Description</stentry>
    </sthead>
    <strow>
     <stentry>1</stentry>
     <stentry>Pipeline canvas</stentry>
     <stentry>Displays the pipeline. Use to configure the pipeline data flow.</stentry>
    </strow>
    <strow>
     <stentry>2</stentry>
     <stentry>Pipeline Creation Help Bar</stentry>
     <stentry>Offers lists of stages to help complete the pipeline. <p>You can use the help bar to
       connect a stage to an open node. You can also add a stage between linked stages by clicking
       the link. </p></stentry>
    </strow>
    <strow>
     <stentry>3</stentry>
     <stentry>Stage list</stentry>
     <stentry>Lists the stages in the pipeline. Use to select a stage to configure. You can also
      select the stage in the canvas.</stentry>
    </strow>
    <strow>
     <stentry>4</stentry>
     <stentry>Properties panel</stentry>
     <stentry>Displays the properties of the pipeline or selected stage when you configure a
      pipeline. </stentry>
    </strow>
    <strow>
     <stentry>5</stentry>
     <stentry>Stage library</stentry>
     <stentry>List of available stages. Use to add stages to the pipeline. You can drag a stage to a
      location on canvas or click a stage to add it to the end of the pipeline. <p>You can view all
       stages, stages by type, or stages by library. You can also search for a stage by
      name.</p></stentry>
    </strow>
    <strow id="icon-DPM">
     <stentry><image href="../Graphics/icon_DPM.png" scale="70" id="image_vk5_ryk_nx"/></stentry>
     <stentry>Dataflow Performance Manager icon</stentry>
     <stentry>Provides information about StreamSets <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-LongOnly"/> (<ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/>) and lets
      you register this <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> with <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/>.</stentry>
    </strow>
    <strow id="icon-Home">
     <stentry><image href="../Graphics/icon_OverHome.png" id="image_lw2_h23_ts"/></stentry>
     <stentry>Home icon</stentry>
     <stentry>Displays a home page with a list of pipelines and their statuses, allowing you to
      perform pipeline maintenance and navigate to individual pipelines.</stentry>
    </strow>
    <strow id="icon-PackageManager">
     <stentry><image href="../Graphics/icon_PackageManager.png" scale="80" id="image_o51_szk_nx"
      /></stentry>
     <stentry>Package Manager icon</stentry>
     <stentry>Displays the Package Manager which allows you to install additional stage libraries
      for a core <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
      /> installation.</stentry>
    </strow>
    <strow id="icon-Notifications">
     <stentry><image href="../Graphics/icon_MonAlert.png" id="image_mfs_2xd_zs"/></stentry>
     <stentry>Notifications icon</stentry>
     <stentry>Displays notifications.</stentry>
    </strow>
    <strow id="icon-Administration">
     <stentry><image href="../Graphics/icon_OverAdministration.png" id="image_gqn_5yd_zs"
      /></stentry>
     <stentry>Administration icon</stentry>
     <stentry>Provides access to <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
      configuration properties, directories, and log. Also allows you to shut down <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>.</stentry>
    </strow>
    <strow id="icon-User">
     <stentry><image href="../Graphics/icon_OverUser.png" id="image_gvc_kzd_zs"/></stentry>
     <stentry>User icon</stentry>
     <stentry>Displays the active user and the roles assigned to the user. Also allows you to log
      out of <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
      />.</stentry>
    </strow>
    <strow id="icon-Help">
     <stentry><image href="../Graphics/icon_OverCHelp.png" id="image_bkz_wk3_ts"/></stentry>
     <stentry>Help icon</stentry>
     <stentry>Provides context-sensitive help based on the information in the panel. Allows you to
      configure display settings and to specify whether to use a local or hosted version of the
       help.<p>Provides access to the REST API and the <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
       version.</p></stentry>
    </strow>
    <strow id="link-Pipelines">
     <stentry><image href="../Graphics/icon_PipelineList.png" id="image_hvv_4tc_hbb" scale="70"
      /></stentry>
     <stentry>Link to a pipeline list</stentry>
     <stentry>Link to a <xref
       href="../Getting_Started/DataCollectorUI_Home-Pipelines.dita#concept_wc5_ysp_1w">pipeline
       list on the Home page</xref>. Use to view a list of available pipelines, perform pipeline
      maintenance like starting or sharing a pipeline, and navigate to individual pipelines. </stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigMore.png" id="image_prb_4l3_ts"/></stentry>
     <stentry>More icon</stentry>
     <stentry>Provides additional actions for the pipeline. Use to reset the origin for the
      pipeline. Or to view or delete snapshot data.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_Issues.png" id="image_t1h_rl3_ts" scale="80"/></stentry>
     <stentry>Issues icon</stentry>
     <stentry>Displays the number of implicit validation issues in the pipeline. Click to view a
      detailed list of issues found during implicit validation.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon-ViewLogs.png" id="image_s3t_flt_3bb" scale="80"
      /></stentry>
     <stentry>View Log icon</stentry>
     <stentry>Displays the <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> log. The
      equivalent to selecting Administration > Logs.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigUndo.png" id="image_dqr_tl3_ts"/></stentry>
     <stentry>Undo icon</stentry>
     <stentry>Reverts recent changes. On Mac, you can also use Command+Z. On Windows, you can use
      Ctrl+Z. </stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigRedo.png" id="image_pzj_vl3_ts"/></stentry>
     <stentry>Redo icon</stentry>
     <stentry>Restores changes that were reverted. On Mac, you can also use Command+Shift+Z. On
      Windows, you can use Ctrl+Y. </stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigDelete.png" id="image_lqt_wl3_ts"/></stentry>
     <stentry>Delete icon</stentry>
     <stentry>Deletes the selected stage or link.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigCopy.png" id="image_nd1_zl3_ts"/></stentry>
     <stentry>Duplicate Stage icon</stentry>
     <stentry>Duplicates the selected stage. You can duplicate processors and destinations. Because
      a pipeline can include only one origin, you cannot duplicate an origin stage.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigArrange.png" id="image_e25_vm3_ts"/></stentry>
     <stentry>Auto Arrange icon</stentry>
     <stentry>Arranges the stages in the pipeline.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigDataPreview.png" id="image_t1h_xm3_ts"/></stentry>
     <stentry>Preview icon</stentry>
     <stentry>Starts a data preview. </stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigValidate.png" id="image_e3d_mn3_ts"/></stentry>
     <stentry>Validate icon</stentry>
     <stentry>Validates the pipeline. Performs explicit validation.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon-Share.png" id="image_eqt_wbv_xy" scale="90"/></stentry>
     <stentry>Share icon</stentry>
     <stentry>Shares the pipeline with users and groups. Use to configure pipeline
      permissions.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigStartPipeline.png" id="image_kyk_4n3_ts"
      /></stentry>
     <stentry>Start icon</stentry>
     <stentry>Starts the pipeline.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigStageLibrary.png" id="image_hbw_pn3_ts"
      /></stentry>
     <stentry>Stage Library icon</stentry>
     <stentry>Toggles the display of the stage library.</stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigLink.png" scale="85" id="image_ctp_sn3_ts"
      /></stentry>
     <stentry>Stream Link icon</stentry>
     <stentry>Indicates the flow of data through the pipeline. Select to configure data rules and
      alerts. </stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigStageIssue.png" id="image_ndf_5n3_ts"/></stentry>
     <stentry>Error icon</stentry>
     <stentry>Indicates that one or more required properties are not defined. Can display on a stage
      for stage properties or in the canvas for pipeline properties. <p>Related error messages
       display when you hover over the icon. You can also view the messages in the Issues
       list.</p><p>The icon can also display on tabs in the properties panel to indicate the
       location of missing properties. </p></stentry>
    </strow>
    <strow>
     <stentry><image href="../Graphics/icon_ConfigMaximize.png" id="image_lfb_l43_ts"/></stentry>
     <stentry>Maximize Pane icon</stentry>
     <stentry>Maximizes the configuration panel. </stentry>
    </strow>
   </simpletable>
  </p>
  <draft-comment author="Loretta">Data Collector configs > The info in these tables used in
   Configuring Data Collector and Shell Impersonation &amp; Security</draft-comment>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_nvx_45v_vz">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.0*"/>
     <colspec colname="c2" colnum="2" colwidth="1.0*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row>
       <entry/>
       <entry/>
      </row>
      <row>
       <entry>
        <p><ph>stage.conf_com.streamsets.pipeline.stage.executor.shell.impersonation_mode</ph></p>
       </entry>
       <entry><ph>Enforces the use of the current logged in user to execute scripts. </ph></entry>
      </row>
      <row>
       <entry><ph>stage.conf_com.streamsets.pipeline.stage.executor.shell.shell</ph></entry>
       <entry><ph>Defines the relative or absolute path to the command line interpreter to use to
         execute scripts. </ph><p>Default is <codeph>sh</codeph>, typically a symlink. </p></entry>
      </row>
      <row>
       <entry><ph>stage.conf_com.streamsets.pipeline.stage.executor.shell.sudo</ph></entry>
       <entry><ph>Defines the relative or absolute path to the sudo to use when executing scripts.
         Use to control the commands that can be executed in scripts.</ph></entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">table used by Rules/Alerts > Configuring a Webhook</draft-comment>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="WebhookTable">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Webhook Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry id="WH-Webhook-entry">Webhooks <xref href="../Pipeline_Configuration/Webhooks.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_cr2_33t_rz"
        /></xref></entry>
       <entry>Webhook to send when an alert triggers. Using <xref
         href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb">simple or bulk
         edit mode</xref>, click the <uicontrol>Add</uicontrol> icon to add additional
        webhooks.</entry>
      </row>
      <row id="WH-URL-row">
       <entry>Webhook URL</entry>
       <entry>URL to send the HTTP request.</entry>
      </row>
      <row id="WH-Header-row">
       <entry>Headers</entry>
       <entry>Optional HTTP request headers. </entry>
      </row>
      <row id="WH-HTTPMethod-row">
       <entry>HTTP Method</entry>
       <entry>HTTP method. Use one of the following methods: <ul id="ul_dr2_33t_rz">
         <li>GET</li>
         <li>PUT</li>
         <li>POST</li>
         <li>DELETE</li>
         <li>HEAD</li>
        </ul></entry>
      </row>
      <row>
       <entry id="WH-Payload-entry">Payload <xref
         href="../Pipeline_Configuration/Webhooks-Payload_Parameters.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_er2_33t_rz"
        /></xref></entry>
       <entry>Optional payload to include. Available for PUT, POST, and DELETE methods. <p>Use any
         valid content type.</p><p>You can use webhook parameters in the payload to include
         information about the triggering event, such as the alert name or condition. Enclose
         webhook parameters in double curly brackets as follows:
         <codeph>{{ALERT_NAME}}</codeph>.</p></entry>
      </row>
      <row id="WH-ContentType-row">
       <entry>Content Type</entry>
       <entry>Optional content type of the payload. Configure this property when the content type is
        not declared in the request headers.</entry>
      </row>
      <row id="WH-AuthenticationType-row">
       <entry>Authentication Type</entry>
       <entry>Optional authentication type to include in the request. Use None, Basic, Digest, or
         Universal.<p>Use Basic for Form authentication.</p></entry>
      </row>
      <row id="WH-UserName-row">
       <entry>User Name </entry>
       <entry>User name to include when using authentication. </entry>
      </row>
      <row id="WH-Password-row">
       <entry>Password</entry>
       <entry>Password to include when using authentication.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">The following table is in Error Record Selection and the rows in
    the Internal Attributes table.</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_cdy_fdg_n1b">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row>
       <entry/>
       <entry/>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">The following IgnoreControlChar-row is used in Configuring a -
    Directory, File Tail, Kafka Consumer, Kinesis Consumer, JSON Parser, Log
    Parser:]</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_mxl_xrm_js">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row id="IgnoreControlChars-row">
       <entry>Ignore Ctrl Characters <xref href="../Pipeline_Design/ControlCharacters.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
          id="image_xwx_xrm_js"/></xref></entry>
       <entry>Removes all ASCII control characters except for the tab, line feed, and carriage
        return characters.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">Origin - first step rows for event handling
    origins</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_f5l_np2_px">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>General Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="O-1stStep-Name">
       <entry>Name</entry>
       <entry>Stage name.</entry>
      </row>
      <row id="O-1stStep-Desc">
       <entry>Description</entry>
       <entry>Optional description.</entry>
      </row>
      <row>
       <entry>Produce Events </entry>
       <entry id="O-1stStep-entry-Events">Generates event records when events occur. Use for event
        handling. <xref href="../Event_Handling/EventFramework-Overview.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_plp_tp2_px"
        /></xref></entry>
      </row>
      <row id="O-1stStep-Error">
       <entry>On Record Error <xref href="../Pipeline_Design/ErrorHandling-Stage.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_g5l_np2_px"
        /></xref></entry>
       <entry>Error record handling for the stage: <ul id="ul_h5l_np2_px">
         <li>Discard - Discards the record.</li>
         <li>Send to Error - Sends the record to the pipeline for error handling.</li>
         <li>Stop Pipeline - Stops the pipeline. </li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">ORIGIN rows - <uicontrol>ProduceSingleRec</uicontrol> is used by
    Kafka Consumer and MapR Streams Consumer. <uicontrol>MaxBatchSize</uicontrol> and
     <uicontrol>BatchWaitTime</uicontrol> rows are used in Configuring Kafka Consumer, JMS Consumer,
    Salesforce origin, Salesforce Lookup. See if they should go anywhere else.</draft-comment>
   <draft-comment author="Loretta">Charsets are used by message and file origins. But Directory
    Charset is standalone. </draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_tft_4jk_dt">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry/>
       <entry/>
      </row>
      <row id="ProduceSingleRec">
       <entry>Produce Single Record</entry>
       <entry>For each partition, generates a single record for records that include multiple
        objects. <p>When not selected, the origin generates multiple records when a record includes
         multiple objects.</p></entry>
      </row>
      <row id="MaxBatchSize">
       <entry>Max Batch Size (records)</entry>
       <entry>Maximum number of records processed at one time. Honors values up to the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> maximum
        batch size. <p>Default is 1000. The <ph
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> default
         is 1000.</p></entry>
      </row>
      <row id="BatchWaitTime">
       <entry>Batch Wait Time (ms) <xref href="../Origins/BatchSizeWaitTime.dita#concept_ypd_vgr_5q">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_mgp_2q3_br"
          placement="inline"/></xref></entry>
       <entry>Number of milliseconds to wait before sending a partial or empty batch. </entry>
      </row>
      <row id="MessagesCharset">
       <entry>Charset</entry>
       <entry>Character encoding of the messages to be processed.</entry>
      </row>
      <row id="Charset">
       <entry>Charset</entry>
       <entry>Character encoding of the files to be processed.</entry>
      </row>
      <row id="row-CharsetData">
       <entry>Charset</entry>
       <entry>Character encoding of the data to be processed.</entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">The following row is used in Configuring Hive
    Streaming</draft-comment>
  </p>
  <table frame="all" rowsep="1" colsep="1" id="table_ps1_hln_jt">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry/>
      <entry/>
     </row>
    </thead>
    <tbody>
     <row id="FIELD2ColumnMapping">
      <entry>Field to Column Mapping</entry>
      <entry>
       <p>Use to override the default field to column mappings. </p>
       <p>By default, fields are written to columns of the same name. </p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="alisontaylor">The following descriptions are used by the AWS destinations:
    Amazon S3, Kinesis Firehose, Kinesis Producer. And by writing aggregated statistics to Kinesis,
    in DPM chapter > Aggregated Statistics for Pipelines > Configuring a Pipeline to Aggregate
    Statistics</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_vnv_ncr_mv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry>Access Key ID </entry>
       <entry id="AWSDest_AccessKeyID">
        <p>AWS access key ID.</p>
        <p>Required when not using IAM roles with IAM instance profile credentials.</p>
       </entry>
      </row>
      <row>
       <entry>Secret Access Key</entry>
       <entry id="AWSDest_SecretAccessKey">
        <p>AWS secret access key. </p>
        <p>Required when not using IAM roles with IAM instance profile credentials. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="alisontaylor">The following descriptions are used by the AWS origins:
    Amazon S3 and Kinesis Consumer.</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_lnx_51w_mv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <tbody>
      <row>
       <entry>Access Key ID</entry>
       <entry id="AWSOrigin_AccessKeyID">
        <p>AWS access key ID.</p>
        <p>Required when not using IAM roles with IAM instance profile credentials.</p>
       </entry>
      </row>
      <row>
       <entry>Secret Access Key</entry>
       <entry id="AWSOrigin_SecretAccessKey">
        <p>AWS secret access key. </p>
        <p>Required when not using IAM roles with IAM instance profile credentials. </p>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <draft-comment author="Loretta">The following rows are used in full for the Amazon S3 origin. The
    Amazon S3 destination and Kinesis Firehose destination reuse individual rows. Data Format, File
    Compression, and File Name Pattern within Compressed Directory are also used in Directory, File
    Tail, HTTP Client, and SFTP Client.</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="AmazonS3-oProps">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>File Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="S3-Region">
       <entry>Region</entry>
       <entry>Amazon S3 region. </entry>
      </row>
      <row id="S3-Endpoint">
       <entry>Endpoint</entry>
       <entry>Endpoint to connect to when you select Other for the region. Enter the endpoint
        name.</entry>
      </row>
      <row id="S3Bucket">
       <entry>Bucket</entry>
       <entry>Bucket that contains the objects to be read.</entry>
      </row>
      <row id="S3Folder">
       <entry>Common Prefix <xref href="../Origins/AmazonS3-CommonPrefixPatterns.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wm3_212_wv"/>
        </xref></entry>
       <entry id="entry-CommonPrefix">Optional common prefix that describes the location of the
        objects. When defined, the common prefix acts as a root for the prefix pattern.</entry>
      </row>
      <row id="S3ObjectPathDelimiter">
       <entry>Delimiter</entry>
       <entry>Delimiter used by Amazon S3 to define the prefix hierarchy.<p>Default is slash ( /
         ).</p></entry>
      </row>
      <row id="S3IncludeMetadata-row">
       <entry>Include Metadata <xref href="../Origins/AmazonS3-RecordHeaderAttrs-ObjectMeta.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_sw3_wjx_yw"/>
        </xref></entry>
       <entry>Includes system-defined and user-defined metadata in record header attributes.
       </entry>
      </row>
      <row id="S3FileNamePattern">
       <entry>Prefix Pattern <xref href="../Origins/AmazonS3-CommonPrefixPatterns.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_tfr_bk5_ht"/>
        </xref></entry>
       <entry id="entry-PrefixPattern">Prefix pattern that describes the objects to be processed.
         <p>You can include the entire path to the objects. You can also use Ant-style path patterns
         to read objects recursively. </p></entry>
      </row>
      <row id="S3BufferLimit">
       <entry>Buffer Limit (KB)</entry>
       <entry>Maximum buffer size. The buffer size determines the size of the record that can be
        processed. <p>Decrease when memory on the <ph
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> machine
         is limited. Increase to process larger records when memory is available. </p><p>Default is
         128 KB.</p></entry>
      </row>
      <row id="Origin-FileCompression">
       <entry>Compression Format <xref href="../Origins/FileCompressionFormats.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
          id="image_xqq_yv4_c5"/></xref></entry>
       <entry>The compression format of the files:<ul id="ul_vph_jp2_qs">
         <li>None - Processes only uncompressed files.</li>
         <li>Compressed File - Processes files compressed by the supported compression formats.</li>
         <li>Archive - Processes files archived by the supported archive formats.</li>
         <li>Compressed Archive - Processes files archived and compressed by the supported archive
          and compression formats.</li>
        </ul></entry>
      </row>
      <row id="Origin-FilePatternCompressed">
       <entry>File Name Pattern within Compressed Directory</entry>
       <entry>File name pattern that represents the files to process within the compressed
        directory. You can use UNIX-style wildcards, such as an asterisk or question mark. For
        example, *.json.<p>Default is *, which processes all files.</p></entry>
      </row>
      <row id="S3DataFormat">
       <entry>Data Format <xref href="../Origins/AmazonS3-DataFormat.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_w4w_q3p_ht"/>
        </xref></entry>
       <entry id="entryDataFormats">Data format for source files. Use one of the following formats:<ul id="ul_y1t_wql_5q">
         <li>Avro</li>
         <li>Delimited</li>
         <li>JSON</li>
         <li>Log</li>
         <li>Protobuf</li>
         <li>SDC Record <xref href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
           <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wjh_ycl_br"
            placement="inline"/></xref></li>
         <li>Text</li>
         <li>Whole File <xref href="../Data_Formats/WholeFile.dita">
           <image href="../Graphics/icon_moreInfo.png" scale="10" placement="inline"
            id="image_igx_zzm_zw"/></xref></li>
         <li>XML</li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="alisontaylor">Kinesis Consumer, Kinesis Producer and DPM chapter >
   Aggregated Statistics for Pipelines > Configuring a Pipeline to Aggregate Statistics use these
   rows</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_kqy_mw1_fx">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <tbody>
     <row id="rowKinesisRegion">
      <entry>Region</entry>
      <entry>Amazon Web Services region that hosts the Kinesis cluster.</entry>
     </row>
     <row id="rowKinesisEndpoint">
      <entry>Endpoint</entry>
      <entry>Endpoint to connect to when you select Other for the region. Enter the endpoint
       name.</entry>
     </row>
     <row id="rowKinesisStreamName">
      <entry>Stream Name</entry>
      <entry>Kinesis stream name.</entry>
     </row>
     <row id="rowKinesisPartitionStrategy">
      <entry>Partitioning Strategy</entry>
      <entry>Strategy to write data to Kinesis shards:<ul id="ul_nqh_qw1_fx">
        <li>Random - Generates a random partition key.</li>
        <li>
         <p>Expression - Uses the result of an expression as the partition key.</p>
        </li>
       </ul></entry>
     </row>
     <row id="rowKinesisPartitionExp">
      <entry>Partition Expression</entry>
      <entry>Expression to generate the partition key used to pass data to different shards. <p>Use
        for the expression partition strategy. </p></entry>
     </row>
     <row id="rowKinesisConfig">
      <entry>Kinesis Producer Configuration</entry>
      <entry>Additional Kinesis properties. <p>When you add a configuration property, enter the
        exact property name and the value. The Kinesis Producer does not validate the property names
        or values. </p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="Loretta">HTTP Client origin, processor, and destination
    rows:</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_jf4_g24_jw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="HTTP-ReqTransferEncod">
       <entry>Request Transfer Encoding</entry>
       <entry>Use one of the following encoding types:<ul id="ul_o3l_mgv_vw">
         <li>Buffered - The standard transfer encoding type. </li>
         <li>Chunked - Transfers data in chunks. Not supported by all servers.</li>
        </ul><p>The default is Chunked.</p></entry>
      </row>
      <row id="HTTP-ConnectTimeout">
       <entry>Connect Timeout</entry>
       <entry>Maximum number of milliseconds to wait for a connection. <p>Use 0 to wait
         indefinitely.</p></entry>
      </row>
      <row id="HTTP-ReadTimeout">
       <entry>Read Timeout</entry>
       <entry>Maximum number of milliseconds to wait for data. <p>Use 0 to wait
        indefinitely.</p></entry>
      </row>
      <row id="HTTP-AuthType">
       <entry>Authentication Type</entry>
       <entry>Determines the authentication type used to connect to the server:<ul
         id="ul_icm_h1l_35">
         <li>None - Performs no authentication.</li>
         <li>Basic - Uses basic authentication. Requires a username and password. <p>Use with HTTPS
           to avoid passing unencrypted credentials.</p></li>
         <li>Digest - Uses digest authentication. Requires a username and password.</li>
         <li>Universal - Makes an anonymous connection, then provides authentication credentials
          upon receiving a 401 status and a WWW-Authenticate header request. <p>Requires a username
           and password associated with basic or digest authentication.</p><p>Use only with servers
           that respond to this workflow.</p></li>
         <li>OAuth - Uses OAuth 1.0 authentication. Requires OAuth credentials.</li>
        </ul></entry>
      </row>
      <row id="HTTP-OAuth2">
       <entry>Use OAuth 2</entry>
       <entry>Enables using OAuth 2 authorization to request access tokens. <p>You can use OAuth 2
         authorization with none, basic, digest, or universal authentication.</p></entry>
      </row>
      <row id="HTTP-UseProxy">
       <entry>Use Proxy</entry>
       <entry>
        <p>Enables using an HTTP proxy to connect to the system. </p>
       </entry>
      </row>
      <row id="HTTP-MaxRequestTime">
       <entry>Maximum Request Time</entry>
       <entry>Maximum number of seconds to wait for a request to complete.</entry>
      </row>
      <row id="HTTP-RateLimit">
       <entry>Rate Limit</entry>
       <entry>Maximum number of requests to make per second. Set a rate limit when sending requests
        to a rate-limited API. <p>Default is 0, which means there is no delay between
        requests.</p></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="alisontaylor">OAuth 2 table for HTTP Client origin, processor, and
   destination</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="HTTP_OAuth2">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>OAuth 2 Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Credentials Grant Type</entry>
      <entry>Type of client credentials grant type required by the HTTP service:<ul
        id="ul_m3c_t1f_5y">
        <li>Client credentials grant</li>
        <li>Resource owner password credentials grant</li>
        <li>JSON Web Tokens (JWT)</li>
       </ul></entry>
     </row>
     <row>
      <entry>Token URL</entry>
      <entry>URL to request the access token.</entry>
     </row>
     <row>
      <entry>Client ID</entry>
      <entry>Client ID that the HTTP service uses to identify the HTTP client.<p>Enter for the
        client credentials grant that uses a client ID and secret for authentication. Or, for the
        resource owner password credentials grant that requires a client ID and secret.</p></entry>
     </row>
     <row>
      <entry>Client Secret</entry>
      <entry>Client secret that the HTTP service uses to authenticate the HTTP client.<p>Enter for
        the client credentials grant that uses a client ID and secret for authentication. Or, for
        the resource owner password credentials grant that requires a client ID and secret.</p><note
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_ClientID"/></entry>
     </row>
     <row>
      <entry>User Name</entry>
      <entry>Resource owner user name.<p>Enter for the resource owner password credentials
        grant.</p></entry>
     </row>
     <row>
      <entry>Password</entry>
      <entry>Resource owner password.<p>Enter for the resource owner password credentials
        grant.</p><note conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"/></entry>
     </row>
     <row>
      <entry>JWT Signing Algorithm</entry>
      <entry>Algorithm used to sign the JSON Web Token (JWT).<p>Default is none. Enter for the JSON
        Web Tokens grant.</p></entry>
     </row>
     <row>
      <entry>JWT Signing Key</entry>
      <entry>Base64 encoded key used to sign the JSON Web Token, if you selected a signing
        algorithm.<note conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_JWTKey"/><p>Enter for
        the JSON Web Tokens grant.</p></entry>
     </row>
     <row>
      <entry>JWT Claims</entry>
      <entry>Claims to use in the JSON Web Token request, entered in JSON format. Enter each claim
       required by the HTTP service. You can include the expression language in the JWT
        claims.<p>For example, to read from Google service accounts, enter the following claims with
        the appropriate
        values:</p><codeblock>{
  "iss":"my_name@my_account.iam.gserviceaccount.com",
  "scope":"https://www.googleapis.com/auth/drive",
  "aud":"https://www.googleapis.com/oauth2/v4/token",
  "exp":${(time:dateTimeToMilliseconds(time:now())/1000) + 50 * 60},
  "iat":${time:dateTimeToMilliseconds(time:now())/1000}
}</codeblock><p>Enter
        for the JSON Web Tokens grant.</p></entry>
     </row>
     <row>
      <entry>Request Transfer Encoding</entry>
      <entry>Form of encoding to use when the stage requests an access token: buffered or
        chunked.<p>Default is buffered.</p></entry>
     </row>
     <row>
      <entry>Additional Key-Value Pairs</entry>
      <entry>Optional key-value pairs to send to the token URL when requesting an access token. For
       example, you can define the OAuth 2 <codeph>scope</codeph> request parameter.<p>Using <xref
         href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb">simple or bulk
         edit mode</xref>, click the <uicontrol>Add</uicontrol> icon to add additional key-value
        pairs.</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="Loretta">PROCESSOR rows</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_u2r_4x5_lv">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.0*"/>
     <colspec colname="c2" colnum="2" colwidth="1.0*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="P-HashType">
       <entry>Hash Type</entry>
       <entry>Algorithm to use to hash field values:<ul id="ul_kmd_rnk_wq">
         <li>MD5 - Produces a 128-bit (16-byte) hash value, typically expressed in text format as a
          32 digit hexadecimal number.</li>
         <li>SHA1 - Produces a 160-bit (20-byte) hash value.</li>
         <li>SHA2 - Based on SHA1, but uses a set of four hash functions: 224, 256, 384, or 512
          bits.</li>
         <li>MURMUR3_128 - Produces a 128-bit (16 byte) hash value.</li>
        </ul></entry>
      </row>
      <row id="P-HashTargetField">
       <entry>Target Field</entry>
       <entry>Field in the record to use for hashed data. If the field does not exist, Field Hasher
        creates the field. </entry>
      </row>
      <row id="P-HashHeaderAtt">
       <entry>Header Attribute</entry>
       <entry>Attribute in the record header to use for hashed data. If the attribute does not
        exist, Field Hasher creates the attribute.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">The Hive Metadata processor and the Hive Metastore destination
   configuring topics use these individual rows and the Max entries description. Hive Query executor
   also uses rows.</draft-comment>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_w1l_34y_dw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Hive Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="P-D_HiveJDBCURL">
       <entry>JDBC URL</entry>
       <entry>JDBC URL for Hive. You can use the default, or replace the expression for the database
        name with a specific database name when appropriate.<p>For more information about specifying
         the URL, see our <xref
          href="https://ask.streamsets.com/question/7/how-do-you-configure-a-hive-impala-jdbc-driver-for-data-collector/?answer=8#post-id-8"
          format="html" scope="external">Ask StreamSets post</xref>.</p></entry>
      </row>
      <row id="P-D_HiveJDBCdriver">
       <entry>JDBC Driver Name</entry>
       <entry>The fully-qualified JDBC driver name.</entry>
      </row>
      <row id="P-D_HiveConfigDir">
       <entry>Hadoop Configuration Directory</entry>
       <entry>
        <p>Absolute path to the directory containing the Hive and Hadoop configuration files. For a
         Cloudera Manager installation, enter hive-conf. </p>
        <p>The stage uses the following configuration files: <ul
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HiveStreamingFiles"
          id="ul_tqf_lms_dw">
          <li/>
         </ul></p>
        <note>Properties in the configuration files are overridden by individual properties defined
         in this stage. </note>
       </entry>
      </row>
      <row id="P-D-Hive-AddConfig">
       <entry>Additional Hadoop Configuration</entry>
       <entry>
        <p>Additional properties to use. </p>
        <p>To add properties, click <uicontrol>Add</uicontrol> and define the property name and
         value. Use the property names and values as expected by HDFS and Hive.</p>
       </entry>
      </row>
      <row>
       <entry>Max Cache Size (not conrefed)</entry>
       <entry id="P-D_HiveMaxCacheSize">Maximum number of entries in the cache. <p>When the cache
         reaches the maximum size, the oldest cached entries are evicted to allow for new
         data.</p><p>Default is -1, an unlimited cache size.</p></entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">Event destinations and executors, 1st step. Rows
   used.</draft-comment>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_wsl_2rx_mx">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>General Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="D-1stStep-Name">
       <entry>Name</entry>
       <entry>Stage name.</entry>
      </row>
      <row id="D-1stStep-Desc">
       <entry>Description</entry>
       <entry>Optional description.</entry>
      </row>
      <row id="D-1stStep-Library">
       <entry>Stage Library</entry>
       <entry>Library version that you want to use. </entry>
      </row>
      <row>
       <entry>Produce Events <xref href="../Event_Handling/StageEvents.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_wrq_mrx_mx"
        /></xref></entry>
       <entry id="D-1stStep-entry-Events">Generates event records when events occur. Use for event
        handling. <xref href="../Event_Handling/EventFramework-Overview.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_l15_ft2_px"
        /></xref></entry>
      </row>
      <row id="D-1stStep-ReqFields">
       <entry>Required Fields <xref href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_xsl_2rx_mx"
        /></xref></entry>
       <entry>Fields that must include data for the record to be passed into the stage. <note
         outputclass="" type="tip">You might include fields that the stage uses.</note><p>Records
         that do not include all required fields are processed based on the error handling
         configured for the pipeline.</p></entry>
      </row>
      <row id="D-1stStep-Precond">
       <entry>Preconditions <xref href="../Pipeline_Design/Preconditions.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_ysl_2rx_mx"
        /></xref></entry>
       <entry>Conditions that must evaluate to TRUE to allow a record to enter the stage for
        processing. Click <uicontrol>Add</uicontrol> to create additional preconditions. <p>Records
         that do not meet all preconditions are processed based on the error handling configured for
         the stage.</p></entry>
      </row>
      <row id="D-1stStep-Error">
       <entry>On Record Error <xref href="../Pipeline_Design/ErrorHandling-Stage.dita">
         <image href="../Graphics/icon_moreInfo.png" scale="10" id="image_zsl_2rx_mx"
        /></xref></entry>
       <entry>Error record handling for the stage: <ul id="ul_atl_2rx_mx">
         <li>Discard - Discards the record.</li>
         <li>Send to Error - Sends the record to the pipeline for error handling.</li>
         <li>Stop Pipeline - Stops the pipeline.</li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">Hadoop FS, MapR FS, Local FS use
     <uicontrol>D-Event-RHattributes</uicontrol> in Event Records. All others use the creation_date
    entry: <uicontrol>entry-Event-creationDate</uicontrol>. </draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="D-Event-RHattributes">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Record Header Attribute</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry id="entry-eventType">sdc.event.type</entry>
       <entry>Event type. Uses one of the following types:<ul id="ul_m12_mgp_qx">
         <li>file-closed - Generated when the destination closes a file.</li>
         <li>wholeFileProcessed - Generated when the destination completes streaming a whole
          file.</li>
        </ul></entry>
      </row>
      <row id="row-eventVersion">
       <entry>sdc.event.version</entry>
       <entry>An integer that indicates the version of the event record type.</entry>
      </row>
      <row id="row-eventTimestamp">
       <entry>sdc.event.creation_timestamp</entry>
       <entry id="entry-Event-creationDate">Epoch timestamp when the stage created the event.
       </entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">Table used by Salesforce Event Record and Multitable JDBC. JDBC
    Query uses the no-more-data bullet.</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="EventHead-NoMoreData-table">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Record Header Attribute</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry-eventType"/>
       <entry>Event type. Uses the following type:<ul id="ul_f4w_xrs_kz">
         <li id="noMoreData-bullet">no-more-data - Generated when the origin completes processing
          all data returned by a query.</li>
        </ul></entry>
      </row>
      <row conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/row-eventVersion">
       <entry/>
      </row>
      <row conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/row-eventTimestamp">
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">DESTINATION rows</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="table_l3k_ksh_r5">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry/>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="D-CHARSET-file">
       <entry>Charset</entry>
       <entry>Character set to use when writing files. </entry>
      </row>
      <row id="D-CHARSET-other">
       <entry>Charset</entry>
       <entry>Character set to use when writing data. </entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="alisontaylor"><b>table-JDBCProps</b> - JDBC Query Consumer, Multi Table
    JDBC Consumer, JDBC Lookup, JDBC Producer, and JDBC Tee use most rows in this table. Oracle CDC
    is using a couple too. JDBC Query executor also uses some. – no one uses the whole table AFAI
    can tell.&lt;lc> multi-row insert - only phrases are used by JDBC Tee &amp; Producer to allow
    for the xref.</draft-comment>
   <draft-comment author="Loretta">SQL Change Tracking uses rows and the Multitable JDBC Consumer's
    multithreaded entries. (rows used only by Multitable JDBC)</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table_vwl_qcq_tw">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.0*"/>
     <thead>
      <row>
       <entry>JDBC Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="JDBCConnectString_row">
       <entry>JDBC Connection String</entry>
       <entry>Connection string to use to connect to the database.<p>Some databases, such as
         Postgres, require the schema in the connection string. Use the connection string format
         required by the database.</p></entry>
      </row>
      <row id="FieldToColumnJDBC_row">
       <entry>Field to Column Mapping</entry>
       <entry>Use to override the default field to column mappings. By default, fields are written
        to columns of the same name.<p>When you override the mappings, you can define parameterized
         values to apply SQL functions to the field values before writing them to columns. For
         example, to convert a field value to an integer, enter the following for the parameterized
         value:<codeblock>CAST(? AS INTEGER)</codeblock></p><p>The question mark (?) is substituted
         with the value of the field. Leave the default value of ? if you do not need to apply a SQL
         function.</p><p>Using <xref
          href="../Pipeline_Configuration/SimpleBulkEdit.dita#concept_alb_b3y_cbb">simple or bulk
          edit mode</xref>, click the <uicontrol>Add</uicontrol> icon to create additional field to
         column mappings.</p></entry>
      </row>
      <row id="JDBCQueryInterval">
       <entry>Query Interval</entry>
       <entry>Amount of time to wait between queries. Enter an expression based on a unit of time.
        You can use SECONDS, MINUTES, or HOURS.<p>Default is 10 seconds: ${10 * SECONDS}.
        </p></entry>
      </row>
      <row id="JDBCUseCredentials_row">
       <entry>Use Credentials</entry>
       <entry>Enables entering credentials on the Credentials tab. Use when you do not include
        credentials in the JDBC connection string.</entry>
      </row>
      <row id="JDBC-NumThreads">
       <entry>Number of Threads <xref href="../Origins/MultiTableJDBCConsumer-Multithread.dita"
          ><image href="../Graphics/icon_moreInfo.png" scale="10" id="image_r2s_8lq_fs"/>
        </xref></entry>
       <entry id="entry-JDBCNumThreads">Number of threads the origin generates and uses for
        multithreaded processing.<p>Configure the Maximum Pool Size property on the Advanced tab to
         be equal to or greater than this value.</p></entry>
      </row>
      <row id="JDBC-perBatch">
       <entry>Per Batch Strategy <xref href="../Origins/MultiTableJDBCConsumer-BatchStrategy.dita"
          ><image href="../Graphics/icon_moreInfo.png" scale="10" id="image_r3s_7rq_ds"/>
        </xref></entry>
       <entry id="entry-JDBCperBatch">Strategy to create each batch of data:<ul id="ul_fk4_11f_py">
         <li>Switch Tables - Each thread creates a set of batches from one table, and then switches
          to the next available table to create the next set of batches. Define the Result Set Cache
          Size and the Batches from Result Set properties when you configure a switch tables
          strategy.</li>
         <li>Process All Available Rows from the Table - Each thread creates multiple batches of
          data from one table, until all available rows are read from that table.</li>
        </ul></entry>
      </row>
      <row id="JDBCMaxBatch_row">
       <entry>Max Batch Size (records)</entry>
       <entry>Maximum number of records to include in a batch.</entry>
      </row>
      <row id="JDBC-BatchResultSet">
       <entry>Batches from Result Set <xref
         href="../Origins/MultiTableJDBCConsumer-BatchStrategy.dita"><image
          href="../Graphics/icon_moreInfo.png" scale="10" id="image_h3m_wkh_hz"/>
        </xref></entry>
       <entry id="entry-BatchResultSet">Number of batches to create from the result set. After a
        thread creates this number of batches, the database closes the result set and then another
        thread can read from the same table.<p>Use a positive integer to set a limit on the number
         of batches created from the result set. Use -1 to opt out of this property.</p><p>By
         default, the origin creates an unlimited number of batches from the result set, keeping the
         result set open as long as possible.</p></entry>
      </row>
      <row id="JDBC-ResultSetCache">
       <entry>Result Set Cache Size <xref
         href="../Origins/MultiTableJDBCConsumer-BatchStrategy.dita"><image
          href="../Graphics/icon_moreInfo.png" scale="10" id="image_r9h_3rq_ds"/>
        </xref></entry>
       <entry id="entry-ResultSetCache">Number of result sets to cache in the database. Use a
        positive integer to set a limit on the number of cached result sets. Use -1 to opt out of
        this property.<p>By default, the origin caches an unlimited number of result
        sets.</p></entry>
      </row>
      <row id="JDBCMaxClob_row">
       <entry>Max Clob Size (characters)</entry>
       <entry>Maximum number of characters to be read in a Clob field. Larger data is
        truncated.</entry>
      </row>
      <row id="JDBCMaxBlob_row">
       <entry>Max Blob Size (bytes)</entry>
       <entry>
        <p>Maximum number of bytes to be read in a Blob field. </p>
       </entry>
      </row>
      <row id="JDBC_NumRetries">
       <entry>Number of Retries on SQL Error</entry>
       <entry>Number of times a thread tries to read a batch of data after receiving an SQL error.
        After a thread retries this number of times, the thread handles the error based on the error
        handling configured for the origin.<p>Use to handle transient network or connection issues
         that prevent a thread from reading a batch of data.</p><p>Default is 0. </p></entry>
      </row>
      <row id="JDBC-DataTimeZone">
       <entry>Data Time Zone</entry>
       <entry>Time zone to use to evaluate datetime-based offset column conditions. </entry>
      </row>
      <row id="JDBC-FetchSize">
       <entry id="entry-FetchSize">Fetch Size</entry>
       <entry><ph id="ph-FetchSize1">Maximum number of rows to fetch and store in memory on the <ph
          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> machine.
         The size cannot be zero.</ph><p><ph id="ph-FetchSize2">Default is 1,000.</ph></p><p>
         <note>By default, MySQL fetches and stores the complete result set in memory on the <ph
           conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
          machine. If the result sets have a large number of rows or large values that exceed
          available memory, specify a fetch size of <codeph>Integer.MIN_VALUE</codeph> so that MySQL
          streams the results to the <ph
           conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> machine
          one row at a time.</note>
        </p><p>For more information about configuring a fetch size, see your database
         documentation.</p></entry>
      </row>
      <row>
       <entry/>
       <entry/>
      </row>
      <row id="LogFormat_row">
       <entry>Change Log Format</entry>
       <entry>Format of change capture data. Use when processing change capture data. </entry>
      </row>
      <row>
       <entry>Default Operation </entry>
       <entry>&lt;&lt; using conref to elasticsearch>></entry>
      </row>
      <row>
       <entry>Unsupported Operation Handling </entry>
       <entry>&lt;&lt; using conref to elasticsearch>></entry>
      </row>
      <row id="RollbackBatch_row">
       <entry>Rollback Batch On Error</entry>
       <entry>Rolls back the entire batch when an error occurs within the batch. </entry>
      </row>
      <row id="MultiRowInsert_row">
       <entry>Use Multi-Row Operation</entry>
       <entry><ph id="ph-MultirowDesc">Determines how the stage processes records. Select to enable
         inserting and deleting multiple records at a time. Before enabling this option, verify that
         the database supports the multi-row SQL statements used by the stage. </ph> For more
        information, see &lt;&lt;local xref>>.<p><ph id="ph-Multirow-Default">By default, the stage
          performs single-row operations.</ph>
        </p></entry>
      </row>
      <row id="ParamLimit_row">
       <entry>Statement Parameter Limit</entry>
       <entry>Defines the number of parameters allowed in the prepared statement for multi-row
         inserts.<p>Use -1 to disable the parameter limit. Default is -1.</p></entry>
      </row>
      <row id="JDBCAddtitionalProps_row">
       <entry>Additional JDBC Configuration Properties</entry>
       <entry>Additional JDBC configuration properties to use. To add properties, click
         <uicontrol>Add</uicontrol> and define the JDBC property name and value. <p>Use the property
         names and values as expected by JDBC. </p></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p/>
  <p/>
  <p>
   <draft-comment author="Loretta"><b>table-JDBCAdvProps</b> - JDBC Lookup uses rows in the whole
    table. JDBC Query Consumer uses rows but has its own. JDBC Tee uses some rows. Producer uses a
    bunch of rows. Oracle CDC uses most rows but Auto Commit. – note updated LC 2/9/17 – LC adding
    On Unknown Type - so far just used by JDBC origins.</draft-comment>
   <table frame="all" rowsep="1" colsep="1" id="table-JDBCAdvProps">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Advanced Property</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row id="row-MaxPoolSize">
       <entry>Maximum Pool Size </entry>
       <entry>The maximum number of connections to create. <p>Default is 1. The recommended value is
         1.</p></entry>
      </row>
      <row id="MaxPoolSize-Multithread">
       <entry>Maximum Pool Size</entry>
       <entry>The maximum number of connections to create. Must be equal to or greater than the
        value of the Number of Threads property.<p>Default is 1.</p></entry>
      </row>
      <row id="row-MaxIdleConn">
       <entry>Minimum Idle Connections</entry>
       <entry>The minimum number of connections to create and maintain. To define a fixed connection
        pool, set to the same value as Maximum Pool Size. <p>Default is 1. </p></entry>
      </row>
      <row id="row-ConTimeout">
       <entry>Connection Timeout</entry>
       <entry>Maximum time to wait for a connection. Use a time constant in an expression to define
        the time increment. <p>Default is 30 seconds, defined as follows:
         <codeblock>${30 * SECONDS}</codeblock></p></entry>
      </row>
      <row id="row-IdleTimeout">
       <entry>Idle Timeout</entry>
       <entry>Maximum time to allow a connection to idle. Use a time constant in an expression to
        define the time increment. <p>Use 0 to avoid removing any idle connections.</p><p>Default is
         30 minutes, defined as follows: <codeblock>${30 * MINUTES}</codeblock></p></entry>
      </row>
      <row id="row-MaxConLife">
       <entry>Max Connection Lifetime</entry>
       <entry>Maximum lifetime for a connection. Use a time constant in an expression to define the
        time increment. <p>Use 0 to avoid removing any idle connections.</p><p>Default is 30
         seconds, defined as follows: <codeblock>${30 * SECONDS}</codeblock></p></entry>
      </row>
      <row id="row-AutoCommit">
       <entry>Auto Commit</entry>
       <entry>Determines if auto-commit mode is enabled. In auto-commit mode, the database commits
        the data for each record. <p>Default is disabled.</p></entry>
      </row>
      <row id="row-EnReadOnly">
       <entry>Enforce Read-only Connection</entry>
       <entry>Creates read-only connections to avoid any type of write. <p>Default is enabled.
         Disabling this property is not recommended. </p></entry>
      </row>
      <row id="row-TransactionIsolation">
       <entry>Transaction Isolation</entry>
       <entry>Transaction isolation level used to connect to the database. <p>Default is the default
         transaction isolation level set for the database. You can override the database default by
         setting the level to any of the following:</p><ul id="ul_arc_mdf_2y">
         <li>Read committed </li>
         <li>Read uncommitted </li>
         <li>Repeatable read</li>
         <li>Serializable</li>
        </ul></entry>
      </row>
      <row id="row-InitTableOrder">
       <entry>Initial Table Order Strategy <xref href="../Origins/MultiTableJDBCConsumer-TableOrder.dita"
          ><image href="../Graphics/icon_moreInfo.png" scale="10" id="image_r5t_0kq_ds"/>
        </xref></entry>
       <entry id="entry-InitTableOrder">Initial order used to read the tables:<ul id="ul_opw_1jf_py">
         <li id="li-InitialOrder-None">None - Reads the tables in the order that they are listed in
          the database.</li>
         <li id="li-IniitalOrder-Alpha">Alphabetical - Reads the tables in alphabetical order.</li>
         <li>Referential Constraints - Reads the tables based on the dependencies between the
          tables.</li>
        </ul></entry>
      </row>
      <row id="row-JDBCunknownType">
       <entry>On Unknown Type</entry>
       <entry>Action to take when the origin encounters a record with an unsupported data type:<ul
         id="ul_jcm_gdc_41b">
         <li>Stop Pipeline - Stops the pipeline after completing processing the previous
          records.</li>
         <li>Convert to String - Converts the data to string and continues processing. </li>
        </ul></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p/>
  <draft-comment author="alisontaylor"><b>table-S3AdvProps</b> Amazon S3 destination uses the whole
   table, S3 and Kinesis Consumer use certain rows.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table-S3AdvProps">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Advanced Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-UseProxy">
      <entry>Use Proxy</entry>
      <entry>Specifies whether to use a proxy to connect.</entry>
     </row>
     <row id="row-ProxyHost">
      <entry>Proxy Host</entry>
      <entry>Proxy host.</entry>
     </row>
     <row id="row-ProxyPort">
      <entry>Proxy Port</entry>
      <entry>Proxy port.</entry>
     </row>
     <row id="row-ProxyUser">
      <entry>Proxy User</entry>
      <entry>User name for proxy credentials.</entry>
     </row>
     <row id="row-ProxyPassword">
      <entry>Proxy Password</entry>
      <entry>Password for proxy credentials.<note
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"/></entry>
     </row>
     <row>
      <entry>Thread Pool Size for Parallel Uploads</entry>
      <entry>Size of the thread pool for parallel uploads. Used when writing to multiple partitions
       and writing large objects in multiple parts.<p>When writing to multiple partitions, setting
        this property up to the number of partitions being written to can improve performance.
        </p><p>For more information about this and the following properties, see the Amazon S3
        TransferManager documentation.</p></entry>
     </row>
     <row>
      <entry>Multipart Upload Threshold</entry>
      <entry>Minimum batch size in bytes for the destination to use multipart uploads.</entry>
     </row>
     <row>
      <entry>Minimum Upload Part Size</entry>
      <entry>Minimum part size in bytes for multipart uploads.</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">HBase, Redis, and Static Lookup processors use rows in the
   table. See below for same properties worded slightly differently for JDBC Lookup. Make same
   changes in both places.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_ns1_p1s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Lookup Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-Mode">
      <entry>Mode</entry>
      <entry>Mode used to perform the lookups:<ul id="ul_t11_3fs_zv">
       <li>Per Batch - Performs a bulk lookup of all keys in a
        batch. The processor performs a single lookup for
        each batch.</li>
       <li>Per Key in Each Record - Performs individual lookups
        of each key in each record. If you configure
        multiple key expressions, the processor performs
        multiple lookups for each record.</li>
      </ul><p>Default is Per Batch.</p></entry>
     </row>
     <row>
      <entry>Enable Local Caching </entry>
      <entry id="entry-LocalCaching">Specifies whether to locally cache the returned key-value
       pairs.</entry>
     </row>
     <row id="row-MaxEntriesCache">
      <entry>Maximum Entries to Cache</entry>
      <entry>Maximum number of key-value pairs to cache. When the
       maximum number is reached, the processor evicts the oldest
       key-value pairs from the cache. <p>Default is -1, which
        means unlimited.</p></entry>
     </row>
     <row id="row-EvictionPolicy">
      <entry>Eviction Policy Type</entry>
      <entry>Policy used to evict key-value pairs from the local cache
       when the expiration time has passed:<ul id="ul_jql_yns_zv">
        <li>Expire After Last Access - Measures the expiration
         time since the key-value pair was last accessed by a
         read or a write.</li>
        <li>Expire After Last Write - Measures the expiration
         time since the key-value pair was created, or since
         the value was last replaced.</li>
       </ul></entry>
     </row>
     <row id="row-ExpirationTime">
      <entry>Expiration Time</entry>
      <entry>Amount of time that a key-value pair can remain in the
       local cache without being accessed or written to. <p>Default
        is 1 second.</p></entry>
     </row>
     <row id="row-timeUnit">
      <entry>Time Unit</entry>
      <entry>Unit of time for the expiration time. <p>Default is
       seconds.</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">JDBC Lookup, Salesforce Lookup, and Kudu Lookup use rows in
   this table</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_ns3_p7s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Lookup Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Enable Local Caching </entry>
      <entry id="entry-JDBCLocalCaching">Specifies whether to locally cache the returned
       values.</entry>
     </row>
     <row id="row-JDBCMaxEntriesCache">
      <entry>Maximum Entries to Cache</entry>
      <entry>Maximum number of values to cache. When the maximum number is reached, the processor
       evicts the oldest values from the cache. <p>Default is -1, which means unlimited.</p></entry>
     </row>
     <row id="row-JDBCEvictionPolicy">
      <entry>Eviction Policy Type</entry>
      <entry>Policy used to evict values from the local cache when the expiration time has
        passed:<ul id="ul_jik_yns_zv">
        <li>Expire After Last Access - Measures the expiration time since the value was last
         accessed by a read or a write.</li>
        <li>Expire After Last Write - Measures the expiration time since the value was created, or
         since the value was last replaced.</li>
       </ul></entry>
     </row>
     <row id="row-JDBCExpirationTime">
      <entry>Expiration Time</entry>
      <entry>Amount of time that a value can remain in the local cache without being accessed or
       written to. <p>Default is 1 second.</p></entry>
     </row>
     <row id="row-JDBCtimeUnit">
      <entry>Time Unit</entry>
      <entry>Unit of time for the expiration time. <p>Default is
       seconds.</p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">HBase destination and HBase Lookup use rows and entries in
   this table</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_bgt_kly_bw">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>HBase Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-ZooKeeperQuorum">
      <entry>ZooKeeper Quorum</entry>
      <entry>Comma-separated list of servers in the ZooKeeper quorum. Use the following format:
        <codeblock>&lt;host>.&lt;domain>.com</codeblock><p>To ensure a connection, enter additional
        broker URIs.</p></entry>
     </row>
     <row id="row-ZooKeeperClient">
      <entry>ZooKeeper Client Port</entry>
      <entry>Port number clients use to connect to the ZooKeeper servers. </entry>
     </row>
     <row id="row-ZooKeeperParent">
      <entry>ZooKeeper Parent Znode</entry>
      <entry>Root node that contains all znodes used by the HBase cluster.</entry>
     </row>
     <row id="row-TableName">
      <entry>Table Name</entry>
      <entry>Name of the HBase table to use. Enter a table name or a namespace and table name as
       follows: &lt;namespace>.&lt;tablename>. <p>If you do not enter a table name, HBase uses the
        default namespace. </p></entry>
     </row>
     <row>
      <entry>Kerberos Authentication </entry>
      <entry id="entry-Kerberos">Uses Kerberos credentials to connect to HBase.<p>When selected,
        uses the Kerberos principal and keytab defined in the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
        configuration file, <codeph>$SDC_CONF/sdc.properties</codeph>. </p></entry>
     </row>
     <row>
      <entry>HBase User </entry>
      <entry id="entry-HBaseUser">The HBase user to use to write to HBase. When using this property,
       make sure HBase is configured appropriately.<p>When not configured, the pipeline uses the
        currently logged in <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> user.
        </p><p>Not configurable when <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> is
        configured to use the currently logged in <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> user. For
        more information, see <xref
         href="../Configuration/HadoopImpersonationMode.dita#concept_pmr_sy5_nz"/>.</p></entry>
     </row>
     <row>
      <entry>HBase Configuration Directory</entry>
      <entry id="entry-HBaseConfigDirectory">Location of the HDFS configuration files. <p>For a
        Cloudera Manager installation, enter <codeph>hbase-conf</codeph>. For all other
        installations, use a directory or symlink within the <ph
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> resources
        directory.</p><p>You can use the following file with HBase:<ul
         conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HDFSfiles_HBasedest"
         id="ul_ezj_cvr_bt">
         <li/>
        </ul></p><note>Properties in the configuration files are overridden by individual properties
        defined in the stage.</note></entry>
     </row>
     <row>
      <entry>HBase Configuration</entry>
      <entry id="entry-HBaseConfig">
       <p>Additional HBase configuration properties to use. </p>
       <p>To add properties, click <uicontrol>Add</uicontrol> and
        define the property name and value. Use the property
        names and values as expected by HBase. </p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Redis origin, destination, and Lookup processor use this
   row</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_i5s_54s_zv">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Redis Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-RedisURI">
      <entry>URI</entry>
      <entry>URI of the Redis server. Use the following
        format:<codeblock>redis://&lt;host name>:&lt;port number>/&lt;database></codeblock><p>You
        can omit the database if the server uses the default database.</p><p>You can optionally
        include your password to log in to the Redis server. For
        example:<codeblock>redis://:&lt;password>@&lt;host name>:&lt;port number>/&lt;database></codeblock></p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">MongoDB origin and destination use all rows in this table.
   They use the description for the Enable SSL property</draft-comment>
    <table frame="all" rowsep="1" colsep="1" id="table_xwz_jsd_3t">
     <tgroup cols="2">
      <colspec colname="c1" colnum="1" colwidth="1.5*"/>
      <colspec colname="c2" colnum="2" colwidth="3.5*"/>
      <thead>
       <row>
        <entry>Advanced Property</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row id="MongoDB_Connections_row">
      <entry>Connections Per Host</entry>
      <entry>Maximum number of connections for each host.<p>Default is 100.</p></entry>
     </row>
       <row id="MongoDB_MinConnections_row">
      <entry>Min Connections Per Host</entry>
      <entry>Minimum number of connections for each host.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_ConnectionTimeout_row">
      <entry>Connection Timeout</entry>
      <entry>Maximum time in milliseconds to wait for a connection. <p>Default is
       10,000.</p></entry>
     </row>
       <row id="MongoDB_MaxConnIdleTime_row">
      <entry>Max Connection Idle Time</entry>
      <entry>Maximum time in milliseconds that a pooled connection can remain idle. When a pooled
       connection exceeds the idle time, the connection is closed. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_MaxConnLifetime_row">
      <entry>Max Connection Lifetime</entry>
      <entry>Maximum time in milliseconds that a pooled connection can be active. When a pooled
       connection exceeds the lifetime, the connection is closed. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_MaxWaitTime_row">
      <entry>Max Wait Time</entry>
      <entry>Maximum time in milliseconds that a thread can wait for a connection to become
       available. Use 0 to opt out of this property. Use a negative value to wait
        indefinitely.<p>Default is 120,000.</p></entry>
     </row>
       <row id="MongoDB_ServerTimeout_row">
      <entry>Server Selection Timeout</entry>
      <entry>Maximum time in milliseconds that <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> waits for a server selection
       before throwing an exception. If you use 0, an exception is thrown immediately if no server
       is available. Use a negative value to wait indefinitely.<p>Default is 30,000.</p></entry>
     </row>
       <row id="MongoDB_ThreadsAllowed_row">
      <entry>Threads Allowed to Block for Connection Multiplier</entry>
      <entry>Multiplier that determines the maximum number of threads that can wait for a connection
       to become available from the pool. This number multiplied by the Connections Per Host value
       determines the maximum number of threads.<p>Default is 5.</p></entry>
     </row>
       <row id="MongoDB_Heartbeat_row">
      <entry>Heartbeat Frequency</entry>
      <entry>The frequency in milliseconds at which <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> attempts to determine the
       current state of each server in the cluster.<p>Default is 10,000.</p></entry>
     </row>
       <row id="MongoDB_MinHeartbeat_row">
      <entry>Min Heartbeat Frequency</entry>
      <entry>Minimum heartbeat frequency in milliseconds. <ph
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> waits at least this long
       before checking the state of each server.<p>Default is 500.</p></entry>
     </row>
       <row id="MongoDB_HeartbeatConn_row">
      <entry>Heartbeat Connection Timeout</entry>
      <entry>Maximum time in milliseconds to wait for a connection used for the cluster
        heartbeat.<p>Default is 20,000.</p></entry>
     </row>
       <row id="MongoDB_HeartbeatSocket_row">
      <entry>Heartbeat Socket Timeout</entry>
      <entry>Maximum time in milliseconds for a socket timeout for connections used for the cluster
        heartbeat.<p>Default is 20,000.</p></entry>
     </row>
       <row id="MongoDB_LocalThreshold_row">
      <entry>Local Threshold</entry>
      <entry>Local threshold in milliseconds. Requests are sent to a server whose ping time is less
       than or equal to the server with the fastest ping time plus the local threshold
        value.<p>Default is 15.</p></entry>
     </row>
       <row id="MongoDB_Replica_row">
      <entry>Required Replica Set Name</entry>
      <entry>Required replica set name to use for the cluster.</entry>
     </row>
       <row id="MongoDB_Cursor_row">
      <entry>Cursor Finalizer Enabled</entry>
      <entry>Specifies whether to enable cursor finalizers.</entry>
     </row>
       <row id="MongoDB_SocketKeepAlive_row">
      <entry>Socket Keep Alive</entry>
      <entry>Specifies whether to enable socket keep alive. </entry>
     </row>
       <row id="MongoDB_SocketTimeout_row">
      <entry>Socket Timeout</entry>
      <entry>Maximum time in milliseconds for the socket timeout. Use 0 to opt out of this
        property.<p>Default is 0.</p></entry>
     </row>
       <row id="MongoDB_SSLInvalid_row">
      <entry>SSL Invalid Host Name Allowed</entry>
      <entry>Specifies whether invalid host names are allowed in SSL/TLS certificates.</entry>
     </row>
      </tbody>
     </tgroup>
    </table>
  <draft-comment author="alisontaylor">Salesforce origin, Salesforce Lookup processor, and
   Salesforce destination use the first three rows in this table, and use the description of the
   last row. Wave Analytics destination uses the first two rows in the table. Salesforce origin and
   Lookup processor use the last row.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_fg1_cbc_tx">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Salesforce Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="SalesforceUser">
      <entry>Username</entry>
      <entry>Salesforce username in the following email format:
       <codeph>&lt;text>@&lt;text>.com</codeph>. </entry>
     </row>
     <row id="SalesforcePassword">
      <entry>Password</entry>
      <entry>Salesforce password. <note
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"/></entry>
     </row>
     <row id="SalesforceEndpoint">
      <entry>Auth Endpoint</entry>
      <entry>Salesforce SOAP API authentication endpoint. Enter one of the following values:<ul
        id="ul_lnr_x2c_tx">
        <li>login.salesforce.com - Use to connect to a Production or Developer Edition
         organization.</li>
        <li>test.salesforce.com - Use to connect to a sandbox organization.</li>
       </ul><p>Default is login.salesforce.com.</p></entry>
     </row>
     <row>
      <entry>API Version</entry>
      <entry id="SalesforceAPIVersion">Salesforce API version to use to connect to Salesforce.
        <p>Default is <ph
         conref="Reusable_Topics/Salesforce-APIVersion.dita#task_qzy_4ck_dy/ph_SalesforceAPIVersion"
        />. If you change the version, you also must download the relevant JAR files from Salesforce
        Web Services Connector (WSC).</p></entry>
     </row>
     <row id="SalesforceIncludeDeleted">
      <entry>Include Deleted Records</entry>
      <entry><ph id="ph-DeletedRecords1">Determines whether the SOQL query also retrieves deleted
        records from the Salesforce recycle bin.</ph><p><ph id="ph-DeletedRecords2">The query can
         retrieve deleted records when the stage uses the Salesforce SOAP API or the Bulk API
         version 39.0 or later. Earlier versions of the Bulk API do not support retrieving deleted
         records.</ph></p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">All  Salesforce stages use the rows in this
   table</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="Salesforce_Proxy">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Advanced Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row_SalesforceProxy">
      <entry>Use Proxy</entry>
      <entry>Specifies whether to use an HTTP proxy to connect to Salesforce.</entry>
     </row>
     <row>
      <entry>Proxy Hostname</entry>
      <entry>Proxy host.</entry>
     </row>
     <row id="row_SalesforceProxyPort">
      <entry>Proxy Port</entry>
      <entry>Proxy port.</entry>
     </row>
     <row id="row_SalesforceProxyCredentials">
      <entry>Proxy Requires Credentials</entry>
      <entry>Specifies whether the proxy requires a user name and password.</entry>
     </row>
     <row id="row_SalesforceProxyUser">
      <entry>Proxy Username</entry>
      <entry>User name for proxy credentials.</entry>
     </row>
     <row id="row_SalesforceProxyPassword">
      <entry>Proxy Password</entry>
      <entry>Password for proxy credentials.<note
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"/></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Description of each role is conref'd in the LDAP and File
   Authentication sections. The whole table is conrefed by the Roles topic. (LC
   2/17)</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="Role_table">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.0*"/>
    <colspec colname="c2" colnum="2" colwidth="3.0*"/>
    <thead>
     <row>
      <entry>Role</entry>
      <entry>Tasks</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>admin</entry>
      <entry id="entry_adminRole">Perform any <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> task. Can
       perform all tasks listed below, as well as start and stop <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>, view <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
       configuration, JVM metrics, and log information. Install libraries using the Package Manager.
       Generate support bundles.</entry>
     </row>
     <row>
      <entry>manager</entry>
      <entry id="entry_managerRole">Start and stop pipelines, monitor pipelines, configure and reset
       alerts. Take, review, and manage snapshots. </entry>
     </row>
     <row>
      <entry>creator</entry>
      <entry id="entry_creatorRole">Create and configure pipelines and alerts, preview data, and
       monitor the pipeline. Import pipelines.</entry>
     </row>
     <row>
      <entry>guest</entry>
      <entry id="entry_guestRole">View pipelines and alerts, and general monitoring information.
       Export a pipeline. </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <p>
   <draft-comment author="Loretta">Permissions table - used in Pipeline Maintenance > Sharing
    Pipelines and Configuring > Users, Groups, blah blah > Pipeline Permissions</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="Permissions_pipe">
    <tgroup cols="2">
     <colspec colname="c1" colnum="1" colwidth="1.5*"/>
     <colspec colname="c2" colnum="2" colwidth="3.5*"/>
     <thead>
      <row>
       <entry>Permission</entry>
       <entry>Description</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>Read</entry>
       <entry>View <ph product="SDC">and monitor </ph>the pipeline, and see alerts. <ph
         product="SDC">View existing snapshot data.</ph></entry>
      </row>
      <row>
       <entry>Write</entry>
       <entry>Edit the pipeline and alerts.</entry>
      </row>
      <row>
       <entry>Execute</entry>
       <entry>Start and stop the pipeline. Preview data<ph product="SDC"> and take a
        snapshot</ph>.</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <p>
   <draft-comment author="Loretta">The following User-Roles table used in Users, Groups, Roles,
    Perms > Users and also in File-Based Authentication > Step 2.</draft-comment>
  </p>
  <p>
   <table frame="all" rowsep="1" colsep="1" id="User-Roles">
    <tgroup cols="3">
     <colspec colname="newCol3" colnum="1" colwidth="1*"/>
     <colspec colname="c1" colnum="2" colwidth="1*"/>
     <colspec colname="c2" colnum="3" colwidth="3.0*"/>
     <thead>
      <row>
       <entry>User Login</entry>
       <entry>Role</entry>
       <entry>Tasks</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry><codeph>admin</codeph> / <codeph>admin</codeph></entry>
       <entry>Admin</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_adminRole"/>
      </row>
      <row>
       <entry><codeph>manager</codeph> / <codeph>manager</codeph></entry>
       <entry>Manager</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_managerRole"
       />
      </row>
      <row>
       <entry><codeph>creator</codeph> / <codeph>creator</codeph></entry>
       <entry>Creator</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_creatorRole"
       />
      </row>
      <row>
       <entry><codeph>guest</codeph> / <codeph>guest</codeph></entry>
       <entry>Guest</entry>
       <entry conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/entry_guestRole"/>
      </row>
     </tbody>
    </tgroup>
   </table>
  </p>
  <draft-comment author="Loretta">The following User-Roles table used in Users, Groups, Roles, Perms
   > Users and also in File-Based Authentication > Step 2.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="User-Groups">
   <tgroup cols="3">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="2.0*"/>
    <colspec colname="newCol3" colnum="3" colwidth="1.5*"/>
    <thead>
     <row>
      <entry>User Login</entry>
      <entry>Roles</entry>
      <entry>Group</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry><codeph>user1</codeph> / <codeph>user1</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>dev</entry>
     </row>
     <row>
      <entry><codeph>user2</codeph> / <codeph>user2</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>dev</entry>
     </row>
     <row>
      <entry><codeph>user3</codeph> / <codeph>user3</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>test</entry>
     </row>
     <row>
      <entry><codeph>user4</codeph> / <codeph>user4</codeph></entry>
      <entry>Manager and Creator</entry>
      <entry>test</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Descriptions are used in MapR DB JSON origin and
   destination</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_rcs_gcd_ty">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>MapR DB JSON Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Table Name</entry>
      <entry>
       <p id="MapRDBJSON_Table1">If you do not include a path to the table, the stage assumes that
        the table exists in the user's home directory. For example, <codeph>/user/&lt;user
         name>/&lt;table name></codeph>.</p>
       <p id="MapRDBJSON_Table2">You can include a path relative to the user's home directory or an absolute path when you
        enter the table name. For tables in a default cluster, specify the absolute path as
         <codeph>/&lt;table path></codeph>. For tables in a specific cluster, specify the absolute
        path as <codeph>/mapr/&lt;cluster name>/&lt;table path></codeph>.</p>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">The following table is used by the Salesforce origin and the
   Salesforce Lookup processor</draft-comment>
  <table
   frame="all" rowsep="1" colsep="1" id="table_SalesforceFieldAtts">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Salesforce Field Attribute</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>&lt;Salesforce prefix>salesforceType</entry>
      <entry id="entry_JDBCType">Provides the original Salesforce data type
       for the field.</entry>
     </row>
     <row>
      <entry>&lt;Salesforce prefix>length</entry>
      <entry id="entry_JDBCprecision">Provides the original length for all
       string and textarea fields. </entry>
     </row>
     <row>
      <entry>&lt;Salesforce prefix>precision</entry>
      <entry>Provides the original precision for all double fields. </entry>
     </row>
     <row>
      <entry>&lt;Salesforce prefix>scale</entry>
      <entry id="entry_JDBCscale">Provides the original scale for all double
       fields. </entry>
     </row>
     <row>
      <entry>&lt;Salesforce prefix>digits</entry>
      <entry>Provides the maximum number of digits for all integer
       fields.</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">These rows are used by the Groovy, JavaScript, Jython
   Evaluators</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_sgr_tmr_kz">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-InitScript">
      <entry>Init Script</entry>
      <entry>Optional initialization script to use in the processor.<p>Use to set up connections or
       resources required by the processor. Runs once when the pipeline starts.</p></entry>
     </row>
    <row id="row-Script">
     <entry>Script</entry>
     <entry>Main processing script to use in the processor.<p>Runs for each record or each batch of
       data, based on the configured processing mode.</p></entry>
    </row>
    <row id="row-DestroyScript">
     <entry>Destroy Script</entry>
     <entry>Optional destroy script to use in the processor.<p>Use to close any connections or
       resources required by the processor. Runs once when the pipeline stops.</p></entry>
    </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">These rows are used by the MQTT Subscriber and MQTT
   Publisher</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_yhx_fwr_mz">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>MQTT Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row_MQTTBrokerURL">
      <entry>Broker URL</entry>
      <entry>MQTT Broker URL. Enter in the following
        format:<codeblock>&lt;tcp | ssl>://&lt;hostname>:&lt;port></codeblock><p>Use ssl for secure
        connections to the broker.</p><p>For
       example:<codeblock>tcp://localhost:1883</codeblock></p></entry>
     </row>
     <row id="row_MQTTClientID">
      <entry>Client ID</entry>
      <entry>MQTT Client ID. The ID must be unique across all clients connecting to the same
        broker.<p>You can define an expression that evaluates to the client ID. For example, you can
        enter the following expression to use the unique pipeline ID as the client
        ID:<codeblock>${pipeline:id()}</codeblock></p></entry>
     </row>
     <row id="row_MQTTQoS">
      <entry>Quality of Service</entry>
      <entry>Determines the quality of service level used to guarantee message delivery:<ul
        id="ul_gc5_mcr_mz">
        <li>At Most Once (0)</li>
        <li>At Least Once (1)</li>
        <li>Exactly Once (2)</li>
       </ul><p>For more information, see the <xref
         href="http://www.hivemq.com/blog/mqtt-essentials-part-6-mqtt-quality-of-service-levels"
         format="html" scope="external">HiveMQ documentation on quality of service
        levels</xref>.</p></entry>
     </row>
     <row id="row_MQTTUseCredentials">
      <entry>Use Credentials</entry>
      <entry>Enables entering credentials on the Credentials tab. Use when the MQTT broker requires
       a user name and password.</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Rows in this table are used in MongoDB origin, MongoDB Oplog
   origin, and MongoDB destination</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_mh4_bxs_ns">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3*"/>
    <thead>
     <row>
      <entry>MongoDB Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-MongoSingleMode">
      <entry>Enable Single Mode</entry>
      <entry>Select to connect to a single MongoDB server or node. If
       multiple nodes are defined in the connection string, the
       origin connects only to the first node. <p>Use this option
        with care. If the origin cannot connect or the
        connection fails, the pipeline stops. </p></entry>
     </row>
     <row id="row-MongoDatabase">
      <entry>Database</entry>
      <entry>Name of the MongoDB database.</entry>
     </row>
     <row id="row-MongoCollection">
      <entry>Collection</entry>
      <entry>Name of the MongoDB collection to use.</entry>
     </row>
     <row id="row-MongoCappedCollection">
      <entry>Capped Collection</entry>
      <entry>The collection is capped. Clear this option to read an
       uncapped collection.</entry>
     </row>
     <row id="row-MongoInitialOffset">
      <entry>Initial Offset</entry>
      <entry>Initial offset to use to begin reading. When using an
       Object ID field as the offset field, enter a timestamp with
       the following format: <codeph>YYYY-MM-DD hh:mm:ss</codeph>.
       When using a string field, enter the string to
       use.<p>Default is: 2015-01-01 00:00:00</p></entry>
     </row>
     <row id="row-MongoOffsetField">
      <entry>Offset Field Type</entry>
      <entry>Data type of the offset field. Use Object ID for Object
       ID fields. Use String for string offset fields. <p>Default
        is Object ID.</p></entry>
     </row>
     <row id="row-MongoOffset">
      <entry>Offset Field <xref
       href="../Origins/MongoDB-OffsetField.dita">
       <image href="../Graphics/icon_moreInfo.png" scale="10"
        id="image_k2w_rsd_3t"/></xref>
      </entry>
      <entry>Field to use to track reads. Default is the _id field.
       <p>You can use a nested offset field, such as o._id, or
        any Object ID field or any string field. Results are not
        guaranteed for anything but the _id field.</p></entry>
     </row>
     <row id="row-Mongo-BatchSize">
      <entry>Batch Size (records)</entry>
      <entry>Maximum number of records allowed in a batch.</entry>
     </row>
     <row id="row-MongoBatchWait">
      <entry>Max Batch Wait Time <xref
       href="../Origins/BatchSizeWaitTime.dita">
       <image href="../Graphics/icon_moreInfo.png" scale="10"
       /></xref>
      </entry>
      <entry id="entry-Mongo-BatchWaitTime">Amount of time the origin
       will wait to fill a batch before sending an empty batch.
      </entry>
     </row>
     <row id="row-MongoReadPreference">
      <entry>Read Preference <xref
       href="../Origins/MongoDB-ReadPreference.dita">
       <image href="../Graphics/icon_moreInfo.png" scale="10"
        id="image_o23_1rd_ww"/></xref>
      </entry>
      <entry id="entry-Mongo-ReadPref">Determines how the origin reads
       data from different members of the MongoDB replica
       set.</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Rows in this table are used in MongoDB origin, MongoDB Oplog
   origin, and MongoDB destination</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_bdl_csd_3t">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Credentials</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row-MongoAuthType">
      <entry>Authentication Type</entry>
      <entry>Authentication used by the MongoDB server:
       Username/Password or LDAP.</entry>
     </row>
     <row id="row-MongoUsername">
      <entry>Username</entry>
      <entry>MongoDB or LDAP user name.</entry>
     </row>
     <row id="row-MongoPassword">
      <entry>Password</entry>
      <entry>MongoDB or LDAP password.<note
        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/SDCDPM_Tip_Usernames"/></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Includes reusable table entry for regsitering SDCs with
   DPM.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_bvv_rz2_31b">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry><ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/> Base URL</entry>
      <entry id="entry_DPMURL">Enter the appropriate URL:<ul id="ul_DPMURL">
        <li>For <ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/> cloud, use
          <codeph>https://cloud.streamsets.com</codeph>.</li>
        <li>For <ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"/> on-premises, use
         the URL provided by your system administrator. For example,
          <codeph>https://&lt;hostname>:18631</codeph>.</li>
       </ul></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">Table below is conref'd in full in the Configuring Data
   Collector topic. All rows except for the first two are conref'd in the Configuring SDC to Publish
   Metadata topic</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_uhx_g25_p1b">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="2.8*"/>
    <colspec colname="c2" colnum="2" colwidth="3.2*"/>
    <thead>
     <row>
      <entry>Metadata Publisher Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>lineage.publishers</entry>
      <entry>Application to publish metadata to. Do not change the default value of
       <codeph>navigator</codeph>.<p>Uncomment the property to enable metadata
        publishing.</p></entry>
     </row>
     <row>
      <entry>lineage.publisher.navigator.def</entry>
      <entry>Stage library name used to connect to Cloudera Navigator, entered in the following
        format:<codeblock>&lt;stage library name>::
com_streamsets_pipeline_stage_plugin_navigator_NavigatorLineagePublisher</codeblock><p>Uncomment
        the property and define the appropriate Cloudera CDH stage library version to enable
        metadata publishing. <ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> can
        use the Cloudera CDH stage library version 5.10 or higher to connect to Cloudera Navigator.
        Do not modify the second half of the property value.</p><p>Default is
         <codeph>streamsets-datacollector-cdh_5_11-lib</codeph>.</p></entry>
     </row>
     <row id="row_navAppURL">
      <entry>lineage.publisher.navigator.config.application_url</entry>
      <entry>URL to <ph conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>. For
       example:<codeblock conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/SDCBaseURL"/></entry>
     </row>
     <row id="row_navURL">
      <entry>lineage.publisher.navigator.config.navigator_url</entry>
      <entry>URL to the Cloudera Navigator UI. For example:<p>
        <codeblock>http://&lt;Navigator Metadata Server host>:&lt;port number>/</codeblock>
       </p><p>
        <codeph>&lt;Navigator Metadata Server host></codeph> is the name of the host on which you
        are running the Navigator Metadata Server role. <codeph>&lt;port number></codeph> is the
        port configured for the role. The default port number is 7187.</p></entry>
     </row>
     <row id="row_NavNamespace">
      <entry>lineage.publisher.navigator.config.namespace</entry>
      <entry>Namespace that Cloudera Navigator uses to uniquely identify <ph
       conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> properties.<p>Default is
        <codeph>sdc</codeph>.</p></entry>
     </row>
     <row id="row_NavUserName">
      <entry>lineage.publisher.navigator.config.username</entry>
      <entry>User name to connect to Cloudera Navigator. </entry>
     </row>
     <row id="row_NavPassword">
      <entry>lineage.publisher.navigator.config.password</entry>
      <entry>Password for the Cloudera Navigator account.<p>Enter one of the following:<ul
         id="ul_zlz_zlk_fbb">
         <li>Password value.</li>
         <li>File that contains the password value. For increased security, store the password in a
          separate file located in the <systemoutput>$SDC_CONF</systemoutput> directory and
          reference the file in this property as follows: ${file("&lt;filename>")}. For
           example:<codeblock>lineage.publisher.navigator.config.password=${file("NavigatorPassword.txt")}</codeblock><p>For
           more information, see <xref
            href="../Configuration/StoringLoadingSensitiveInformation.dita#concept_aqc_dbt_zr"
           />.</p></li>
        </ul></p></entry>
     </row>
     <row id="row_NavAutoCommit">
      <entry>lineage.publisher.navigator.config.autocommit</entry>
      <entry>Enables Cloudera Navigator to immediately process the published metadata.<p>Setting to
       true can use a large number of resources on the Cloudera Navigator machine. Set to true only
       in a development or test environment.</p><p>Default is false. </p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <draft-comment author="alisontaylor">The following table is conref'd in the Vault Credential
   Stores topic. All rows but the first are conref'd in the Accessing Hashicorp Vault Secrets
   topic.</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_spl_ycl_r1b">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="2.3*"/>
    <colspec colname="c2" colnum="2" colwidth="2.7*"/>
    <thead>
     <row>
      <entry>Vault Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>credentialStore.vault.config.pathKey.separator</entry>
      <entry>Optional. Separator to use for the path and key values in the credential name argument
       used by the <xref href="../Expression_Language/CredentialFunctions.dita#concept_yvc_3qs_r1b"
        >credential functions</xref>.<p>You use the following format for the name
        argument:<codeblock>&lt;path>&lt;separator>&lt;key></codeblock></p><p>For example, if you
        keep the default ampersand (&amp;), the format for the name argument
        is:<codeblock>&lt;path>&amp;&lt;key></codeblock></p></entry>
     </row>
     <row id="row_VaultAddr">
      <entry>credentialStore.vault.config.addr</entry>
      <entry>Required. Vault server URL entered in the following
        format:<codeblock>https://&lt;host name>:&lt;port number></codeblock><p>Use HTTPS to avoid
        unencrypted communication.</p></entry>
     </row>
     <row id="row_VaultRoleID">
      <entry>credentialStore.vault.config.role.id</entry>
      <entry>Required. Vault Role ID that <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> uses to
       authenticate with Vault. The Role ID is configured within Vault by your Vault administrator.
        <p>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
        Vault integration relies on Vault's App Role authentication backend.<note type="important"
         >The App ID authentication backend has been deprecated by Hashicorp and will be removed in
         a future release. As a result, do not configure the
          <codeph>credentialStore.vault.config.app.id</codeph> property for new
         installations.</note></p></entry>
     </row>
      <row id="row_VaultSecretID">
      <entry>credentialStore.vault.config.secret.id</entry>
      <entry>Required. Vault Secret ID that <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> uses to
       authenticate with Vault. The Secret ID is configured within Vault by your Vault
       administrator. <p>Enter one of the following:<ul id="ul_i4d_hct_51b">
         <li>Secret ID value.</li>
         <li>File that contains the Secret ID value. For increased security, store the Secret ID in
          a separate file and reference the file in the
           <codeph>$SDC_CONF/credential-stores.properties</codeph> file as follows:
          ${file("&lt;filename>")}. <p>By default, the file name is <codeph>vault-secret-id</codeph>
           and expected in the <systemoutput>$SDC_CONF</systemoutput> directory. For more
           information, see <xref
            href="../Configuration/StoringLoadingSensitiveInformation.dita#concept_aqc_dbt_zr"
           />.</p></li>
        </ul></p></entry>
     </row>
     <row id="row_VaultRenewal">
      <entry>credentialStore.vault.config.lease.renewal.interval.sec</entry>
      <entry>Optional. Seconds to wait before checking for leases that need renewal. <p>Default is
        60. </p></entry>
     </row>
     <row id="row_VaultExpire">
      <entry>credentialStore.vault.config.lease.expiration.buffer.sec</entry>
      <entry>Optional. Buffer for expiring leases. <ph
        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> renews
       leases that expire in less than the specified number of seconds.<p>Default is 120.
       </p></entry>
     </row>
     <row id="row_VaultOpenTimeout">
      <entry>credentialStore.vault.config.open.timeout</entry>
      <entry>Optional. Timeout to establish an HTTP connection to Vault in milliseconds. <p>Default
        is 0 for no limit. </p></entry>
     </row>
     <row id="row_VaultProxyAddr">
      <entry>credentialStore.vault.config.proxy.address</entry>
      <entry>Optional. Proxy URL. Configure to use a proxy to access Vault. </entry>
     </row>
     <row id="row_VaultProxyPort">
      <entry>credentialStore.vault.config.proxy.port</entry>
      <entry>Optional. Proxy port. Configure to use a proxy to access Vault. </entry>
     </row>
     <row id="row_VaultProxyUser">
      <entry>credentialStore.vault.config.proxy.username</entry>
      <entry>Optional. Proxy username. Configure to use a proxy to access Vault. </entry>
     </row>
     <row id="row_VaultProxyPwd">
      <entry>credentialStore.vault.config.proxy.password</entry>
      <entry>Optional. Proxy password. Configure to use a proxy to access Vault. </entry>
     </row>
     <row id="row_VaultReadTimeout">
      <entry>credentialStore.vault.config.read.timeout</entry>
      <entry>Optional. Milliseconds to wait for data before timing out. <p>Default is 0 for no
        limit. </p></entry>
     </row>
     <row id="row_VaultSSL">
      <entry>credentialStore.vault.config.ssl.enabled.protocols</entry>
      <entry>Optional. SSL/TLS-enabled protocols. Versions TLSv1.2 or later are
        recommended.<p>Default is TLSv1.2,TLSv1.3.</p></entry>
     </row>
     <row id="row_VaultTruststore">
      <entry>credentialStore.vault.config.ssl.truststore.file</entry>
      <entry>Optional. Path to a Java truststore file. Required when using a private CA or
       certificates not trusted by the Java default truststore. </entry>
     </row>
     <row id="row_VaultTruststorePwd">
      <entry>credentialStore.vault.config.ssl.truststore.password</entry>
      <entry>Optional. Password for the truststore file. </entry>
     </row>
     <row id="row_VaultSSLVerify">
      <entry>credentialStore.vault.config.ssl.verify</entry>
      <entry>Optional. Whether to verify that the Vault server hostname matches its
        certificate.<p>Default is true. False is not recommended.</p></entry>
     </row>
     <row id="row_VaultSSLTimeout">
      <entry>credentialStore.vault.config.ssl.timeout</entry>
      <entry>Optional. Timeout for the SSL/TLS handshake in milliseconds. <p>Default is 0 for no
        limit. </p></entry>
     </row>
     <row id="row_VaultTimeout">
      <entry>credentialStore.vault.config.timeout</entry>
      <entry>Optional. Timeout to read from Vault in milliseconds, after a connection has been
        established.<p>Default is 0 for no limit. </p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 <draft-comment author="alisontaylor">The following rows are used in Kudu Lookup processor and Kudu
  destination</draft-comment>
  <table frame="all" rowsep="1" colsep="1" id="table_sqr_2qn_51b">
   <tgroup cols="2">
    <colspec colname="c1" colnum="1" colwidth="1.5*"/>
    <colspec colname="c2" colnum="2" colwidth="3.5*"/>
    <thead>
     <row>
      <entry>Kudu Property</entry>
      <entry>Description</entry>
     </row>
    </thead>
    <tbody>
     <row id="row_KuduMasters">
      <entry>Kudu Masters</entry>
      <entry>Comma-separated list of connection information for the Kudu masters to use. Use the
       following format: <codeblock>&lt;host>:&lt;port></codeblock></entry>
     </row>
     <row id="row_KuduTableName">
      <entry>Table Name</entry>
      <entry>Table to write to. Enter one of the following: <p>
        <ul id="ul_KuduTableName">
         <li>Name of an existing Kudu table. If the table doesn't exist, the pipeline fails to
          start.</li>
         <li>Expression that evaluates to the name of an existing Kudu table. For example, if the
          table name is stored in the "tableName" record attribute, enter the following
          expression:<codeblock>${record:attribute('tableName')}</codeblock>If the table doesn't
          exist, the records are treated as error records.</li>
        </ul>
       </p><p id="p_KuduTableNote">
        <note>Impala creates Kudu tables with the following naming convention:
          <codeblock>impala:&lt;database name>.&lt;table name></codeblock><p>When using a table
          created by Impala, be sure to use the entire table name. </p></note>
       </p></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </conbody>
</concept>
