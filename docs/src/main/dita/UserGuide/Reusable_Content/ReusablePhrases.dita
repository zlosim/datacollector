<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_vhs_5tz_xp">
      <title>Reusable phrases</title>
      <shortdesc>Use the following reusable phrases for conrefs in the book. <draft-comment
                  author="Loretta">Company name - not really using this</draft-comment></shortdesc>
      <conbody>
            <p>Company name: <ph id="company">StreamSets</ph></p>
            <draft-comment author="Loretta">product name: trying to consistently use
                  this</draft-comment>
            <p>Full, init cap version of the product name: <ph id="pName-long">Data
                  Collector</ph></p>
            <p>DPM long only: <ph id="DPM-LongOnly">Dataflow Performance Manager</ph></p>
            <p>DPM short: <ph id="DPM-short">DPM</ph></p>
            <p>Full, init cap version of the edge: <ph id="Edge-Long">Data Collector Edge</ph></p>
            <p>Full, init cap version of the edge in plural form: <ph id="Edge-Long-Plural">Edge Data Collectors</ph></p>
            <p>Shortened version of edge: <ph id="Edge-Short">SDC Edge</ph></p>
            <p>
                  <draft-comment author="Loretta">The old Hive Drift Solution
                        rename.</draft-comment>
            </p>
            <p><ph id="HiveDrift-ph">Drift Synchronization Solution for Hive</ph></p>
            <draft-comment author="alisontaylor">The following ph's for the current version are used
                  in the install and upgrade steps:</draft-comment>
            <p><ph id="version">3.0.0.0</ph></p>
            <p><ph id="version_DirName">3000</ph></p>
            <draft-comment author="alisontaylor">Supported operating systems for install
                  instructions</draft-comment>
            <p><ph id="ph_LinuxEL6">CentOS 6, Red Hat Enterprise Linux 6, or Ubuntu 14.04
                  LTS</ph></p>
            <p><ph id="ph_LinuxEL6_RPM">CentOS 6 or Red Hat Enterprise Linux 6</ph></p>
            <p><ph id="ph_LinuxEL7">CentOS 7, Red Hat Enterprise Linux 7, or Ubuntu 16.04
                  LTS</ph></p>
            <p><ph id="ph_LinuxEL7_RPM">CentOS 7 or Red Hat Enterprise Linux 7</ph></p>
            <draft-comment author="alisontaylor">SDC service shutdown instructions used in
                  Uninstallation, Upgrade, and Shutdown task in Admin chapter. </draft-comment>
            <p id="shutdownServiceLinuxEL6">For <ph conref="#concept_vhs_5tz_xp/ph_LinuxEL6"/>, use: <codeph>service sdc
                        stop</codeph></p>
            <p id="shutdownServiceLinuxEL7">For <ph conref="#concept_vhs_5tz_xp/ph_LinuxEL7"/>, use: <codeph>systemctl stop
                        sdc</codeph></p>
            <p id="shutdownServiceLinxuEL6_RPM">For <ph conref="#concept_vhs_5tz_xp/ph_LinuxEL6_RPM"/>, use: <codeph>service sdc
                        stop</codeph></p>
            <p id="shutdownServiceLinxuEL7_RPM">For <ph conref="#concept_vhs_5tz_xp/ph_LinuxEL7_RPM"/>, use: <codeph>systemctl stop
                        sdc</codeph></p>
            <draft-comment author="alisontaylor">SDC UI shutdown instructions used in
                  Uninstallation, Upgrade, and Shutdown task in Admin chapter. </draft-comment>
            <p id="shutdownUI">To use the <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                  /> UI, click <menucascade>
                        <uicontrol>Administration</uicontrol>
                        <uicontrol> Shut Down</uicontrol>
                  </menucascade>. When the confirmation dialog box appears, click
                        <uicontrol>Yes</uicontrol>.</p>
            <p>
                  <draft-comment author="Loretta" >Used in Configuring the SDC
                        Environment and in MapR Prereq > Install Client Libraries</draft-comment>
            </p>
            <p>
                  <ul id="ul-SDCenvfiles">
                        <li><codeph>$SDC_DIST/libexec/sdc-env.sh</codeph> - Used when you start <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> manually from the command line or when you start Data Collector as
                              a service on operating systems that use the systemd init system -
                              including <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ph_LinuxEL7"
                              />. </li>
                        <li><codeph>$SDC_DIST/libexec/sdcd-env.sh</codeph> - Used when you start <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> as a service on operating systems that use the SysV init system -
                              including <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ph_LinuxEL6"
                              />.</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">
                  <p>For SDC/DPM xrefs to Configuring SDC.</p>
            </draft-comment>
            <p><ph id="SDCDPM_ConfigSDC">
                        <ph product="SDC">For more information, see <xref
                                    href="../Configuration/ConfiguringDataCollector.dita#task_lxk_kjw_1r"
                              /></ph><ph product="DPM">For more information, see <xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/DCConfig.html"
                                    format="html" scope="external">Configuring Data Collector</xref>
                              in the <ph conref="#concept_vhs_5tz_xp/pName-long"/>
                              documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">For SDC/DPM xrefs to SDC Environment
                        Config.</draft-comment>
            </p>
            <p><ph id="SDCDPM_SDCenvConfigs">For more information about environment variables, see
                              <xref
                              href="../Configuration/DCEnvironmentConfig.dita#concept_rng_qym_qr"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/DCEnvironmentConfig.html"
                                    format="html" scope="external">Data Collector Environment
                                    Configuration</xref> in the <ph
                                    conref="#concept_vhs_5tz_xp/pName-long"/>
                        documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM Data Collector Logs</draft-comment>
            </p>
            <p><ph id="SDCDPM_SDCLogs">For details, see <xref
                              href="../Administration/ViewingLogData.dita#task_gbm_s3k_br"
                              product="SDC"/><ph product="DPM">Viewing Data Collector Logs in the
                                    <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> documentation</ph>.</ph></p>
            <draft-comment author="Loretta">SDC/DPM Data Collector Log Level</draft-comment>
            <p><ph id="SDCDPM_LogLevel">For details, see <xref
                              href="../Administration/ModifyingLogLevel.dita#task_lkv_g2f_wy"
                              product="SDC"/><ph product="DPM">Modifying the Log Level in the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM external libraries</draft-comment>
            </p>
            <p><ph id="SDCDPM_ExtLibs">For information about installing additional drivers, see
                              <xref href="../Configuration/ExternalLibs.dita#concept_pdv_qlw_ft"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/ExternalLibs.html%23concept_pdv_qlw_ft"
                                    format="html" scope="external">Install External Libraries</xref>
                              in the <ph conref="#concept_vhs_5tz_xp/pName-long"/>
                              documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM Hadoop Impersonation Mode</draft-comment>
            </p>
            <p><ph id="SDCDPM_HDFSImperMode">For more information, see <xref
                              href="../Configuration/HadoopImpersonationMode.dita#concept_pmr_sy5_nz"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/DCConfig.html%23concept_pmr_sy5_nz"
                                    format="html" scope="external">Hadoop Impersonation Mode</xref>
                              in the <ph conref="#concept_vhs_5tz_xp/pName-long"/>
                              documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM HTTPS for Cluster
                        Pipelines</draft-comment>
            </p>
            <p><ph id="SDCDPM_httpsClusterPipes">For more information, see <xref
                              href="../Configuration/HTTP_protocolsCluster.dita#concept_rdt_h54_cw"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/DCConfig.html%23concept_rdt_h54_cw"
                                    format="html" scope="external">Configuring HTTPS for Cluster
                                    Pipelines</xref> in the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM Java Heap Size</draft-comment>
            </p>
            <p><ph id="SDCDPM-JavaHeapSize">For information about configuring the <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> heap size, see <xref
                              href="../Configuration/JavaHeapSize.dita#concept_mdc_shg_qr"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/DCEnvironmentConfig.html%23concept_mdc_shg_qr"
                                    format="html" scope="external">Java Heap Size</xref> in the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM Kerberos Authentication</draft-comment>
            </p>
            <p><ph id="SDCDPM-Kerberos">For more information about enabling Kerberos authentication
                        for <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        />, see <xref href="../Configuration/Kerberos.dita#concept_hnm_n4l_xs"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/DCConfig.html%23concept_hnm_n4l_xs"
                                    format="html" scope="external">Kerberos Authentication</xref> in
                              the Data Collector documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM MapR Prereqs</draft-comment>
            </p>
            <p><ph id="SDCDPM_MapRPrereq">For more information, see <xref
                              href="../Installation/MapR-Prerequisites.dita#concept_jgs_qpg_2v"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Installation/MapR-Prerequisites.html%23concept_jgs_qpg_2v"
                                    format="html" scope="external">MapR Prerequisites</xref> in the
                                    <ph conref="#concept_vhs_5tz_xp/pName-long"/>
                        documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">SDC/DPM Pipeline Monitoring
                        Overview</draft-comment>
            </p>
            <p><ph id="SDCDPM-PipelineMonitoring">For more information about monitoring pipelines,
                        see <xref
                              href="../Pipeline_Monitoring/PipelineMonitoring.dita#concept_hsp_tnt_lq"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Pipeline_Monitoring/PipelineMonitoring_title.html%23concept_hsp_tnt_lq"
                                    format="html" scope="external">Pipeline Monitoring
                                    Overview</xref> in the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">Data Collector max buffer size for records,
                        overridden by the parser.limit SDC property. used in Configuring SDC and
                        origins > Max Record Size.</draft-comment>
            </p>
            <p><ph id="ph-DefaultMaxRecordSize">1048576 bytes</ph></p>
            <draft-comment author="alisontaylor">The following codeblock is used in all launching
                  SDC topics</draft-comment>
            <codeblock id="SDCBaseURL">http://&lt;hostname>:18630/</codeblock>
            <p>
                  <draft-comment author="alisontaylor">The following p is used in the registering
                        and unregistering from DPM topics.</draft-comment>
            </p>
            <p id="p-DPMUserNameFormat">
                  <codeblock>&lt;ID>@&lt;organization ID></codeblock>
            </p>
            <p>
                  <draft-comment author="Loretta">The following ph is used by the registration DPM
                        topics. </draft-comment>
            </p>
            <p><ph id="ph-DPMTransferPerm">To enable <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"
                        /> users to work with pipelines, transfer permissions to the <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"
                        /> users and groups. For more information, see <xref
                              href="../DPM/TransferPermissions.dita"/>.</ph></p>
            <p>
                  <draft-comment author="Loretta">The following ph is used by the unregistration
                        from DPM topics. </draft-comment>
            </p>
            <p><ph id="ph-SDCuser-uninstall">After restarting <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        />, use your <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> user account to log in. </ph></p>
            <p>
                  <draft-comment author="Loretta">The following two ph is used by the unregistration
                        from DPM topics. </draft-comment>
            </p>
            <p><ph id="ph-TransferPerms-Unregister">To ensure that users can access pipelines,
                        transfer pipeline permissions from the obsolete <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/DPM-short"
                        /> users and groups back to <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> users and groups. Or, you can edit pipeline permissions
                        individually.</ph></p>
            <p><ph id="ph-DPMperm-xrefs">For more information about transferring permissions, see
                              <xref
                              href="../Configuration/TransferringPermissions.dita#task_wkr_gdd_1z"
                        />. For information about configuring individual pipeline permissions, see
                              <xref
                              href="../Pipeline_Maintenance/SharingPipelines.dita#concept_jrg_1vy_wy"
                        />.</ph></p>
            <draft-comment author="alisontaylor">The following number is used in Installation
                  Requirements and Configuring the Open File Limit</draft-comment>
            <p><ph id="ph-MinFileDescriptors">32768</ph></p>
            <p>
                  <draft-comment author="Loretta">Note &amp; ph used in manual &amp; package manager
                        steps for Install External Libraries</draft-comment>
            </p>
            <p><note id="note-extLib-Allstagelibs">If you use multiple stage libraries for a
                        particular stage, and you want to use an external library with all stage
                        libraries, you must install the external library for each stage library.
                        </note><ph id="ph-SparkExample">For example, say you want to use an external
                        library with the Spark Evaluator processor, but you use two versions of the
                        processor - each from a different stage library. To make the external
                        library available to both processor versions, you must upload the external
                        library to both stage libraries.</ph></p>
            <p>
                  <draft-comment author="Loretta">Used in stage overviews that use the shared TLS
                        properties.</draft-comment>
            </p>
            <p><ph id="TLS-overview-ph">You can also configure SSL/TLS properties, including default
                        transport protocols and cipher suites.</ph></p>
            <p>
                  <draft-comment author="Loretta">default TLS protocol, used in reusable TLS
                        topics</draft-comment>
            </p>
            <p><ph id="TLS-defaultProtocol-ph">TLSv1.2</ph></p>
            <p>
                  <draft-comment author="Loretta">The following is used in TCP Server doc in a few
                        places.</draft-comment>
            </p>
            <p><ph id="JavaSyntax-ph">Specify one or more characters using the Java Unicode syntax,
                        as follows: <codeph>\u&lt;Unicode character code></codeph>. To specify
                        multiple characters, repeat the syntax for each character, as follows:
                              <codeph>\u&lt;Unicode character code>\u&lt;Unicode character
                              code>\u&lt;Unicode character code></codeph>.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>ph-VaultFunctions-Usage</uicontrol> -
                        used in Vault-Step3-CallVault.dita</draft-comment>
            </p>
            <p><ph id="ph-VaultFunctions-Usage">username, password, and similar properties such as
                        AWS access key IDs and secret access keys. You can also use the functions in
                        HTTP headers and bodies when using HTTPS.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>ph-VaultOneFunction-Usage</uicontrol> -
                        used in Misc Functions for each Vault function</draft-comment>
            </p>
            <p><ph id="ph-VaultOneFunction-Usage">username, password, and similar properties such as
                        AWS access key IDs and secret access keys. You can also use the function in
                        HTTP headers and bodies when using HTTPS.</ph></p>
            <p>
                  <draft-comment author="Loretta">List of origins that are part of the Basic stage
                        library. Used in CoreInstall_Overview and Available Stage
                        Libraries.</draft-comment>
            </p>
            <p>
                  <ul id="BasicOrigins">
                        <li>CoAP Server</li>
                        <li>Directory</li>
                        <li>File Tail</li>
                        <li>HTTP Client</li>
                        <li>HTTP Server</li>
                        <li>MQTT Subscriber</li>
                        <li>OPC UA Client</li>
                        <li>SDC RPC</li>
                        <li>SFTP/FTP Client</li>
                        <li>TCP Server</li>
                        <li>UDP Multithreaded Source</li>
                        <li>UDP Source</li>
                        <li>WebSocket Client</li>
                        <li>WebSocket Server</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">Core install includes all processors except... </draft-comment>
            <p><ph id="CoreNotProcessor">except the Groovy Evaluator, HBase Lookup, Jython
                        Evaluator, Kudu Lookup, Redis Lookup, and Spark Evaluator.</ph></p>
            <p>
                  <draft-comment author="Loretta">List of destinations that are part of the Core
                        installation. Used in CoreInstall_Overview and Available Stage
                        Libraries.</draft-comment>
            </p>
            <p>
                  <ul id="BasicDestinations">
                        <li>CoAP Client</li>
                        <li>HTTP Client</li>
                        <li>Local FS</li>
                        <li>MQTT Publisher</li>
                        <li>SDC RPC</li>
                        <li>To Error</li>
                        <li>Trash</li>
                        <li>WebSocket Client</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">List of executors in basiclib. Used in
                        CoreInstall_Overview and Available Stage Libraries.</draft-comment>
            </p>
            <p>
                  <ul id="BasicExecutors">
                        <li>Email</li>
                        <li>Pipeline Finisher</li>
                        <li>Shell</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">KafkaSecure - used in KConsumer &amp; Producer
                        encryption areas. Original value: Kafka 0.9.0.0</draft-comment>
            </p>
            <p>Kafka version that supports SSL/TLS and SASL/Kerberos security: <ph id="KafkaSecure"
                        >Kafka version 0.9.0.0 or later</ph></p>
            <draft-comment author="Loretta">Using this in all DC Console topics:</draft-comment>
            <p>
                  <note id="Note-OptionDispay">Some icons and options might not display. The items
                        that display are based on the task that you are performing and roles
                        assigned to your user account. </note>
            </p>
            <p>
                  <draft-comment author="Loretta">
                        <p>The following bullets are used in "Previewing a Single Stage" and
                              "Troubleshooting":</p>
                  </draft-comment>
            </p>
            <ul id="ul_EditPreview">
                  <li>The output data column for an origin.</li>
                  <li>The input data column for processors.</li>
            </ul>
            <p>
                  <draft-comment author="Loretta">WEBHOOKS: The following ph are used in the Webhook
                        overview and Alert Webhook overview:</draft-comment>
            </p>
            <p><ph id="WH-Definition-ph">A webhook is a user-defined HTTP callback - an HTTP request
                        that the pipeline sends automatically when certain actions occur. You can
                        use webhooks to automatically trigger external tasks based on an HTTP
                        request. Tasks can be as simple as sending a message through an application
                        API or as powerful as passing commands to the <ph
                              conref="#concept_vhs_5tz_xp/pName-long"/> command line interface.</ph>
            </p>
            <p id="WH-APIdocNote">
                  <note type="important">You must configure webhooks as expected by the receiving
                        system. For details on how to configure incoming webhooks check the
                        receiving system's documentation. You might also need to enable webhook
                        usage within that system.</note>
            </p>
            <p><ph id="WH-Config-URLmethods-ph">you specify the URL to send the request and the HTTP
                        method to use. Some HTTP methods allow you to include a request body or
                        payload.</ph>
                  <ph id="WH-Config-params-ph">In the payload, you can use parameters to include
                        information about the cause of the trigger, such as</ph> the text of the
                  alert or the latest pipeline state. <ph id="WH-Config-Optional-ph">You can also
                        include request headers, content type, authentication type, username and
                        password as needed.</ph></p>
            <p>
                  <draft-comment author="Loretta">The following bullets are the types of origins
                        that can be reset / that remember where you left off. These are used
                        currently in "Starting a Pipeline" and "Resetting an
                        Origin":</draft-comment>
                  <ul id="ul_saveOffset">
                        <li>Directory</li>
                        <li>Kafka Consumer</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">The following bullets are CSV file types. Used in
                        "Configuring a Directory Origin", Directory-Data Formats and "Configuring
                        the Kafka Consumer" -- Make sure changes to this are copied to the Delimited
                        data format info later in this SAME FILE.</draft-comment>
            </p>
            <p>
                  <ul id="ul_delFileTypes">
                        <li><uicontrol>Default CSV</uicontrol> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>
                        <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>
                        <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel comma-separated
                              file.</li>
                        <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma separated file.</li>
                        <li><uicontrol>Tab-Separated Values</uicontrol> - File that includes
                              tab-separated values.</li>
                        <li><uicontrol>Custom</uicontrol> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">Use the following for invoking the expression editor. So
                  far, using in Config Expression Evaluator, Config Stream Selector, and Expression
                  Editor:</draft-comment>
            <p id="EEditor">Optionally, click <uicontrol>Ctrl + Space Bar</uicontrol> for help with
                  creating the expression. </p>
            <p>
                  <draft-comment author="Loretta">Using the following in the Configuring topic for
                        several processors (Expression Evaluator, Field Converter, Field Hasher, -
                        whichever ones allow wildcard use:</draft-comment>
            </p>
            <p id="wildcard"><ph id="ph-wildcard">You can use the asterisk wildcard to represent
                        array indices and map elements. <xref
                              href="../Pipeline_Configuration/WildcardsArraysMaps.dita#concept_vqr_sqc_wr"
                                    ><image href="../Graphics/icon_moreInfo.png" scale="10"
                                    id="image_ukp_2dt_bz"/>
                        </xref></ph>
            </p>
            <p>
                  <draft-comment author="Loretta">The following tip is used several times in
                        Configuring the Value Replacer.</draft-comment>
            </p>
            <p>
                  <note type="tip" id="tip-asterisk">To use all fields, you can use the asterisk
                        wildcard as follows: <codeph>/*</codeph>. You can also use the asterisk
                        wildcard to represent array indices and map elements. <ph><xref
                                    href="../Pipeline_Configuration/WildcardsArraysMaps.dita#concept_vqr_sqc_wr"
                                          ><image href="../Graphics/icon_moreInfo.png" scale="10"
                                          id="image_phy_rdt_bz"/>
                              </xref></ph>
                  </note>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>DataFormats-ALL</uicontrol> - <b>no
                              longer all - no datagram, no wholefile</b> As of 7/14, Kafka Consumer
                        has datagram, but no one else does. Some origins have whole file. Used in
                        MapR Streams Consumer, possibly others.</draft-comment>
            </p>
            <p>
                  <ul id="DataFormats-ALL">
                        <li>Avro</li>
                        <li>Binary</li>
                        <li>Delimited</li>
                        <li>JSON</li>
                        <li>Log</li>
                        <li>Text</li>
                        <li>Protobuf</li>
                        <li>SDC Record <xref
                                    href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
                                    <image href="../Graphics/icon_moreInfo.png" scale="11"
                                          id="image_wjh_ycl_br"/></xref></li>
                        <li>XML</li>
                  </ul>
            </p>
            <p/>
            <p>
                  <draft-comment author="Loretta">DataFormats-Kafka - used in Kafka Consumer – move
                        back to Kafka Consumer? </draft-comment>
            </p>
            <p>
                  <ul id="DataFormats-Kafka">
                        <li>Avro</li>
                        <li>Binary</li>
                        <li>Datagram</li>
                        <li>Delimited</li>
                        <li>JSON</li>
                        <li>Log</li>
                        <li>Protobuf</li>
                        <li>SDC Record <xref
                                    href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
                                    <image href="../Graphics/icon_moreInfo.png" scale="11"
                                          id="image_drz_1nf_qw"/></xref></li>
                        <li>Text</li>
                        <li>XML</li>
                  </ul>
            </p>
            <p/>
            <p>
                  <draft-comment author="Loretta">ORIGIN DATA FORMATS</draft-comment>
            </p>
            <p>
                  <draft-comment author="Loretta">Used in Max &lt;record size> Data Format property
                        descriptions in reusable steps. </draft-comment>
            </p>
            <p><ph id="ph-MaxRecordSize">This property can be limited by the <ph
                              conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> parser
                        buffer size. For more information, see <xref
                              href="../Origins/MaxRecordSize.dita#concept_svg_2zl_d1b"/>.</ph></p>
            <p>
                  <draft-comment author="Loretta">Avro for Directory and HTTP Server - with no extra
                        configs.</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="O-AvroFile-NoConfigs">
                              <dt>Avro</dt>
                              <dd>Generates a record for every Avro record. Includes a "precision"
                                    and "scale" field attribute for each Decimal field. For more
                                    information about field attributes, see <xref
                                          href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                    />.</dd>
                              <dd>The origin writes the Avro schema to an avroSchema record header
                                    attribute. For more information about record header attributes,
                                    see <xref
                                          href="../Pipeline_Design/RecordHeaderAttributes.dita#concept_wn2_jcz_dz"
                                    />. </dd>
                              <dd>The origin expects each file to contain the Avro schema and uses
                                    the schema to process the Avro data.</dd>
                              <dd>The origin reads files compressed by Avro-supported compression
                                    codecs without requiring additional configuration. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">Avro-Kafka - ONLY FOR KAFKA CONSUMER and Kafka
                        Multitopic Consumer because of the Kafka property.</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-AVRO-Kafka">
                              <dt>Avro</dt>
                              <dd>Generates a record for every message. Includes a "precision" and
                                    "scale" field attribute for each Decimal field. For more
                                    information about field attributes, see <xref
                                          href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                    />.</dd>
                              <dd>The origin writes the Avro schema to an avroSchema record header
                                    attribute. For more information about record header attributes,
                                    see <xref
                                          href="../Pipeline_Design/RecordHeaderAttributes.dita#concept_wn2_jcz_dz"
                                    />. </dd>
                              <dd>You can use one of the following methods to specify the location
                                    of the Avro schema definition:<ul id="ul_npz_4lz_gx">
                                          <li><uicontrol>Message/Data Includes Schema</uicontrol> -
                                                Use the schema in the message.</li>
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration.</li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the origin to look up the schema in the Confluent
                                                Schema Registry by the schema ID embedded in the
                                                message or by the schema ID or subject specified in
                                                the stage configuration.<p>You must specify the
                                                  method that the origin uses to deserialize the
                                                  message. If the Avro schema ID is embedded in each
                                                  message, set the key and value deserializers to
                                                  Confluent on the <uicontrol>Kafka</uicontrol>
                                                  tab.</p></li>
                                    </ul></dd>
                              <dd>Using a schema in the stage configuration or retrieving a schema
                                    from the Confluent Schema Registry overrides any schema that
                                    might be included in the message and can improve
                                    performance.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO - Used by Kafka Consumer and other message
                        origins</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-AVRO">
                              <dt>Avro</dt>
                              <dd>Generates a record for every message. Includes a "precision" and
                                    "scale" field attribute for each Decimal field. For more
                                    information about field attributes, see <xref
                                          href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                    />. </dd>
                              <dd>The origin writes the Avro schema to an avroSchema record header
                                    attribute. For more information about record header attributes,
                                    see <xref
                                          href="../Pipeline_Design/RecordHeaderAttributes.dita#concept_wn2_jcz_dz"
                                    />. </dd>
                              <dd>You can use one of the following methods to specify the location
                                    of the Avro schema definition:<ul id="ul_npz_1lz_kx">
                                          <li><uicontrol>Message/Data Includes Schema</uicontrol> -
                                                Use the schema in the message.</li>
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration. </li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the origin to look up the schema in the Confluent
                                                Schema Registry by the schema ID embedded in the
                                                message or by the schema ID or subject specified in
                                                the stage configuration.</li>
                                    </ul></dd>
                              <dd>Using a schema in the stage configuration or retrieving a schema
                                    from the Confluent Schema Registry overrides any schema that
                                    might be included in the message and can improve
                                    performance.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p/>
            <p>
                  <draft-comment author="Loretta">AVRO-Files - used by Amazon S3 and other file
                        origins except Hadoop FS, and Directory and HTTP Server. Directory and HTTP
                        Server each have standalone content.</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-AVROfiles">
                              <dt>Avro</dt>
                              <dd>Generates a record for every Avro record. Includes a "precision"
                                    and "scale" field attribute for each Decimal field. For more
                                    information about field attributes, see <xref
                                          href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                    />. </dd>
                              <dd>The origin writes the Avro schema to an avroSchema record header
                                    attribute. For more information about record header attributes,
                                    see <xref
                                          href="../Pipeline_Design/RecordHeaderAttributes.dita#concept_wn2_jcz_dz"
                                    />. </dd>
                              <dd>You can use one of the following methods to specify the location
                                    of the Avro schema definition:<ul id="ul_ykg_g5z_kx">
                                          <li><uicontrol>Message/Data Includes Schema</uicontrol> -
                                                Use the schema in the file.</li>
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration. </li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the origin to look up the schema in the Confluent
                                                Schema Registry by the schema ID or subject
                                                specified in the stage configuration.</li>
                                    </ul></dd>
                              <dd>Using a schema in the stage configuration or retrieving a schema
                                    from the Confluent Schema Registry overrides any schema that
                                    might be included in the file and can improve performance.</dd>
                              <dd>The origin reads files compressed by Avro-supported compression
                                    codecs without requiring additional configuration. To enable the
                                    origin to read files compressed by other codecs, use the
                                    compression format property in the stage.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO-HadoopFS - used by Hadoop FS origin - Not
                        adding the non-avro-compression-codec compression info</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="AVRO-HadoopFS">
                              <dt>Avro</dt>
                              <dd>Generates a record for every Avro record. Includes a "precision"
                                    and "scale" field attribute for each Decimal field. For more
                                    information about field attributes, see <xref
                                          href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                                    />.</dd>
                              <dd>The origin writes the Avro schema to an avroSchema record header
                                    attribute. For more information about record header attributes,
                                    see <xref
                                          href="../Pipeline_Design/RecordHeaderAttributes.dita#concept_wn2_jcz_dz"
                                    />. </dd>
                              <dd>You can use one of the following methods to specify the location
                                    of the Avro schema definition:<ul id="ul_v5j_gvz_kx">
                                          <li><uicontrol>Message/Data Includes Schema</uicontrol> -
                                                Use the schema in the file.</li>
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration.</li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the origin to look up the schema in the Confluent
                                                Schema Registry by the schema ID or subject
                                                specified in the stage configuration.</li>
                                    </ul></dd>
                              <dd>Using a schema in the stage configuration or retrieving a schema
                                    from the Confluent Schema Registry overrides any schema that
                                    might be included in the file and can improve performance.</dd>
                              <dd>The origin reads files compressed by Avro-supported compression
                                    codecs without requiring additional configuration.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">OriginDF-Binary - used by Kafka Consumer and
                        Kinesis Consumer</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="OriginDF-Binary">
                              <dt>Binary</dt>
                              <dd>Generates a record with a single byte array field at the root of
                                    the record. </dd>
                              <dd>When the data exceeds the user-defined maximum data size, the
                                    origin cannot process the data. Because the record is not
                                    created, the origin cannot pass the record to the pipeline to be
                                    written as an error record. Instead, the origin generates a
                                    stage error. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">O-DF-Datagram - Kafka Consumer only at this
                        point.</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="O-DF-Datagram">
                              <dt>Datagram</dt>
                              <dd>Generates a record for every message. The origin <ph
                                          conref="#concept_vhs_5tz_xp/UDP-messagetypes"/><ul
                                          conref="#concept_vhs_5tz_xp/UDP-syslog" id="ul_fj2_3q4_4x">
                                          <li/>
                                    </ul></dd>
                              <dd><ph conref="#concept_vhs_5tz_xp/ph-NetFlowInfo"/></dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta"> - from Directory. DLEntries are conrefed by all origins
                  except HTTP Client. <note> Content for JSON, Text, and HTML copied to HTTP Client
                        and altered. When making changes here, check there as
                  well.</note></draft-comment>
            <p>
                  <dl id="ORIGIN-DFormats">
                        <dlentry id="OriginDF-DELIM">
                              <dt>Delimited</dt>
                              <dd>Generates a record for each delimited line. You can use the
                                    following delimited format types:<ul id="ul_c12_1k2_gt">
                                          <li><uicontrol>Default CSV</uicontrol> - File that
                                                includes comma-separated values. Ignores empty lines
                                                in the file.</li>
                                          <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated
                                                file that strictly follows RFC4180 guidelines.</li>
                                          <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel
                                                comma-separated file.</li>
                                          <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma
                                                separated file.</li>
                                          <li><uicontrol>Tab-Separated Values</uicontrol> - File
                                                that includes tab-separated values.</li>
                                          <li><uicontrol>Custom</uicontrol> - File that uses
                                                user-defined delimiter, escape, and quote
                                                characters.</li>
                                    </ul></dd>
                              <dd>You can use a list or list-map root field type for delimited data,
                                    optionally including the header information when available. For
                                    more information about the root field types, see <xref
                                          href="../Data_Formats/DelimitedDataRootFieldTypes.dita#concept_zcg_bm4_fs"
                                    />.</dd>
                              <dd>When using a header line, you can allow processing records with
                                    additional columns. The additional columns are named using a
                                    custom prefix and integers in sequential increasing order, such
                                    as _extra_1, _extra_2. When you disallow additional columns when
                                    using a header line, records that include additional columns are
                                    sent to error.</dd>
                              <dd>You can also replace a string constant with null values.</dd>
                              <dd>When a record exceeds the maximum record length defined for the
                                    origin, the origin processes the object based on the error
                                    handling configured for the stage.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-DELIMFILE">
                              <dt>Delimited</dt>
                              <dd>Generates a record for each delimited line. You can use the
                                    following delimited format types:<ul id="ul_hbd_f2p_ht">
                                          <li><uicontrol>Default CSV</uicontrol> - File that
                                                includes comma-separated values. Ignores empty lines
                                                in the file.</li>
                                          <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated
                                                file that strictly follows RFC4180 guidelines.</li>
                                          <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel
                                                comma-separated file.</li>
                                          <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma
                                                separated file.</li>
                                          <li><uicontrol>Tab-Separated Values</uicontrol> - File
                                                that includes tab-separated values.</li>
                                          <li><uicontrol>Custom</uicontrol> - File that uses
                                                user-defined delimiter, escape, and quote
                                                characters.</li>
                                    </ul></dd>
                              <dd>You can use a list or list-map root field type for delimited data,
                                    optionally including the header information when available. For
                                    more information about the root field types, see <xref
                                          href="../Data_Formats/DelimitedDataRootFieldTypes.dita#concept_zcg_bm4_fs"
                                    />.</dd>
                              <dd>When using a header line, you can allow processing records with
                                    additional columns. The additional columns are named using a
                                    custom prefix and integers in sequential increasing order, such
                                    as _extra_1, _extra_2. When you disallow additional columns when
                                    using a header line, records that include additional columns are
                                    sent to error.</dd>
                              <dd>You can also replace a string constant with null values.</dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul id="ul_it3_g2p_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry id="OriginDF-JSON">
                              <dt>JSON</dt>
                              <dd>Generates a record for each JSON object. You can process JSON
                                    files that include multiple JSON objects or a single JSON
                                    array.</dd>
                              <dd>When an object exceeds the maximum object length defined for the
                                    origin, the origin processes the object based on the error
                                    handling configured for the stage. </dd>
                        </dlentry>
                        <dlentry id="OriginDF-JSONFILE">
                              <dt>JSON</dt>
                              <dd>Generates a record for each JSON object. You can process JSON
                                    files that include multiple JSON objects or a single JSON
                                    array.</dd>
                              <dd>When an object exceeds the maximum object length defined for the
                                    origin, the origin cannot continue processing data in the file.
                                    Records already processed from the file are passed to the
                                    pipeline. The behavior of the origin is then based on the error
                                    handling configured for the stage:<ul id="ul_sjn_fxf_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry id="OriginDF-LOG">
                              <dt>Log</dt>
                              <dd>Generates a record for every log line. </dd>
                              <dd>When a line exceeds the user-defined maximum line length, the
                                    origin truncates longer lines. </dd>
                              <dd>You can include the processed log line as a field in the record.
                                    If the log line is truncated, and you request the log line in
                                    the record, the origin includes the truncated line.</dd>
                              <dd>You can define the log format or type to be read.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">OriginDF-ProtoMessage - JMS Consumer, Kinesis
                        Consumer, Kafka Consumer, RabbitMQ</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-ProtoMessage">
                              <dt>Protobuf</dt>
                              <dd>Generates a record for every protobuf message. By default, the
                                    origin assumes messages contain multiple protobuf messages.</dd>
                              <dd>Protobuf messages must match the specified message type and be
                                    described in the descriptor file. </dd>
                              <dd>When the data for a record exceeds 1 MB, the origin cannot
                                    continue processing data in the message. The origin handles the
                                    message based on the stage error handling property and continues
                                    reading the next message. </dd>
                              <dd>For information about generating the descriptor file, see <xref
                                          href="../Data_Formats/Protobuf-Prerequisites.dita"/>.</dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta">OriginDF-ProtoFile - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="OriginDF-ProtoFile">
                        <dt>Protobuf</dt>
                        <dd>Generates a record for every protobuf message. </dd>
                        <dd>Protobuf messages must match the specified message type and be described
                              in the descriptor file. </dd>
                        <dd>When the data for a record exceeds 1 MB, the origin cannot continue
                              processing data in the file. The origin handles the file based on file
                              error handling properties and continues reading the next file. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Data_Formats/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
            </dl>
            <p>
                  <dl>
                        <dlentry id="OriginDF-SDC">
                              <dt>SDC Record</dt>
                              <dd>Generates a record for every record. Use to process records
                                    generated by a <ph
                                          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                    /> pipeline using the SDC Record data format.</dd>
                              <dd>For error records, the origin provides the original record as read
                                    from the origin in the original pipeline, as well as error
                                    information that you can use to correct the record. </dd>
                              <dd>When processing error records, the origin expects the error file
                                    names and contents as generated by the original pipeline.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-TEXT">
                              <dt>Text</dt>
                              <dd>Generates a record for each line of text or for each section of
                                    text based on a custom delimiter.</dd>
                              <dd>When a line or section exceeds the maximum line length defined for
                                    the origin, the origin truncates it. The origin adds a boolean
                                    field named Truncated to indicate if the line was
                                    truncated.</dd>
                              <dd>For more information about processing text with a custom
                                    delimiter, see <xref
                                          href="../Data_Formats/TextCDelim.dita#concept_lg2_gcg_jx"
                                    />.</dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="Loretta">This is for JMS Consumer only.</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-JMS-TEXT">
                              <dt>Text</dt>
                              <dd>Generates a record for each line of text or for each section of
                                    text based on a custom delimiter. Reads text data of the
                                    BytesMessage format. </dd>
                              <dd>When a line or section exceeds the maximum line length defined for
                                    the origin, the origin truncates it. The origin adds a boolean
                                    field named Truncated to indicate if the line was
                                    truncated.</dd>
                              <dd>For more information about processing text with a custom
                                    delimiter, see <xref
                                          href="../Data_Formats/TextCDelim.dita#concept_lg2_gcg_jx"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt/>
                              <dd>
                                    <draft-comment author="Loretta"
                                                ><uicontrol>OriginDF-WFile</uicontrol> - no
                                          checksum. Used by Directory</draft-comment>
                              </dd>
                        </dlentry>
                  </dl>
                  <dl>
                        <dlentry id="OriginDF-WFile">
                              <dt>Whole File</dt>
                              <dd>Streams whole files from the origin system to the destination
                                    system. You can specify a transfer rate or use all available
                                    resources to perform the transfer. </dd>
                              <dd>The origin generates two fields: one for a file reference and one
                                    for file information. For more information, see <xref
                                          href="../Data_Formats/WholeFile.dita#concept_nfc_qkh_xw"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt/>
                              <dd>
                                    <draft-comment author="Loretta"
                                                ><uicontrol>OriginDF-WFile-Checksum</uicontrol> -
                                          used in s3 origin</draft-comment>
                              </dd>
                        </dlentry>
                        <dlentry id="OriginDF-WFile-Checksum">
                              <dt>Whole File</dt>
                              <dd>Streams whole files from the origin system to the destination
                                    system. You can specify a transfer rate or use all available
                                    resources to perform the transfer. </dd>
                              <dd>The origin uses checksums to verify the integrity of data
                                    transmission.</dd>
                              <dd>The origin generates two fields: one for a file reference and one
                                    for file information. For more information, see <xref
                                          href="../Data_Formats/WholeFile.dita#concept_nfc_qkh_xw"
                                    />.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">Two versions of XML below. First for message
                        origins, then file origins - which have the extra error
                        info.</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-XML">
                              <dt>XML</dt>
                              <dd>Generates records based on a user-defined delimiter element. Use
                                    an XML element directly under the root element or define a
                                    simplified XPath expression. If you do not define a delimiter
                                    element, the origin treats the XML file as a single record.</dd>
                              <dd>Generated records include XML attributes and namespace
                                    declarations as fields in the record by default. You can
                                    configure the stage to include them in the record as field
                                    attributes. </dd>
                              <dd>You can include XPath information for each parsed XML element and
                                    XML attribute in field attributes. This also places each
                                    namespace in an xmlns record header attribute. <note><ph
                                                conref="#concept_vhs_5tz_xp/warn_FieldRecHeaderAtts"
                                          /></note></dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin skips the record and continues processing with the next
                                    record. It sends the skipped record to the pipeline for error
                                    handling. </dd>
                              <dd>Use the XML data format to process valid XML documents. For more
                                    information about XML processing, see <xref
                                          href="../Data_Formats/XMLDFormat.dita#concept_lty_42b_dy"
                                    />.</dd>
                              <dd>
                                    <note type="tip"><ph>If you want to process invalid XML
                                                documents, you can try using the text data format
                                                with custom delimiters. For more information, see
                                                  <xref
                                                  href="../Data_Formats/TextCDelim-XMLdata.dita#concept_okt_kmg_jx"
                                                />.</ph>
                                    </note>
                              </dd>
                        </dlentry>
                        <dlentry id="OriginDF-XMLFILE">
                              <dt>XML</dt>
                              <dd>Generates records based on a user-defined delimiter element. Use
                                    an XML element directly under the root element or define a
                                    simplified XPath expression. If you do not define a delimiter
                                    element, the origin treats the XML file as a single record. </dd>
                              <dd>Generated records include XML attributes and namespace
                                    declarations as fields in the record by default. You can
                                    configure the stage to include them in the record as field
                                    attributes. </dd>
                              <dd>You can include XPath information for each parsed XML element and
                                    XML attribute in field attributes. This also places each
                                    namespace in an xmlns record header attribute. </dd>
                              <dd>
                                    <note><ph conref="#concept_vhs_5tz_xp/warn_FieldRecHeaderAtts"
                                          /></note>
                              </dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul id="ul_t2x_xvn_qz">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                              <dd>Use the XML data format to process valid XML documents. For more
                                    information about XML processing, see <xref
                                          href="../Data_Formats/XMLDFormat.dita#concept_lty_42b_dy"
                                    />. <note type="tip"><ph id="ph-p-InvalidXML-CustomDelimXref">If
                                                you want to process invalid XML documents, you can
                                                try using the text data format with custom
                                                delimiters. For more information, see <xref
                                                  href="../Data_Formats/TextCDelim-XMLdata.dita#concept_okt_kmg_jx"
                                                />.</ph>
                                    </note></dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">XML delimiter - Used in XML > Creating Multiple
                        Records and XML Parser overview.</draft-comment>
            </p>
            <p><ph id="XML-ph-MultRecords">To create multiple records from an XML document, you
                        define a delimiter element. You can use an actual XML element name, such as
                              <codeph>msg</codeph>. You can also use a simplified XPath expression
                        to specify the location of the delimiter element, such as
                              <codeph>/root/*/msg</codeph>.</ph></p>
            <p id="XML-p-MultRec2"><ph id="XML-ph-MultRec2">Use an XML element when the element
                        resides directly under the root node. Use a simplified XPath expression to
                        access data deeper in the XML document.</ph></p>
            <p>
                  <draft-comment author="Loretta">XML syntax - The following ph and code are used in
                        the XPath Syntax and Node Predicates topics.</draft-comment>
            </p>
            <p><ph id="XML-ph-PosPredicate">The position predicate indicates the instance of the
                        element to use in the file. Use a position predicate when the element
                        appears multiple times in a file, and you want to use a particular instance
                        based on the position of the instances in the file, e.g. the first, second,
                        or third time the element appears in the file. </ph></p>
            <p id="XML-p-PosPred-Syntax">Use the following syntax to specify a position
                  predicate:<codeblock>/&lt;element>[&lt;position number>]</codeblock></p>
            <p><ph id="XML-ph-AttPredicate">The attribute value predicate limits the data to
                        elements with the specified attribute value. Use the attribute value
                        predicate when you want to specify an element with a particular attribute
                        values or an element that simply has an attribute value defined. </ph></p>
            <p id="XML-p-AttPred-Syntax">Use the following syntax to specify an attribute value
                  predicate:<codeblock>/&lt;element>[@&lt;attribute name>='&lt;attribute value>']</codeblock></p>
            <p><ph id="XML-ph-AttAsterisk">You can use the asterisk wildcard as the attribute value.
                        Surround the value in single quotation marks.</ph></p>
            <p>
                  <draft-comment author="Loretta">Oracle Table Names, used in JDBC origins &amp;
                        Oracle CDC Client.</draft-comment>
            </p>
            <p><ph id="Oracletablenames">Oracle uses all caps for schema, table, and column names by
                        default. Names can be lower- or mixed-case only if the schema, table, or
                        column was created with quotation marks around the name.</ph></p>
            <p>
                  <draft-comment author="Loretta">Amazon <uicontrol>S3origin-CopyMove1</uicontrol>
                        and 2 used in Amazon S3 origin Configuring.</draft-comment>
            </p>
            <p id="S3origin-CopyMove1">You can copy or move the object to another prefix or bucket.
                  When you use another prefix, enter the prefix. When you use another bucket, enter
                  a prefix and bucket.</p>
            <p id="S3origin-CopyMove2">Copying the object leaves the original object in place. </p>
            <p>
                  <draft-comment author="Loretta">Azure Event Hub reusable stuff</draft-comment>
            </p>
            <p><ph id="AEHub-config">you specify the Microsoft Azure namespace and event hub names.
                        You also define the shared access policy name and connection string
                        key.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>ul-S3-custSSE</uicontrol> is used by
                        Amazon S3 origin and destination in the SSE topic:</draft-comment>
            </p>
            <p>
                  <ul id="ul-S3-custSSE">
                        <li>Base64 encoded 256-bit encryption key</li>
                        <li>Base64 encoded 128-bit MD5 digest of the encryption key using RFC
                              1321</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta"/>
            </p>
            <p>
                  <dl>
                        <dlentry>
                              <dt>Last Modified Timestamp</dt>
                              <dd>The origin can read files in ascending order based on the last
                                    modified timestamp. Assuming the origin reads from a secondary
                                    location - not the directory where the files are created and
                                    written - the last-modified timestamp is usually when the file
                                    is moved to the directory to be processed. </dd>
                              <dd>When ordering based on timestamp, any files with the same
                                    timestamp are read in lexicographically ascending order based on
                                    the file names.</dd>
                              <dd>For example, when reading files with the
                                          <codeph>log*.json</codeph> file name pattern, Directory
                                    reads the following files in the following order:</dd>
                              <dd>
                                    <simpletable frame="none" relcolwidth="1.0* 1.0*"
                                          id="simpletable_jmm_1g4_xv">
                                          <strow>
                                                <stentry>
                                                  <codeblock><uicontrol>File Name</uicontrol></codeblock>
                                                </stentry>
                                                <stentry>
                                                  <codeblock><uicontrol>Last Modified Timestamp</uicontrol></codeblock>
                                                </stentry>
                                          </strow>
                                          <strow>
                                                <stentry>
                                                  <codeblock>log-1.json</codeblock>
                                                </stentry>
                                                <stentry>
                                                  <codeblock>APR 24 2016 14:03:35</codeblock>
                                                </stentry>
                                          </strow>
                                          <strow>
                                                <stentry>
                                                  <codeblock>log-0054.json</codeblock>
                                                </stentry>
                                                <stentry>
                                                  <codeblock>APR 24 2016 14:05:03</codeblock>
                                                </stentry>
                                          </strow>
                                          <strow>
                                                <stentry>
                                                  <codeblock>log-0055.json </codeblock>
                                                </stentry>
                                                <stentry>
                                                  <codeblock>APR 24 2016 14:45:11</codeblock>
                                                </stentry>
                                          </strow>
                                          <strow>
                                                <stentry>
                                                  <codeblock>log-2.json</codeblock>
                                                </stentry>
                                                <stentry>
                                                  <codeblock>APR 24 2016 14:45:11</codeblock>
                                                </stentry>
                                          </strow>
                                    </simpletable>
                              </dd>
                        </dlentry>
                        <dlentry>
                              <dt>Lexicographically Ascending File Names</dt>
                              <dd>The origin can read files in lexicographically ascending order
                                    based on file names. Note that lexicographically ascending order
                                    reads the numbers 1 through 11 as follows:</dd>
                              <dd>
                                    <codeblock>1, 10, 11, 2, 3, 4... 9</codeblock>
                              </dd>
                              <dd>For example, when reading files with the <codeph>web*.log</codeph>
                                    file name pattern, Directory reads the following files in the
                                    following
                                    order:<codeblock>web-1.log
web-10.log
web-11.log
web-2.log
web-3.log
web-4.log
web-5.log
web-6.log
web-7.log
web-8.log
web-9.log</codeblock></dd>
                              <dd>To read these files in logical and lexicographically ascending
                                    order, you might add leading zeros to the file naming convention
                                    as
                                    follows:<codeblock>web-0001.log
web-0002.log
web-0003.log
...
web-0009.log
web-0010.log
web-0011.log</codeblock></dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>TIP-CompareHTTPOrigins</uicontrol> used
                        in all HTTP origin overviews. </draft-comment>
            </p>
            <p>
                  <note type="tip" id="Tip-CompareHTTPOrigins"><ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> provides several HTTP origins to address different needs. For a quick
                        comparison chart to help you choose the right one, see <xref
                              href="../Origins/Origins-HTTPComparison.dita#concept_rsz_cnw_qy"
                        />.</note>
            </p>
            <draft-comment author="alisontaylor"><uicontrol>TIP-CompareWebSocketOrigins</uicontrol>
                  used in both WebSocket origin overviews</draft-comment>
            <p>
                  <note type="tip" id="Tip-CompareWebSocketOrigins"><ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> provides two WebSocket origins to address different needs. For a quick
                        comparison chart to help you choose the right one, see <xref
                              href="../Origins/Origins-WebSocketComparison.dita#concept_wz5_jqm_gbb"
                        />.</note>
            </p>
            <p>
                  <draft-comment author="Loretta">Avro-HeaderAttribute - Used in Record Header
                        Attribute type topics for origins with the topic (that process Avro
                        data!).</draft-comment>
            </p>
            <p><ph id="ph-Avro-HeaderAttribute">processes Avro data, it includes the Avro schema in
                        an avroSchema record header attribute.</ph></p>
            <p>
                  <draft-comment author="Loretta">li-avroSchema used in bulletted lists in record
                        header attribute topics for origins with the topic (that process Avro
                        data!).</draft-comment>
            </p>
            <p>
                  <ul id="ul_mbp_4dg_z1b">
                        <li id="li-avroSchema">avroSchema - When processing Avro data, provides the
                              Avro schema.</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>O-EventHandling-overview</uicontrol> -
                        used by Directory and File Tail and a lot of other origins</draft-comment>
            </p>
            <p id="O-EventHandling-overview">The origin can generate events for an event stream. For
                  more information about dataflow triggers and the event framework, see <xref
                        href="../Event_Handling/EventFramework-Overview.dita#concept_cph_5h4_lx"/>. </p>
            <p>
                  <draft-comment author="Loretta">Event Generation topic with multiple ph and p used
                        by Directory and File Tail. Last xref also used in executor
                        overviews.</draft-comment>
            </p>
            <p><ph id="O-ph-EventGen1">can generate events that you can use in an event stream. When
                        you enable event generation, the origin generates event records each time
                        the origin starts or completes reading a file.</ph>
            </p>
            <p><ph id="O-ph-EventGen-FileStorage">events can be used with a destination to store
                        file processing information for later use. For an example, see <xref
                              href="../Event_Handling/CaseStudy-EventStorage.dita#concept_ocb_nnl_px"
                        />.</ph></p>
            <p>
                  <draft-comment author="Loretta">next one used in executor overviews that generate
                        events.</draft-comment>
            </p>
            <p id="p-Executor-EventXref">You can also configure the executor to generate events for
                  another event stream. <ph id="O-EventGen-xref">For more information about dataflow
                        triggers and the event framework, see <xref
                              href="../Event_Handling/EventFramework-Overview.dita#concept_cph_5h4_lx"
                        />.</ph></p>
            <p>
                  <draft-comment author="Loretta">The following paragraphs are used in respective
                        executor overviews and Dataflow Trigger > executor stages.</draft-comment>
            </p>
            <p><ph id="HiveMetadata-Use">You can use the executor in any logical way, such as
                        running Hive or Impala queries after the Hive Metadata destination updates
                        the Hive metastore, or after the Hadoop FS or MapR FS destination closes
                        files.</ph>
            </p>
            <p><ph id="FileMetadata-Use">You can use the executor in any logical way, such as
                        changing file metadata after receiving file closure events from the Hadoop
                        FS or Local FS destinations.</ph>
            </p>
            <p><ph id="MapRMetadata-Use">You can use the executor in any logical way, such as
                        creating an empty file after the MapR FS destination closes a file.</ph></p>
            <p><ph id="MapReduce-Use">You can use the executor in any logical way, such as running
                        MapReduce jobs after the Hadoop FS or MapR FS destination closes files.</ph>
            </p>
            <p><ph id="PipeFinish-Use">You can use the Pipeline Finisher in any logical way, such as
                        stopping a pipeline upon receiving a no-more-data event from the JDBC Query
                        Consumer origin.</ph>
            </p>
            <p><ph id="Spark-Use">You can use the executor in any logical way, such as running Spark
                        applications after the Hadoop FS, MapR FS, or Amazon S3 destination closes
                        files.</ph></p>
            <p>
                  <draft-comment author="Loretta">Event Generation topic. Hadoop FS uses the whole
                        ul. Local FS uses ph and some bullets in the list. MapR FS uses all bullets
                        in the list. Other </draft-comment>
            </p>
            <p><ph id="D-ph-EventGen1">can generate events that you can use in an event stream. When
                        you enable event generation, the destination generates event records each
                        time the destination closes a file or completes streaming a whole file.
                  </ph></p>
            <p>
                  <ul id="D-ul-Event-UseCases">
                        <li id="D-li-Event-FilePerms">With the HDFS File Metadata executor to move
                              or change permissions on closed files. <p>For an example, see <xref
                                          href="../Event_Handling/CaseStudy-FileManagement.dita#concept_d1q_xl4_lx"
                                    />.</p></li>
                        <li id="D-li-Event-HiveQuery">With the Hive Query executor to run Hive or
                              Impala queries after closing output files. <p>For an example, see
                                          <xref
                                          href="../Event_Handling/CaseStudy-Impala.dita#concept_szz_xwm_lx"
                                    />.</p></li>
                        <li id="D-li-Event-Mapreduce">With the MapReduce executor to convert
                              completed Avro files to Parquet. <p>For an example, see <xref
                                          href="../Event_Handling/CaseStudy-Parquet.dita#concept_jkm_rnz_kx"
                                    />.</p></li>
                        <li id="D-li-Event-Email">With the Email executor to send a custom email
                              after receiving an event.<p>For an example, see <xref
                                          href="../Event_Handling/CaseStudy-SendEmail.dita#concept_t2t_lp5_xz"
                                    />.</p></li>
                        <li id="D-li-Event-FileStore">With a destination to store event information.
                                    <p>For an example, see <xref
                                          href="../Event_Handling/CaseStudy-EventStorage.dita#concept_ocb_nnl_px"
                                    />.</p></li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">Used by most origins that generate no-more-data in
                        the Event Generation topic. Including S3, Directory, JDBC multitable, SQL
                        Server origins. But Salesforce and JDBC Query have their own
                        versions.</draft-comment>
            </p>
            <p>
                  <ul>
                        <li id="li-UseCase-PipelineFinisher">With the Pipeline Finisher executor to
                              stop the pipeline and transition the pipeline to a Finished state when
                              the origin completes processing available data.<p>When you restart a
                                    pipeline stopped by the Pipeline Finisher executor, the origin
                                    continues processing from the last-saved offset unless you reset
                                    the origin.</p><p>For an example, see <xref
                                          href="../Event_Handling/CaseStudy-StopPipeline.dita#concept_kff_ykv_lz"
                                    />.</p></li>
                  </ul>
            </p>
            <p/>
            <p>
                  <draft-comment author="Loretta">Event Generation topic - Used for JDBC Query and
                        Salesforce, JDBC Multitable uses the mini ph.</draft-comment>
            </p>
            <p>The Salesforce origin <ph id="EventGen-PipeFin-Ph1"><ph id="EventGen-PipeFin-mini"
                              >can generate events that you can use in an event stream. When you
                              enable event generation, the origin generates an event when it
                              completes processing the data returned by the specified</ph> query.
                  </ph></p>
            <p>If the origin is configured to repeat the query, it generates an event each time it
                  completes a query.</p>
            <p>You can use Salesforce events in the following ways:<ul id="EventGen-UseCase-ul">
                        <li>
                              <p>With a destination to store information about completed queries. </p>
                              <p>For an example, see <xref
                                          href="../Event_Handling/CaseStudy-EventStorage.dita#concept_ocb_nnl_px"
                                    />.</p>
                        </li>
                  </ul></p>
            <p><ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/O-EventGen-xref"
                  /></p>
            <p/>
            <draft-comment author="Loretta">Event Record topics for JDBC Multitable and
                  Salesforce.</draft-comment>
            <p id="p-ERec-NoMoreData">The no-more-data event record includes no record fields.</p>
            <p>
                  <draft-comment author="Loretta">D-event-xref</draft-comment>
            </p>
            <p><ph id="D-event-xref">The destination can generate events for an event stream. For
                        more information about the event framework, see <xref
                              href="../Event_Handling/EventFramework-Overview.dita#concept_cph_5h4_lx"
                        />.</ph></p>
            <p>
                  <draft-comment author="Loretta">multithreaded stuff below. </draft-comment>
            </p>
            <draft-comment author="Loretta">Used in Pipeline Concepts &amp; Design > Single &amp;
                  Multithreaded Pipes, and Multithreaded > Overview. And in Multithreaded Processing
                  topics in origin sections.</draft-comment>
            <p><ph id="ph-MultiThread-PipeRunnerDef">A pipeline runner is a <term>sourceless
                              pipeline instance</term> - an instance of the pipeline that includes
                        all of the processors and destinations in the pipeline and represents all
                        pipeline processing after the origin.</ph></p>
            <p><ph id="ph-MultiThread-def1">And <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> creates a number of pipeline runners based on the pipeline Max Runners
                        property to perform pipeline processing. <ph id="ph-MultiThread-origin">Each
                              thread connects to the origin system and creates a batch of data, and
                              passes the batch to an available pipeline runner.</ph></ph></p>
            <p><ph id="ph-MultiThread-def2"> Each pipeline runner processes one batch at a time,
                        just like a pipeline that runs on a single thread. When the flow of data
                        slows, the pipeline runners wait idly until they are needed.</ph></p>
            <p><ph id="ph-MultiThread-batchorder">Multithreaded pipelines preserve the order of
                        records within each batch, just like a single-threaded pipeline. But since
                        batches are processed by different pipeline instances, the order that
                        batches are written to destinations is not ensured.</ph></p>
            <p>
                  <draft-comment author="Loretta">Used in Origins for Multithreaded and the
                        Multithreaded summary.</draft-comment>
            </p>
            <p>
                  <ul id="Multithread-OriginsList">
                        <li><xref href="../Origins/AmazonSQS.dita#concept_xsh_knm_5bb">Amazon SQS
                                    Consumer</xref> - Reads data from queues in Amazon Simple Queue
                              Services (SQS).</li>
                        <li><xref href="../Origins/AzureEventHub.dita#concept_c1z_15q_1bb">Azure
                                    Event Hub Consumer</xref> - Reads data from Microsoft Azure
                              Event Hub.</li>
                        <li><xref href="../Origins/CoAPServer.dita#concept_wfy_ghn_sz">CoAP
                                    Server</xref> - Listens on a CoAP endpoint and processes the
                              contents of all authorized CoAP requests.</li>
                        <li><xref href="../Origins/Directory.dita#concept_qcq_54n_jq"
                                    >Directory</xref> - Reads fully written files from a
                              directory.</li>
                        <li><xref href="../Origins/Elasticsearch.dita#concept_f1q_vpm_2z"
                                    >Elasticsearch</xref> - Reads data from an Elasticsearch
                              cluster.</li>
                        <li><xref href="../Origins/PubSub.dita#concept_pjw_qtl_r1b">Google Pub/Sub
                                    Subscriber</xref> - Consumes messages from a Google Pub/Sub
                              subscription.</li>
                        <li><xref href="../Origins/HTTPServer.dita#concept_s2p_5hb_4y">HTTP
                                    Server</xref> - Listens on a HTTP endpoint and processes the
                              contents of all authorized HTTP POST requests. </li>
                        <li><xref href="../Origins/MultiTableJDBCConsumer.dita#concept_zp3_wnw_4y"
                                    >JDBC Multitable Consumer</xref> - Reads database data from
                              multiple tables through a JDBC connection.</li>
                        <li><xref href="../Origins/KafkaMultiConsumer.dita#concept_ccs_fn4_x1b"
                                    >Kafka Multitopic Consumer</xref> - Reads data from multiple
                              topics in a Kafka cluster.</li>
                        <li><xref href="../Origins/KinConsumer.dita#concept_anh_4y3_yr">Kinesis
                                    Consumer</xref> - Reads data from a Kinesis cluster.</li>
                        <li><xref href="../Origins/MapRdbCDC.dita#concept_qwj_5vm_pbb">MapR DB
                                    CDC</xref> - Reads changed MapR DB data that has been written to
                              MapR Streams.</li>
                        <li><xref
                                    href="../Origins/MapRStreamsMultiConsumer.dita#concept_hvd_hww_lbb"
                                    >MapR Multitopic Streams Consumer</xref> - Reads data from
                              multiple topics in a MapR Streams cluster.</li>
                        <li><xref href="../Origins/SQLServerCDC.dita#concept_ut3_ywc_v1b">SQL Server
                                    CDC Client</xref> - Reads data from Microsoft SQL Server CDC
                              tables.</li>
                        <li><xref href="../Origins/SQLServerChange.dita#concept_ewq_b2s_r1b">SQL
                                    Server Change Tracking</xref> - Processes data from Microsoft
                              SQL Server change tracking tables. </li>
                        <li><xref href="../Origins/TCPServer.dita#concept_ppm_xb1_4z">TCP
                                    Server</xref> - Listens at the specified ports and processes
                              incoming data over TCP/IP connections.</li>
                        <li><xref href="../Origins/UDPMulti.dita#concept_wng_g5f_5bb">UDP
                                    Multithreaded Source</xref> - Reads messages from one or more
                              UDP ports. </li>
                        <li><xref href="../Origins/WebSocketServer.dita#concept_u2r_gpc_3z"
                                    >WebSocket Server</xref> - Listens on a WebSocket endpoint and
                              processes the contents of all authorized WebSocket requests. </li>
                        <li><xref href="../Pipeline_Design/DevStages.dita#concept_czx_ktn_ht">Dev
                                    Data Generator</xref> - Generates random data for development
                              and testing.</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">next couple are used in origin versions of
                        Multithreaded Processing</draft-comment>
            </p>
            <p><ph id="ph-MultiThread-def1_JDBCModified">each thread connects to the origin system
                        and creates a batch of data, and passes the batch to an available pipeline
                        runner. A pipeline runner is a <term>sourceless pipeline instance</term> -
                        an instance of the pipeline that includes all of the processors and
                        destinations in the pipeline and performs all pipeline processing after the
                        origin.</ph></p>
            <p><ph id="ph-MultiThread-def3">Each pipeline runner performs the processing associated
                        with the rest of the pipeline. After a batch is written to pipeline
                        destinations, the pipeline runner becomes available for another batch of
                        data. Each batch is processed and written as quickly as possible,
                        independent from other batches processed by other pipeline runners, so
                        batches may be written differently from the read-order.</ph></p>
            <p>
                  <draft-comment author="Loretta">Used in Multithreaded > How it works and origins >
                        HTTP Server > Multithreaded Processing and JDBC Multitable Consumer >
                        Multithreaded Processing</draft-comment>
            </p>
            <p><ph id="HTTPServer-process-ph"><ph id="ph-createsthreads">the origin creates five
                              threads, and <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> creates a matching number of pipeline runners.</ph>
                        <ph id="ph-Passesbatch">Upon receiving data, the origin passes a batch to
                              each of the pipeline runners for processing.</ph>
                  </ph></p>
            <p id="HTTPServer-process-p">At any given moment, the five pipeline runners can each
                  process a batch, so this multithreaded pipeline processes up to five batches at a
                  time. When incoming data slows, the pipeline runners sit idle, available for use
                  as soon as the data flow increases.</p>
            <p>
                  <draft-comment author="Loretta">JDBC Consumer, Oracle CDC Client origin, and JDBC
                        Query executor use these in the overview:</draft-comment>
            </p>
            <p id="JDBC-legacyInfo"><ph id="ph-JDBC-legacyInfo">To use a JDBC version older than
                        4.0, you can specify the driver class name and define a health check
                        query.</ph></p>
            <p>
                  <draft-comment author="Loretta">JDBC Query Consumer and Multitable Consumer both
                        use this in the overviews.</draft-comment>
            </p>
            <p><ph id="ph-unsupportedType">And you can specify what the origin does when
                        encountering an unsupported data type: convert the data to string or stop
                        the pipeline.</ph></p>
            <p>
                  <draft-comment author="Loretta">JDBC Consumer, Producer, and Query executor use
                        the following <uicontrol>ph-JDBCdriver</uicontrol> and SDCDPM_ExtLib
                  </draft-comment>
            </p>
            <p><ph id="ph-JDBCdriver">install the JDBC driver for the database. You cannot access
                        the database until you install the required driver. </ph></p>
            <p>
                  <draft-comment author="Loretta">JDBC Multitable Consumer uses all three DLentries,
                        and SQL Server Change Tracking uses the first two DLentries in the
                        respective Initial Table Order topics. – could not use dlentries and the
                        entire DL - the pdf pub fails.</draft-comment>
            </p>
            <p>
                  <dl id="DL-InitialTableOrder">
                        <dlentry id="TableOrder_None-dlentry">
                              <dt>None</dt>
                              <dd>Reads the tables in the order that they are listed in the
                                    database.</dd>
                        </dlentry>
                        <dlentry id="TableOrder-Alphabet-dlentry">
                              <dt>Alphabetical</dt>
                              <dd>Reads the tables in alphabetical order.</dd>
                        </dlentry>
                        <dlentry id="TableOrder-Constrants-dlentry">
                              <dt>Referential Constraints</dt>
                              <dd>Reads the tables based on the dependencies between the tables. The
                                    origin reads the parent table first, and then reads the child
                                    tables that refer to the parent table with a foreign key.</dd>
                              <dd>You cannot use the referential constraints order when the tables
                                    to be read have a cyclic dependency. When the origin detects a
                                    cyclic dependency, the pipeline fails to validate with the
                                    following
                                    error:<codeblock>JDBC_68 Tables referring to each other in a cyclic fashion.</codeblock></dd>
                              <dd>Note that the referential constraints order can cause pipeline
                                    validation or initialization to slow down because the origin has
                                    to sort the tables before reading them.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">Oracle CDC Client uses this in Initial Change and
                        Configuring</draft-comment>
            </p>
            <p><ph id="OracleCDC-DateFormat">Use the following format: <codeph>DD-MM-YYYY
                              HH24:MI:SS</codeph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">Oracle CDC Client uses this in the overview and
                        Initial Change.</draft-comment>
            </p>
            <p><ph><ph id="OracleCDC-fulldata"><ph>use the JDBC Consumer origin in a separate
                                    pipeline to read the existing data before you start the pipeline
                                    with Oracle CDC Client</ph></ph></ph></p>
            <p>
                  <draft-comment author="Loretta">The following phrases and bullets are used by SQL
                        Server CDC Client - Table Config and Configuring the origin</draft-comment>
            </p>
            <ul id="CaptureInstance-Formats">
                  <li><ph id="format-ChangeInstanceName">To process the CDC tables that match the
                              specified capture instance name pattern, use the following
                              format:</ph><codeblock id="code-ChangeInstanceName">&lt;capture instance name pattern></codeblock><p>Use
                              this format when CDC tables are created based on capture instance
                              names. You can use the pattern to process a full set of CDC tables or
                              to exclude some CDC tables from processing. </p><p>For example, say
                              you have a <codeph>Sales.Accounts</codeph> table with a CDC table
                              named <codeph>Sales_Accounts_CT</codeph>. After adding several columns
                              to the table, you create a new CDC table called
                                    <codeph>Sales_Accounts2_CT</codeph>. </p><p>To process both CDC
                              tables, you can specify the following capture instance name pattern:
                                    <codeph>Sales_Accounts%</codeph>. To process only the CDC data
                              that occurred after the schema change, you can specify the following
                              capture instance name: <codeph>Sales_Accounts2</codeph>.</p></li>
                  <li><ph id="format-TableNamePattern">To process all available CDC tables for the
                              specified data tables, use the following
                              format:</ph><codeblock id="code-tablenamepattern">&lt;schema name>_&lt;data table name pattern></codeblock><p>Use
                              this format when CDC tables are created based on data tables instead
                              of capture instance names. </p><p>For example, to process all
                              available CDC tables for data tables in a Sales schema, you might use
                                    <codeph>Sales_%</codeph>. Or, to process the CDC tables
                              associated with a set of data tables with the Transact prefix, you
                              might use <codeph>Sales_Transact%</codeph>.</p></li>
                  <li><ph id="format-AllTables">To process all CDC tables associated with the
                              schema, use the following format:</ph><p>
                              <codeblock id="code-AllTables">&lt;schema name>_%</codeblock>
                        </p><p>For example, to process all tables in the sales schema, enter
                                    <codeph>sales_%</codeph>.</p></li>
            </ul>
            <p/>
            <p>
                  <draft-comment author="Loretta">UDP Source and UDP to Kafka use the following ph
                        and ul:</draft-comment>
            </p>
            <p><ph id="UDP-messagetypes">can process <xref href="https://collectd.org/"
                              format="html" scope="external">collectd</xref> messages, <ph
                              id="ph-NetFlowVersions">NetFlow 5 and NetFlow 9 messages</ph>, and the
                        following types of syslog messages:</ph><ul id="UDP-syslog">
                        <li>RFC 5424 (<xref href="https://tools.ietf.org/html/rfc5424" format="html"
                                    scope="external"
                              >https://tools.ietf.org/html/rfc5424</xref>)</li>
                        <li>RFC 3164 (<xref href="https://tools.ietf.org/html/rfc3164" format="html"
                                    scope="external"
                              >https://tools.ietf.org/html/rfc3164</xref>)</li>
                        <li>Non-standard common messages, such as RFC 3339 dates with no version
                              digit</li>
                  </ul></p>
            <p><ph id="ph-NetFlowInfo">When processing NetFlow messages, the stage generates
                        different records based on the NetFlow version. When processing NetFlow 9,
                        the records are generated based on the NetFlow 9 configuration properties.
                        For more information, see <xref
                              href="../Data_Formats/NetFlow_Overview.dita#concept_thl_nnr_hbb"
                        />.</ph></p>
            <p>
                  <draft-comment author="Loretta">UDP to Kafka origin and SDC RPC to Kafka origins
                        use the following text in the Pipeline Configuration topics.
                              <uicontrol>O-ph-PipeConfig</uicontrol> and </draft-comment>
            </p>
            <p><ph id="O-ph-PipeConfig">The origin does not pass records to its output port, so you
                        cannot perform additional processing or write the data to other destination
                        systems.</ph>
            </p>
            <p id="O-p-PipeConfig">However, since a pipeline requires a destination, you should
                  connect the origin to the Trash destination to satisfy pipeline validation
                  requirements.</p>
            <p>
                  <draft-comment author="Loretta">Use din SDC to Kafka > Batch Request Size, Kafka
                        Message Size... > Important: and also in HTTP to Kafka > Request Message
                        Size > Important</draft-comment>
            </p>
            <p><ph id="ph-ToKafka-DefKafkaMessageSize">By default, the maximum message size in a
                        Kafka cluster is 1 MB, as defined by the message.max.bytes property.
                  </ph></p>
            <p>
                  <draft-comment author="Loretta">PROCESSOR INFO –-</draft-comment>
            </p>
            <p>
                  <draft-comment author="Loretta">Aggregator processor</draft-comment>
            </p>
            <p id="AggregateFunctions">You can use the following aggregate functions:<ul
                        id="ul_cwn_cff_wbb">
                        <li>Avg (Integer or Double)</li>
                        <li>Count (Integer only)</li>
                        <li>Min (Integer or Double)</li>
                        <li>Max (Integer or Double)</li>
                        <li>StdDev (Integer or Double)</li>
                        <li>Sum (Integer or Double)</li>
                  </ul></p>
            <p>
                  <draft-comment author="Loretta">Parquet doc - used by HM processor > record header
                        attributes, and Hive Drift > Parquet Processing – the ph is used by the
                        Parquet case study > HM processor topic</draft-comment>
            </p>
            <p id="Parquet-TargetDir1">When processing Parquet data, the Hive Metadata processor <ph
                        id="HD-CStudy-ProcessorAvro">adds .avro to the target directory that it
                        generates for each record. This allows the data-processing destination to
                        write the Avro files to a directory that Hive ignores as a temporary
                        directory.</ph></p>
            <p id="Parquet-TargetDir2">As a result, the destination writes files to the following
                  directories: <codeph id="HD-ParquetDir-ph">&lt;generated
                  directory>/.avro</codeph>.</p>
            <p>
                  <note id="Parquet-TargetDir-Note"><ph id="Parquet-TargetDir-ph">You can configure
                              the MapReduce executor to write the Parquet files to the parent
                              generated directory and to delete the Avro files after processing
                              them. You can also delete the temporary directories after the files
                              are processed, as needed.</ph></note>
            </p>
            <p>
                  <draft-comment author="Loretta">P-HM-CompatChanges - The following is used in a
                        couple places by the Hive Metadata processor: </draft-comment>
            </p>
            <p id="P-HM-CompatChanges ">Compatible changes include new tables and partitions, and
                  the addition or removal of fields in the record. Changes in data type are not
                  compatible.</p>
            <p>
                  <draft-comment author="Loretta">The note <uicontrol>P-HM-MaxHivePS</uicontrol> is
                        used in HM processor > Configuring.</draft-comment>
            </p>
            <p>
                  <note id="P-HM-MaxHivePS">At this time, the maximum precision and scale for
                        decimal data in Hive is 38.</note>
            </p>
            <p>
                  <draft-comment author="Loretta">Using the following in Hive Metadata Processor >
                        Hive Names and Supported Characters and in the Hive
                        Metastore</draft-comment>
            </p>
            <p><ph id="HM-HiveNames">Hive table names, column names, and partition names are created
                        with lowercase letters.</ph></p>
            <p>
                  <draft-comment author="Loretta">D-HM-CreatesAndNot_Note , D-HM-CreatesAndNot_PH:
                        the following is used in a couple places.</draft-comment>
            </p>
            <note id="D-HM-CreatesAndNot_Note"><ph id="D-HM-CreatesAndNot_PH">The destination can
                        create tables and partitions. It can add columns to tables and ignore
                        existing columns. It does not drop existing columns from tables.</ph></note>
            <p>
                  <draft-comment author="Loretta"><uicontrol>P-HM-phPartitionDF</uicontrol> The
                        following sentence is used in Hive Metadata processor - Configuring, and
                        Database, Table, and Partition Expressions.</draft-comment>
            </p>
            <p>
                  <ph id="P-HM-phPartitionDF">You can use the Int, Bigint, and String data formats
                        for partition data.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>P-ListPivot-AddPivoters</uicontrol> -
                        The following phrase is used in the Field Pivoter overview &amp; generated
                        records. </draft-comment>
            </p>
            <p><ph id="P-ListPivot-AddPivoters">To pivot additional fields or nested structures, use
                        additional Field Pivoters.</ph>
            </p>
            <p>
                  <draft-comment author="Loretta">Scripting processors - - Groovy and Jython use
                        both, JavaScript uses Record by Record only
                              (<uicontrol>P-ProcessM-RbyR</uicontrol>).</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="P-ProcessMode-RbyR">
                              <dt>Record by Record</dt>
                              <dd>The processor calls the script for each record. The processor
                                    passes the record to the script as a map and processes each
                                    record individually. </dd>
                              <dd>The script does not require error handling logic. Error records
                                    are passed to the processor for error handling. The processor
                                    handles error records based on the On Record Error
                                    property.</dd>
                              <dd>Use this mode to avoid including error handling logic in the code.
                                    Since this mode calls the script for each record, pipeline
                                    performance will be negatively affected. </dd>
                        </dlentry>
                        <dlentry id="P-ProcessM-BbyB">
                              <dt>Batch by Batch</dt>
                              <dd>The processor calls the script for each batch. The processor
                                    passes the batch to the script as a list and processes the batch
                                    at one time. </dd>
                              <dd>Include error handling logic in the script. Without error handling
                                    logic, a single error record sends the entire batch to the
                                    processor for error handling. The processor handles error
                                    records based on the On Record Error property.</dd>
                              <dd>Use this mode to improve performance by processing a batch of data
                                    at a time.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>P-DL-ScriptObjects</uicontrol> -
                        Groovy, JavaScript, Jython Processor scripting objects. All three use these,
                        and they each have their own Record object versions. </draft-comment>
            </p>
            <p>
                  <dl id="P-DL-ScriptObjects">
                        <dlentry>
                              <dt>state</dt>
                              <dd>An object to store information between invocations of the init,
                                    main, and destroy scripts. A state is a map object that includes
                                    a collection of key/value pairs. You can use the state object to
                                    cache data such as lookups, counters, or a connection to an
                                    external system.</dd>
                              <dd>The state object functions much like a member variable: <ul
                                          id="ul_bh4_1yr_kv">
                                          <li>The information is transient and is lost when the
                                                pipeline stops or restarts.</li>
                                          <li>The state object is available only for the instance of
                                                the processor stage it is defined in. If the
                                                pipeline executes in cluster mode, the state object
                                                is not shared across nodes.</li>
                                    </ul></dd>
                              <dd>The same instance of the state object is available to all three
                                    scripts. For example, you might use the init script to open a
                                    connection to a database and then store a reference to that
                                    connection in the state object. In the main script, you can
                                    access the open connection using the state object. Then in the
                                    destroy script, you can close the connection using the state
                                    object.</dd>
                              <dd>
                                    <note type="warning">The state object is best used for a fixed
                                          or static set of data. Adding to the cache on every record
                                          or batch can quickly consume the memory allocated to <ph
                                                conref="#concept_vhs_5tz_xp/pName-long"/> and cause
                                          out of memory exceptions. </note>
                              </dd>
                        </dlentry>
                        <dlentry>
                              <dt>log</dt>
                              <dd>An object to write messages to the log. Includes four methods:
                                          <codeph>info()</codeph>, <codeph>warn()</codeph>,
                                          <codeph>debug()</codeph>, and <codeph>trace()</codeph>. </dd>
                              <dd>The signature of the four methods is as follows:
                                    <codeblock>(message-template, arguments...) </codeblock>The
                                    message template can have positional variables denoted by curly
                                    brackets: { }. The arguments are replaced in the message
                                    template curly brackets in positional manner, i.e., this is the
                                    first argument in the first { } occurrence, and so on.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>output</dt>
                              <dd>An object that writes the record to the output batch. Includes a
                                          <codeph>write(Record)</codeph> method.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>error</dt>
                              <dd>An object that passes error records to the processor for error
                                    handling. Includes a <codeph>write(Record, message)</codeph>
                                    method.</dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="alisontaylor">- Groovy, JavaScript, Jython Processor sdcFunction
                  scripting object. Groovy uses all of the methods. JavaScript ad Jython use all but
                  the last method.</draft-comment>
            <dl>
                  <dlentry>
                        <dt>sdcFunctions</dt>
                        <dd>An object that runs functions that evaluate or modify data. Includes the
                              following methods: </dd>
                        <dd>
                              <ul id="ul_th2_3lr_mx">
                                    <li id="li_getFieldNull"><codeph>getFieldNull(Record, 'field
                                                path')</codeph> - Method that checks if a field is
                                          assigned a constant such as NULL_INTEGER or
                                          NULL_STRING.</li>
                                    <li id="li_createRecord"><codeph>createRecord(String,
                                                'recordId')</codeph> - Method that creates a new
                                          record with the specified fields and values. The recordId
                                          uniquely identifies the record. It should include enough
                                          information to track down the record source.</li>
                                    <li id="li_createMap"><codeph>createMap(boolean
                                                listMap)</codeph> - Method that creates a map for
                                          use as a field in a record. Pass true to create a list-map
                                          field, or false to create a map field.</li>
                                    <li id="li_createEvent"><codeph>createEvent(String type, int
                                                version)</codeph> - Method that creates a new event
                                          record with the specified event type and version. Be sure
                                          to enable event generation in the stage before
                                          implementing event methods. </li>
                                    <li id="li_toEvent"><codeph>toEvent(Record)</codeph> - Method
                                          that sends an event record to the Event output stream. Be
                                          sure to enable event generation in the stage before
                                          implementing event methods. </li>
                                    <li id="li_pipelineParams"><codeph>pipelineParameters()</codeph>
                                          - Method that returns a map of all <xref
                                                href="../Pipeline_Configuration/RuntimeParameters.dita#concept_rjh_ntz_qr"
                                                >runtime parameters</xref> defined for the
                                          pipeline.</li>
                              </ul>
                        </dd>
                  </dlentry>
            </dl>
            <draft-comment author="alisontaylor">Groovy, JavaScript, Jython Evaluators use this text
                  in Accessing Whole Files topic</draft-comment>
            <p id="Script-WholeFile">The processor can access the fileref field in a whole file
                  record by creating an input stream using the getInputStream() API. For example,
                  you might use the processor to read the file data in the fileref field and then
                  create new records with the data. The processor can access the fileref field, but
                  cannot modify information in the field.</p>
            <p id="Script-CreateStream">Use the following lines to create and then read the input
                  stream:<codeblock>input_stream = record.value['fileRef'].getInputStream()
input_stream.read()</codeblock></p>
            <p id="Script-CloseStream">After the processor reads the input stream, include the
                  following line in the code to close the input
                  stream:<codeblock>input_stream.close()</codeblock></p>
            <p>
                  <draft-comment author="Loretta">DESTINATION DATA FORMATS. Individual items are
                        conrefed, not the whole list. (8/10/16) </draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry>
                              <dt/>
                              <dd>
                                    <draft-comment author="Loretta"><uicontrol>AvroFlume</uicontrol>
                                          is used only by Flume. <b>AvroFile</b> used by Hadoop FS,
                                          Local and S3, and other file destinations.
                                                <b>AvroKafka</b> used by Kafka. <b>AvroMess</b> used
                                          by Kinesis Producer and other message destinations.
                                    </draft-comment>
                              </dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="alisontaylor">Avro Flume</draft-comment>
                  <dl>
                        <dlentry id="Avro-Flume">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema. You can
                                    use one of the following methods to specify the location of the
                                    Avro schema definition:</dd>
                              <dd>
                                    <ul id="ul_xwl_1sf_lx">
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration.</li>
                                          <li><uicontrol>In Record Header</uicontrol> - Use the
                                                schema included in the avroSchema record header
                                                attribute.</li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the destination to look up the schema in the
                                                Confluent Schema Registry by the schema ID or
                                                subject. <p>If using the Avro schema in the stage or
                                                  in the record header attribute, you can optionally
                                                  configure the destination to register the Avro
                                                  schema with the Confluent Schema Registry. You can
                                                  also optionally include the schema definition as
                                                  part of the Flume event. Omitting the schema
                                                  definition can improve performance, but requires
                                                  the appropriate schema management to avoid losing
                                                  track of the schema associated with the
                                                data.</p></li>
                                    </ul>
                              </dd>
                              <dd>You can also compress data with an Avro-supported compression
                                    codec. </dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="alisontaylor">Avro File</draft-comment>
                  <dl>
                        <dlentry id="DEST-DF-AvroFILE">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema. You can
                                    use one of the following methods to specify the location of the
                                    Avro schema definition:</dd>
                              <dd>
                                    <ul id="ul_iyw_vrf_lx">
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration.</li>
                                          <li><uicontrol>In Record Header</uicontrol> - Use the
                                                schema included in the avroSchema record header
                                                attribute.</li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the destination to look up the schema in the
                                                Confluent Schema Registry by the schema ID or
                                                subject. <p>If using the Avro schema in the stage or
                                                  in the record header attribute, you can optionally
                                                  configure the destination to register the Avro
                                                  schema with the Confluent Schema
                                                Registry.</p></li>
                                    </ul>
                              </dd>
                              <dd>The destination includes the schema definition in each file.</dd>
                              <dd>You can compress data with an Avro-supported compression codec.
                                    When using Avro compression, avoid using other compression
                                    properties in the destination. </dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="alisontaylor">Avro Kafka – for Kafka only because of the
                        Kafka producer mention.</draft-comment>
                  <dl>
                        <dlentry id="D-DF-AvroKafka">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema. </dd>
                              <dd>You can use one of the following methods to specify the location
                                    of the Avro schema definition:</dd>
                              <dd>
                                    <ul id="ul_fkw_tx4_nx">
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration.</li>
                                          <li><uicontrol>In Record Header</uicontrol> - Use the
                                                schema included in the avroSchema record header
                                                attribute.</li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the destination to look up the schema in the
                                                Confluent Schema Registry by the schema ID or
                                                subject. <p>You must specify the method that the
                                                  Kafka Producer uses to serialize the messages in
                                                  the Avro format. To embed the Avro schema ID in
                                                  each message that the destination writes, set the
                                                  key and value serializers to Confluent on the
                                                  <uicontrol>Kafka</uicontrol> tab.</p><p>If using
                                                  the Avro schema in the stage or in the record
                                                  header attribute, you can optionally configure the
                                                  destination to register the Avro schema with the
                                                  Confluent Schema Registry. You can also optionally
                                                  include the schema definition in the
                                                message.</p></li>
                                    </ul>
                              </dd>
                              <dd>You can compress data with an Avro-supported compression codec.
                              </dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="alisontaylor">Avro other message destinations. LC: removed
                        reference to not-using other compression options in the destination because
                        I didn't see any. Checked all destinations using this. 9/29/2017. </draft-comment>
                  <dl>
                        <dlentry id="D-DF-AvroMess">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema. You can
                                    use one of the following methods to specify the location of the
                                    Avro schema definition:</dd>
                              <dd>
                                    <ul id="ul_gkw_tx4_nx">
                                          <li><uicontrol>In Pipeline Configuration</uicontrol> - Use
                                                the schema that you provide in the stage
                                                configuration.</li>
                                          <li><uicontrol>In Record Header</uicontrol> - Use the
                                                schema included in the avroSchema record header
                                                attribute.</li>
                                          <li><uicontrol>Confluent Schema Registry</uicontrol> -
                                                Retrieve the schema from Confluent Schema Registry.
                                                The Confluent Schema Registry is a distributed
                                                storage layer for Avro schemas. You can configure
                                                the destination to look up the schema in the
                                                Confluent Schema Registry by the schema ID or
                                                subject. <p>If using the Avro schema in the stage or
                                                  in the record header attribute, you can optionally
                                                  configure the destination to register the Avro
                                                  schema with the Confluent Schema Registry. You can
                                                  also optionally include the schema definition in
                                                  the message. Omitting the schema definition can
                                                  improve performance, but requires the appropriate
                                                  schema management to avoid losing track of the
                                                  schema associated with the data.</p></li>
                                    </ul>
                              </dd>
                              <dd>You can compress data with an Avro-supported compression codec.
                                    When using Avro compression, avoid using any other compression
                                    properties in the destination. </dd>
                        </dlentry>
                  </dl>
                  <draft-comment author="Loretta"><b>DESTDataFormat-Binary</b> - the following is
                        just for Kafka Producer, Amazon S3, Azure IoT Hub.</draft-comment>
                  <dl>
                        <dlentry id="DESTDataFormat-Binary">
                              <dt>Binary</dt>
                              <dd>The destination writes binary data from a single field in the
                                    record. </dd>
                        </dlentry>
                        <dlentry id="DEST-DataF-Delim">
                              <dt>Delimited</dt>
                              <dd>The destination writes records as delimited data. When you use
                                    this data format, the root field must be list or list-map.</dd>
                        </dlentry>
                        <dlentry id="DEST-DataF-JSON">
                              <dt>JSON</dt>
                              <dd>The destination writes records as JSON data. You can use one of
                                    the following formats:<ul id="ul_dd1_5y1_wr">
                                          <li>Array - Each file includes a single array. In the
                                                array, each element is a JSON representation of each
                                                record.</li>
                                          <li>Multiple objects - Each file includes multiple JSON
                                                objects. Each object is a JSON representation of a
                                                record. </li>
                                    </ul></dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta"><b>DestDF-ProtoMess</b> - Amazon S3, Directory? Hadoop
                  FS?, Azure IoT Hub</draft-comment>
            <dl>
                  <dlentry id="DestDF-ProtoMess">
                        <dt>Protobuf</dt>
                        <dd>Writes one record in a message. Uses the user-defined message type and
                              the definition of the message type in the descriptor file to generate
                              the message. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Data_Formats/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
            </dl>
            <draft-comment author="Loretta"><b>DestDF-ProtoFile</b> - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="DestDF-ProtoFile">
                        <dt>Protobuf</dt>
                        <dd>Writes a batch of messages in each file. </dd>
                        <dd>Uses the user-defined message type and the definition of the message
                              type in the descriptor file to generate the messages in the file. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Data_Formats/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
                  <dlentry id="DEST-DataF-SDC">
                        <dt>SDC Record</dt>
                        <dd>The destination writes records in the SDC Record data format. </dd>
                  </dlentry>
            </dl>
            <draft-comment author="Loretta"><b>DESTDataF-Text</b> and others in this list are used
                  by Kafka, Flume, Hadoop FS, Local FS, Kinesis Firehose. </draft-comment>
            <dl>
                  <dlentry id="DESTDataF-Text">
                        <dt>Text</dt>
                        <dd>The destination writes data from a single text field to the destination
                              system. When you configure the stage, you select the field to use.
                              When necessary, merge record data into the field earlier in the
                              pipeline. </dd>
                        <dd>You can configure the characters to use as record separators. By
                              default, the destination uses a Unix-style line ending (\n) to
                              separate records.</dd>
                        <dd>When a record contains no data in the text field, you can configure the
                              destination to write the record separator characters, creating an
                              empty line. By default, the destination discards the record.</dd>
                  </dlentry>
            </dl>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-DF-WholeFile</uicontrol> - must be
                        used by S3 - cannot change permissions.
                              <uicontrol>D-DF-WholeF-Basic</uicontrol> is used by Local FS.
                              <uicontrol>D-DF-WholeF-noEvent</uicontrol> - can change permission, no
                        events - used by Azure Data Lake Store.
                              <uicontrol>D-DF-WholeF-HDFS</uicontrol> is used by Hadoop FS and MapR
                        FS.</draft-comment>
            </p>
            <dl>
                  <dlentry id="D-DF-WholeFile">
                        <dt>Whole File</dt>
                        <dd>Streams whole files to the destination system. The destination writes
                              the data to the file and location defined in the stage. If a file of
                              the same name already exists, you can configure the destination to
                              overwrite the existing file or send the current file to error.</dd>
                        <dd>Written files use the default permissions defined in the destination
                              system. </dd>
                        <dd>You can configure the destination to generate a checksum for the written
                              file and pass checksum information to the destination system in an
                              event record. </dd>
                        <dd>For more information about the whole file data format, see <xref
                                    href="../Data_Formats/WholeFile.dita#concept_nfc_qkh_xw"/>.</dd>
                  </dlentry>
                  <dlentry id="D-DF-WholeF-Basic">
                        <dt>Whole File</dt>
                        <dd>Streams whole files to the destination system. The destination writes
                              the data to the file and location defined in the stage. If a file of
                              the same name already exists, you can configure the destination to
                              overwrite the existing file or send the current file to error.</dd>
                        <dd>By default, written files use the default access permissions for the
                              destination system. You can specify an expression that defines access
                              permissions. </dd>
                        <dd>You can configure the destination to generate a checksum for the written
                              file and pass checksum information to the destination system in an
                              event record. </dd>
                        <dd>For more information about the whole file data format, see <xref
                                    href="../Data_Formats/WholeFile.dita#concept_nfc_qkh_xw"/>.</dd>
                  </dlentry>
                  <dlentry id="D-DF-WholeF-noEvent">
                        <dt>Whole File</dt>
                        <dd>Streams whole files to the destination system. The destination writes
                              the data to the file and location defined in the stage. If a file of
                              the same name already exists, you can configure the destination to
                              overwrite the existing file or send the current file to error.</dd>
                        <dd>By default, written files use the default access permissions for the
                              destination system. You can specify an expression that defines access
                              permissions. </dd>
                        <dd>For more information about the whole file data format, see <xref
                                    href="../Data_Formats/WholeFile.dita#concept_nfc_qkh_xw"/>.</dd>
                  </dlentry>
                  <dlentry id="D-DF-WholeF-HDFS">
                        <dt>Whole File</dt>
                        <dd>Streams whole files to the destination system. The destination writes
                              the data to the file and location defined in the stage. If a file of
                              the same name already exists, you can configure the destination to
                              overwrite the existing file or send the current file to error.</dd>
                        <dd>By default, written files use the default access permissions for the
                              destination system. You can specify an expression that defines access
                              permissions. </dd>
                        <dd>You can configure the destination to generate a checksum for the written
                              file and pass checksum information to the destination system in an
                              event record. </dd>
                        <dd>Using this data format requires setting the <wintitle>File
                                    Type</wintitle> property to <uicontrol>Whole
                              File</uicontrol>.</dd>
                        <dd>For more information about the whole file data format, see <xref
                                    href="../Data_Formats/WholeFile.dita#concept_nfc_qkh_xw"/>.</dd>
                  </dlentry>
            </dl>
            <p>
                  <draft-comment author="Loretta">The following DEST-XMLmessages is used by Kafka
                        Producer and JMS Producer and Azure IoT Hub</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="DEST-XMLmessages ">
                              <dt>XML</dt>
                              <dd>The destination creates a valid XML document for each record. The
                                    destination requires the record to have a single root field that
                                    contains the rest of the record data. For details and
                                    suggestions for how to accomplish this, see <xref
                                          href="../Data_Formats/WritingXML-Requirement.dita#concept_cmn_hml_r1b"
                                          />.<p>The destination can include indentation to produce
                                          human-readable documents. It can also validate <ph
                                                id="ph-WriteXML-ValidateSchema">that the generated
                                                XML conforms to the specified schema definition.
                                                Records with invalid schemas are handled based on
                                                the error handling configured for the
                                                destination.</ph></p></dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">The note/tip
                              <uicontrol>WF-TIP-DataPreview</uicontrol> is used in Whole File -
                        Additional Processors. <uicontrol>WF-ph-DataPreview</uicontrol> is used in
                        Whole File Records.</draft-comment>
            </p>
            <p>
                  <note type="tip" id="WF-TIP-DataPreview"><ph id="WF-ph-DataPreview">You can use
                              data preview to determine the information and field names that are
                              included in the fileInfo field.</ph> The information and field names
                        can differ based on the origin system.</note>
            </p>
            <p>
                  <draft-comment author="Loretta">Hive Drift / Data Synchronization Avro/Parquet
                        reused content - used in Avro &amp; Parquet implementation. </draft-comment>
            </p>
            <p><ph id="HD-BasicImp-ph">the origin of your choice, the Hive Metadata processor
                        connected to the Hive Metastore destination to perform metadata updates, and
                        to either the Hadoop FS or MapR FS destination to process data</ph></p>
            <p>As with Avro data, the <ph id="HD-DataStream-ph">Hive Metadata processor passes
                        records through the first output stream - the data stream. Connect the data
                        stream to the Hadoop FS or MapR FS destination to write data to the
                        destination system using record header attributes.</ph></p>
            <p id="HD-MetadataStream-p">The Hive Metadata processor passes the metadata record
                  through the second output stream - the metadata output stream. Connect the Hive
                  Metastore destination to the metadata output stream to enable the destination to
                  create and update tables in Hive. The metadata output stream contains no record
                  data. </p>
            <p id="HD-NestedFields">If your data contains nested fields, you would add a Field
                  Flattener to flatten records as follows: </p>
            <p>
                  <draft-comment author="Loretta">Hive Drift / Data Synchronization Avro/Parquet
                        reused content - used in Avro &amp; Parquet case studies.</draft-comment>
            </p>
            <p id="HD-CStudy-ConnectHMetastore">Connect the destination to the second output stream
                  of the processor and configure the destination. Configuration of this destination
                  is a breeze - just configure the Hive connection information and optionally
                  configure some advanced options. </p>
            <p><ph id="HM-CStudy-HiveMetastoreUpdates-ph">And if the structure of the record going
                        to a table changes, like adding a couple new fields, the destination updates
                        the table so the record can be written to it.</ph></p>
            <p id="HD-CStudy-HDFS">To write data to Hive using record header attributes, you can use
                  the Hadoop FS or MapR FS destinations. We'll use Hadoop FS destination. </p>
            <p id="HD-CStudy-HDFSuseatts">When you configure the destination, instead of configuring
                  a directory template, you configure the destination to use the directory in the
                  record header. Configure the destination to roll files when it sees a "roll"
                  attribute in the record header, and when configuring the Avro properties, indicate
                  that the schema is in the record header. </p>
            <p id="HD-CStudy-HDFSprocessing-p"><ph id="HD-CStudy-HDFSprocessing-ph">With this
                        configuration, the destination uses the information in record header
                        attributes to write data to HDFS. It writes each record to the directory in
                        the targetDirectory header attribute, using the Avro schema in the
                        avroSchema header attribute.</ph> And it rolls a file when it spots the roll
                  attribute in a record header. </p>
            <p>
                  <draft-comment author="Loretta">The following used in Parquet case study > Hive
                        Metastore and the Parquet Processing topic.</draft-comment>
            </p>
            <p><ph id="HD-StoredAsParquet-ph">The destination uses the Stored as Parquet clause when
                        generating the table so it does not need to generate a new schema for each
                        change.</ph></p>
            <p>
                  <draft-comment author="Loretta">The following is used in Parquet case study > Hive
                        Metadata processor and Wrapup</draft-comment>
            </p>
            <p><ph id="HD-CStudy-HMProc-Actions">When a record includes a schema change, the
                        processor writes the new schema to the avroSchema header attribute and adds
                        the roll header attribute to the record. It also generates a metadata record
                        for the Hive Metastore destination. The combination of these actions enables
                        the Hive Metastore destination to update Parquet tables as needed and for
                        the Hadoop FS destination to write the file with schema drift to the updated
                        table.</ph></p>
            <p/>
            <p>
                  <draft-comment author="Loretta">The following paragraphs are used for Expressions
                        (Pipeline Config chapter) and Expression Language (appendix).
                  </draft-comment>
            </p>
            <p id="EXP-p1">Use the expression language to configure expressions and conditions in
                  processors, such as the Expression Evaluator or Stream Selector. Some destination
                  properties also require the expression language, such as the directory template
                  for Hadoop FS or Local FS. </p>
            <p id="EXP-p2">Optionally, you can use the expression language to define any stage or
                  pipeline property that represents a numeric or string value. This allows you to
                  use runtime parameters or runtime properties throughout the pipeline. You can use
                  expression completion to determine where you can use an expression and the
                  expression elements that you can use in that location. </p>
            <p id="EXP-p3">You can use the following elements in an expression:<ul
                        id="ul_w34_vxl_2s">
                        <li>Constants</li>
                        <li>Datetime variables</li>
                        <li>Field names</li>
                        <li>Functions</li>
                        <li>Literals</li>
                        <li>Operators</li>
                        <li>Runtime parameters</li>
                        <li>Runtime properties</li>
                        <li>Runtime resources</li>
                  </ul></p>
            <p>
                  <draft-comment author="Loretta">The following is used in the Tutorial chapter, in
                        tables in the Creating a Pipeline... topic and the Write to the Destination
                        topic.</draft-comment>
            </p>
            <p id="FilePrefix">By default, the files are prefixed with "SDC" and an expression that
                  returns the <ph conref="#concept_vhs_5tz_xp/pName-long"/> ID, but that's more than
                  we need here. </p>
            <draft-comment author="Loretta">The following is used in HDFS origin and destination
                  overviews:</draft-comment>
            <p id="HDFS_user_props">When necessary, you can enable Kerberos authentication and
                  specify a Hadoop user. You can also use Hadoop configuration files and add other
                  Hadoop configuration properties as needed. </p>
            <p>
                  <draft-comment author="Loretta">The following p's and ph's are used in
                        Hadoop-related stages in the Hadoop User (or similar)
                        topics.</draft-comment>
            </p>
            <p><ph id="HUser-Intro"><ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> can either use the currently logged in <ph
                              conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> user or a user configured in the</ph> origin to read from HDFS. </p>
            <p id="HUser-ImpProperty">A <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                  /> configuration property can be set that requires using the currently logged in
                        <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                  /> user. When this property is not set, you can specify a user in the origin. For
                  more information about Hadoop impersonation and the Data Collector property, see
                        <xref
                        href="../Configuration/HadoopImpersonationMode.dita#concept_pmr_sy5_nz"
                        product="SDC"/><ph product="DPM"><xref
                              href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/DCConfig.html%23concept_pmr_sy5_nz"
                              format="html" scope="external">Hadoop Impersonation Mode</xref> in the
                        Data Collector documentation</ph>. </p>
            <p>Note that the origin <ph id="HUser-ConnectUser">uses a different user account to
                        connect to HDFS. <ph id="HUser-ConnectGeneric">By default, <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> uses the user account who started it to connect to external
                              systems. When using Kerberos, <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> uses the Kerberos principal.</ph>
                  </ph></p>
            <p>
                  <ol>
                        <li id="HUser-Step1">On Hadoop, configure the user as a proxy user and
                              authorize the user to impersonate a Hadoop user. <p>For more
                                    information, see the Hadoop documentation. </p></li>
                  </ol>
            </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by Hadoop FS
                  origin - used in Hadoop Properties and Configuring Hadoop FS
                  origin.</draft-comment>
            <p>
                  <ul id="ul-HDFSfiles_HDFSorigin">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                        <li>yarn-site.xml</li>
                        <li>mapred-site.xml</li>
                  </ul>
                  <draft-comment author="Loretta" id="ul_">list of HDFS configuration files used by
                        Hadoop FS destination - used in Hadoop Properties and Configuring Hadoop FS
                        destination.</draft-comment>
                  <ul id="HDFSfiles_HDFSdest">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                  </ul>
            </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by HBase
                  destination - used in Hadoop Properties and Configuring HBase
                  destination.</draft-comment>
            <ul id="HDFSfiles_HBasedest">
                  <li>hbase-site.xml</li>
            </ul>
            <draft-comment author="Loretta">List of config files used by Hive Streaming - used in
                  Hive Properties &amp; Configuring Hive Streaming dest. Also in Hive Metadata
                  processor - Hive properties &amp; configuring.</draft-comment>
            <ul id="HiveStreamingFiles">
                  <li>core-site.xml</li>
                  <li>hdfs-site.xml</li>
                  <li>hive-site.xml</li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following DD is used by </draft-comment>
            </p>
            <p>
                  <ol id="OL-HiveConfigSteps">
                        <li>Store the files or a symlink to the files in the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> resources directory or elsewhere in a path local to the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              />.</li>
                        <li>If the files are stored in the resources directory, specify a relative
                              path to the files in the stage. If the files are stored outside of the
                              resources directory, specify an absolute path to the files. <note>For
                                    a Cloudera Manager installation, <ph
                                          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                    /> automatically creates a symlink to the files named
                                          <codeph>hive-conf</codeph>. Enter
                                          <codeph>hive-conf</codeph> for the location of the files
                                    in the stage.</note></li>
                  </ol>
            </p>
            <p>
                  <draft-comment author="Loretta">DRIFT-IgnoreWMissing - Using this in the Data
                        Drift Functions topic:</draft-comment>
            </p>
            <p id="DRIFT-ignoreWMissing">Use the ignoreWhenMissing flag to determine the behavior
                  when the field is missing. When set to "true", a missing field causes no errors.
                  When set to "false", a missing field generates an alert for the record missing the
                  field, and for the next record that includes the field.</p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HM-Overview1</uicontrol> - used by
                        Hadoop FS and MapR FS overviews</draft-comment>
            </p>
            <p><ph id="D-HM-Overview1">you can define a directory template and time basis to
                        determine the output directories that the destination creates and the files
                        where records are written. </ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HM-HiveSolution</uicontrol>,
                              <uicontrol>D-HM-Overview2</uicontrol>
                        <uicontrol>D-HM-compression</uicontrol> - used by Hadoop FS and MapR FS
                        overviews</draft-comment>
            </p>
            <p id="D-HM-HiveSolutionOverview1">As part of the Drift Synchronization Solution for
                  Hive, you can alternatively use record header attributes to perform record-based
                  writes. You can write records to the specified directory, use the defined Avro
                  schema, and roll files based on record header attributes. For more information,
                  see <xref
                        href="../Pipeline_Design/RecordBasedWrites-overview.dita#concept_lmn_gdc_1w"
                  />.</p>
            <p id="D-HM-Overview2">You can define a file prefix and suffix, the data time zone, and
                  properties that define when the destination closes a file. You can specify the
                  amount of time that a record can be written to its associated directory and what
                  happens to late records.</p>
            <p id="D-HM-compression">You can use Gzip, Bzip2, Snappy, LZ4, and other compression
                  formats to write output files. </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HLM-Dirtemp-targetDir</uicontrol>
                        used in HadoopFS, LocalFS, MapR FS</draft-comment>
            </p>
            <p id="D-HLM-Dirtemp-targetDir">You can alternatively write records to directories based
                  on the targetDirectory record header attribute. Using the targetDirectory
                  attribute disables the ability to define directory templates.</p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-HLM-TimeB-targetDir</uicontrol> used
                        in HadoopFS, LocalFS, MapR FS</draft-comment>
            </p>
            <p><ph id="D-HLM-TimeB-targetDir">When using the targetDirectory record header attribute
                        to write records, the time basis determines only whether a record is
                        late.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-GenAttributes</uicontrol> used in
                        Record Header Attributes for RB Writes and Generating RH
                        Attributes</draft-comment>
            </p>
            <p><ph id="D-GenAttributes">The Hive Metadata processor automatically generates record
                        header attributes for Hadoop FS and MapR FS to use as part of the Drift
                        Synchronization Solution for Hive. For all other destinations, you can use
                        the Expression Evaluator or a scripting processor to add record header
                        attributes.</ph>
            </p>
            <draft-comment author="alisontaylor">Directory Templates - used for Hadoop FS, Local FS,
                  and MapR FS destinations</draft-comment>
            <p id="DirectoryTemplate_Intro">When you define a directory template, you can use a mix
                  of constants, field values, and datetime variables. You can use the
                        <codeph>every</codeph> function to create new directories at regular
                  intervals based on hours, minutes, or seconds, starting on the hour. You can also
                  use the <codeph>record:valueOrDefault</codeph> function to use field values or a
                  default in the directory template. </p>
            <p id="DirectoryTemplate_Example">For example, the following directory template creates
                  output directories for event data based on the state and timestamp of a record
                  with hours as the smallest unit of measure, creating a new directory every
                  hour:<codeblock> /outputfiles/${record:valueOrDefault("/State", "unknown")}/${YY()}-${MM()}-${DD()}-${hh()}</codeblock></p>
            <p id="DirectoryTemplate_DL">You can use the following elements in a directory template:<dl>
                        <dlentry>
                              <dt>Constants</dt>
                              <dd>You can use any constant, such as "output".</dd>
                        </dlentry>
                        <dlentry>
                              <dt>Datetime Variables</dt>
                              <dd>You can use datetime variables, such as <codeph>${YYYY()}</codeph>
                                    or <codeph>${DD()}</codeph>. The destination creates directories
                                    as needed, based on the smallest datetime variable that you use.
                                    For example, if the smallest variable is hours, then the
                                    directories are created for every hour of the day that receives
                                    output records.</dd>
                              <dd>When you use datetime variables in an expression, use all of the
                                    datetime variables between one of the year variables and the
                                    smallest variable that you want to use. For example, to create
                                    directories on a daily basis for a Hadoop FS destination, use a
                                    year variable, a month variable, and then a day variable. You
                                    might use one of the following datetime variable progressions: </dd>
                              <dd>
                                    <codeblock>${YYYY()}-${MM()}-${DD()}
${YY()}_${MM()}_${DD()}</codeblock>
                              </dd>
                              <dd>For details about datetime variables, see <xref
                                          href="../Expression_Language/DateTimeVariables.dita#concept_gh4_qd2_sv"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt><codeph>every</codeph> function</dt>
                              <dd>You can use the <codeph>every</codeph> function in a directory
                                    template to create directories at regular intervals based on
                                    hours, minutes, or seconds, beginning on the hour. The intervals
                                    should be a submultiple or integer factor of 60. For example,
                                    you can create directories every 15 minutes or 30 seconds. </dd>
                              <dd>Use the <codeph>every</codeph> function to replace the smallest
                                    datetime variable used in the template.</dd>
                              <dd>For example, the following directory template creates directories
                                    every 5 minutes, starting on the
                                    hour:<codeblock>/HDFS_output/${YYYY()}-${MM()}-${DD()}-${hh()}-${every(5,mm())}</codeblock></dd>
                              <dd>For details about the <codeph>every</codeph> function, see <xref
                                          href="../Expression_Language/MiscFunctions.dita#concept_ddw_ld1_1s"
                                    />.</dd>
                        </dlentry>
                        <dlentry>
                              <dt><codeph>record:valueOrDefault</codeph> function</dt>
                              <dd>You can use the following expression to use the value of a field
                                    and the specified default value if the field does not exist or
                                    if the field is null:
                                    <codeblock>${record:valueOrDefault(&lt;field path>, &lt;default value>)}</codeblock></dd>
                              <dd>For example, the following directory template creates a directory
                                    based on the product field every day, and if the product field
                                    is empty or null, uses Misc in the directory path:
                                    <codeblock>/${record:valueOrDefault("/Product", "Misc")}/${YY()}-${MM()}-${DD()}</codeblock></dd>
                              <dd>This template might create the following
                                    paths:<codeblock>/Shirts/2015-07-31 
/Misc/2015-07-31</codeblock></dd>
                        </dlentry>
                  </dl></p>
            <draft-comment author="alisontaylor">Time Basis - used for Hadoop FS, Local FS, and MapR
                  FS destinations</draft-comment>
            <dl id="TimeBasis">
                  <dlentry>
                        <dt>Processing Time</dt>
                        <dd>When you use processing time as the time basis, the destination creates
                              directories based on the processing time and the directory template,
                              and writes records to the directories based on when they are
                              processed.</dd>
                        <dd>For example, say a directory template creates directories every minute
                              and the time basis is the time of processing. Then, directories are
                              created for every minute that the destination writes output records.
                              And the output records are written to the directory for that minute of
                              processing. </dd>
                        <dd>To use the processing time as the time basis, use the following
                              expression: <codeph>${time:now()}</codeph>. This is the default time
                              basis. </dd>
                  </dlentry>
                  <dlentry>
                        <dt>Record Time</dt>
                        <dd>When you use the time associated with a record as the time basis, you
                              specify a Date field in the record. The destination creates
                              directories based on the datetimes associated with the records and
                              writes the records to the appropriate directories. </dd>
                        <dd>For example, say a directory template creates directories every hour and
                              the time basis is based on the record. Then, directories are created
                              for every hour associated with output records and the destination
                              writes the records to the related output directory. </dd>
                        <dd>To use a time associated with the record, use an expression that calls a
                              field and resolves to a datetime value, such as
                                    <codeph>${record:value("/Timestamp")}</codeph>. </dd>
                  </dlentry>
            </dl>
            <draft-comment author="alisontaylor">Late Records and Late Record Handling - used for
                  Hadoop FS, Local FS, and MapR FS destinations</draft-comment>
            <p id="LateRecordsOverview">When you use a record time as the time basis, you can define
                  a time limit for records to be written to their associated output file. When the
                  destination creates a new output file in a new directory, the previous output file
                  is kept open for the specified late record time limit. When records that belong in
                  that file arrive within the time limit, the destination writes the records to the
                  open output file. When the late record time limit is reached, the output file is
                  closed and any record that arrives past this limit is considered late.</p>
            <note type="tip" id="LateRecordsTip">The late records properties are not applicable if
                  you use processing time as the time basis. If you use processing time, set the
                  late record time limit to one second.</note>
            <p id="LateRecordsError">You can send late records to a late records file or to the
                  stage for error handling. When you send records to a late records file, you define
                  a late records directory template. </p>
            <p id="LateRecordsExample1">For example, you use a record time as the time basis,
                  configure a one hour late record time limit, configure late records to be sent to
                  the stage for error handling, and use the default directory template value:
                  <codeblock>/tmp/out/${YYYY()}-${MM()}-${DD()}-${hh()} </codeblock></p>
            <p id="LateRecordsExample2">The first records that arrive have a datetime between the
                  hours of 02:00 and 02:59, and so are written to an output file in the 02
                  directory. When records with a datetime between the hours of 03:00 and 03:59
                  arrive, the destination creates a new file in an 03 directory. The destination
                  keeps the file in the 02 directory open for another hour. </p>
            <p id="LateRecordsExample3">If a record with a datetime between the hours of 02:00 and
                  02:59 arrives before the hour time limit, the destination writes the record to the
                  open file in the 02 directory. After one hour, the destination closes the output
                  file in the 02 directory. Any records with a datetime between the hours of 02:00
                  and 02:59 that arrive after the one hour time limit are considered late. The late
                  records are sent to the stage for error handling.</p>
            <draft-comment author="alisontaylor">Timeout to Close Idle Files - used for Hadoop FS,
                  Local FS, and MapR FS destinations – also Azure Data Lake Store
                  (lc)</draft-comment>
            <p id="IdleTimeoutOverview">You might want to configure an idle timeout when output
                  files remain open and idle for too long, thus delaying another system from
                  processing the files.</p>
            <p id="IdleTimeoutReasons">Output files might remain idle for too long for the following
                  reasons: <ul id="ul_pcq_5l3_mw">
                        <li>You configured the maximum number of records to be written to output
                              files or the maximum size of output files, but records have stopped
                              arriving. An output file that has not reached the maximum number of
                              records or the maximum file size stays open until more records
                              arrive.</li>
                        <li>You configured a date field in the record as the time basis and have
                              configured a late record time limit, but records arrive in
                              chronological order. When a new directory is created, the output file
                              in the previous directory remains open for the configured late record
                              time limit. However, no records are ever written to the open file in
                              the previous directory.<p>For example, when a record with a datetime
                                    of 03:00 arrives, the destination creates a new file in a new 03
                                    directory. The previous file in the 02 directory is kept open
                                    for the late record time limit, which is an hour by default.
                                    However, when records arrive in chronological order, no records
                                    that belong in the 02 directory arrive after the 03 directory is
                                    created. </p></li>
                  </ul></p>
            <p id="IdleTimeoutSummary">In either situation, configure an idle timeout so that other
                  systems can process the files sooner, instead of waiting for the configured
                  maximum records, maximum file size, or late records conditions to occur. </p>
            <p>
                  <draft-comment author="alisontaylor">Recovery - used for Hadoop FS, Local FS, and
                        MapR FS destinations</draft-comment>
            </p>
            <p id="RecoveryIntro">The destination names temporary open output files using the
                  following format:<codeblock>_tmp_&lt;prefix>_&lt;runnerId></codeblock>Where
                        <codeph>&lt;prefix></codeph> is the file prefix defined for the destination
                  and <codeph>&lt;runnerId></codeph> is the ID of the pipeline runner performing the
                  pipeline processing. For example, when the destination prefix is defined as
                        <codeph>sdc</codeph> and the destination runs from a single-threaded
                  pipeline, the temporary file is named like so: <codeph>_tmp_sdc_0</codeph>. </p>
            <p id="RecoveryClose">When the destination closes the file, either after it is fully
                  written, after the idle timeout expires, or when you deliberately stop the
                  pipeline, it renames the file to remove the <codeph>_tmp_</codeph> string and to
                  replace the pipeline runner ID with a random unique identifier like
                  so:<codeblock>&lt;prefix>_e7ce67c5-013d-47a7-9496-8c882ddb28a0</codeblock></p>
            <p id="RecoveryUnexpected">However, when the pipeline stops unexpectedly, the temporary
                  files remain. When the pipeline restarts, the destination scans all subdirectories
                  of the defined directory template to rename any temporary files that match the
                  defined prefix for the destination. After the destination renames the temporary
                  files, it continues writing to new output files.</p>
            <note id="RecoveryRename">The destination renames all temporary files that match the
                  defined prefix in all subdirectories of the defined directory template, even if
                  those files were not written by that pipeline. So if you happen to have another
                  file whose name begins with the same pattern - <codeph>_tmp_&lt;prefix></codeph> -
                  the destination renames that file also.</note>
            <p id="RecoveryExceptions">The destination might not rename all temporary files in the
                  following situations:<dl>
                        <dlentry>
                              <dt>The directory template includes an expression with the
                                    record:value or record:valueOrDefault function.</dt>
                              <dd>If the record:value or record:valueOrDefault function evaluates to
                                    an empty string or to a subdirectory, the destination cannot
                                    determine those locations when the pipeline restarts. As a
                                    result, the destination cannot rename any temporary files
                                    written to those locations.</dd>
                              <dd>For example, let’s assume that the directory template is defined
                                    as
                                    follows:<codeblock>/tmp/out/${YY()}-${MM()}-${DD()}/${sdc:hostname()}/${record:value('/a')}/${record:value('/b')}</codeblock></dd>
                              <dd>If the expression <codeph>${record:value('/b')}</codeph> evaluates
                                    to an empty string or to a subdirectory such as
                                          <codeph>/folder1/folder2</codeph>, then the destination
                                    cannot determine those locations when the pipeline restarts.
                              </dd>
                        </dlentry>
                        <dlentry>
                              <dt>The directory is defined in the targetDirectory record header
                                    attribute.</dt>
                              <dd>When the directory is defined in the targetDirectory record header
                                    attribute, the destination cannot determine where to look for
                                    temporary files when the pipeline restarts. As a result, it
                                    cannot rename the temporary files.</dd>
                        </dlentry>
                  </dl></p>
            <p id="RecoveryManual">In either of these situations, you must manually rename the
                  temporary files.</p>
            <p id="RecoveryDisable">File recovery can slow down the pipeline as it restarts. If
                  needed, you can configure the destination to skip file recovery.</p>
            <p>
                  <draft-comment author="Loretta">Record Header Attributes - Hadoop FS, Local FS,
                        MapR FS. But Local and MapR FS don't have full functionality yet
                        7/6</draft-comment>
            </p>
            <p><ph id="D-HLM-Attributes1">To use record header attributes for record-based writes,
                        you configure the destination to use the header attribute, and you ensure
                        that the record headers include the header attribute.</ph>
            </p>
            <p id="D-HLM-Attributes2">Use the Expression Evaluator processor to add stage attributes
                  to record headers. </p>
            <p>
                  <draft-comment author="Loretta">** there were 2 versions, but now there's just the
                        one in Record Header Attributes. Delete or keep as conref for some reason?
                        LC, 7/6 ** Two versions of this, first for Hadoop FS, 2nd for Local &amp;
                        MapR FS. This is so we don't forget to update Hadoop FS when updating the
                        others? </draft-comment>
            </p>
            <p>
                  <dl id="D-HadoopFS-AttrDL">
                        <dlentry>
                              <dt>targetDirectory</dt>
                              <dd>The targetDirectory header attribute defines the directory where
                                    the record is written. If the directory does not exist, the
                                    destination creates the directory. The targetDirectory header
                                    attribute replaces the Directory Template property in the
                                    destination.</dd>
                              <dd>When you use targetDirectory to provide the directory, the time
                                    basis configured for the destination is used only for
                                    determining whether a record is late. Time basis is not used to
                                    determine the output directories to create or to write records
                                    to directories.</dd>
                              <dd>To use the targetDirectory header attribute, on the
                                          <wintitle>Output</wintitle> tab, select
                                          <uicontrol>Directory in Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>avroSchema</dt>
                              <dd>The avroSchema header attribute defines the Avro schema for the
                                    record. When you use this header attribute, you cannot define an
                                    Avro schema to use in the destination. </dd>
                              <dd>To use the avroSchema header attribute, on the
                                          <wintitle>Avro</wintitle> tab, select <uicontrol>Load
                                          Schema from Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>roll</dt>
                              <dd>The roll attribute, when present in the record header, triggers a
                                    roll of the file. </dd>
                              <dd>You can define the name of the roll header attribute. When you use
                                    the Hive Metadata processor to generate the roll header
                                    attribute, use the default "roll" attribute name.</dd>
                              <dd>To use a roll header attribute, on the <wintitle>Output</wintitle>
                                    tab, select <uicontrol>Use Roll Attribute</uicontrol> and define
                                    the name of the attribute. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-LocalMapRFS-AttrDL</uicontrol> Local
                        FS and MapR FS versions – NOT USED? DELETE?</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry>
                              <dt>targetDirectory</dt>
                              <dd>The targetDirectory header attribute defines the directory where
                                    the record is written. If the directory does not exist, the
                                    destination creates the directory. The targetDirectory header
                                    attribute replaces the Directory Template property in the
                                    destination.</dd>
                              <dd>When you use targetDirectory to provide the directory, the time
                                    basis configured for the destination is used only for
                                    determining whether a record is late. Time basis is not used to
                                    determine the output directories to create or to write records
                                    to directories.</dd>
                              <dd>To use the targetDirectory header attribute, on the
                                          <wintitle>Output</wintitle> tab, select
                                          <uicontrol>Directory in Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>avroSchema</dt>
                              <dd>The avroSchema header attribute defines the Avro schema for the
                                    record. When you use this header attribute, you cannot define an
                                    Avro schema to use in the destination. </dd>
                              <dd>To use the avroSchema header attribute, on the
                                          <wintitle>Avro</wintitle> tab, select <uicontrol>Load
                                          Schema from Header</uicontrol>.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>roll</dt>
                              <dd>The roll attribute, when present in the record header, triggers a
                                    roll of the file. </dd>
                              <dd>You can define the name of the roll header attribute. When you use
                                    the Hive Metadata processor to generate the roll header
                                    attribute, use the default "roll" attribute name.</dd>
                              <dd>To use a roll header attribute, on the <wintitle>Output</wintitle>
                                    tab, select <uicontrol>Use Roll Attribute</uicontrol> and define
                                    the name of the attribute. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">Start event description - used in Pipeline Events
                        and Pipeline Event Records</draft-comment>
            </p>
            <p><ph id="ph-StartEvent">as the pipeline initializes, immediately after it starts and
                        before individual stages are initialized</ph></p>
            <p>
                  <draft-comment author="Loretta">Stop event description - used in Pipeline Events
                        and Pipeline Event Records</draft-comment>
            </p>
            <p><ph id="phStopEvent">as the pipeline stops, either manually, programmatically, or due
                        to a failure. The stop event is generated after all stages have completed
                        processing and cleaning up temporary resources, such as removing temporary
                        files.</ph></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>ph-Erecord-Fields</uicontrol> - used in
                        Event Handing > Event Records and Event Record Headers</draft-comment>
            </p>
            <p><ph id="ph-ERecord-Fields"><ph id="ph-Event-RFields">Stage-generated event records
                              differ from stage to stage.</ph> For a description of stage events,
                        see "Event Record" in the documentation for the event-generating stage. For
                        a description of pipeline events, see <xref
                              href="../Pipeline_Configuration/EventRecords.dita#concept_cv3_nqt_51b"
                        />.</ph>
            </p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>D-DL-EventRecords</uicontrol> - Event
                        record DL for Hadoop FS, Local FS, MapR FS. Update Amazon S3 destination as
                        needed! Last two rows in the last table is used by S3.</draft-comment>
            </p>
            <p id="D-DL-EventRecords">The destination can generate the following types of event records:<dl>
                        <dlentry>
                              <dt>File closure</dt>
                              <dd>The destination generates a file closure event record when it
                                    closes an output file. </dd>
                              <dd>File closure event records have the
                                          <codeph>sdc.event.type</codeph> record header attribute
                                    set to <codeph>file-closed</codeph> and include the following
                                          fields:<table frame="all" rowsep="1" colsep="1"
                                          id="table_dh2_5gc_rx">
                                          <tgroup cols="2">
                                                <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                                                <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                                                <thead>
                                                  <row>
                                                  <entry>Field</entry>
                                                  <entry>Description</entry>
                                                  </row>
                                                </thead>
                                                <tbody>
                                                  <row>
                                                  <entry>filepath</entry>
                                                  <entry>Absolute path to the closed file. </entry>
                                                  </row>
                                                  <row>
                                                  <entry>filename</entry>
                                                  <entry>File name of the closed file.</entry>
                                                  </row>
                                                  <row>
                                                  <entry>length</entry>
                                                  <entry>Size of the closed file in bytes.</entry>
                                                  </row>
                                                </tbody>
                                          </tgroup>
                                    </table></dd>
                        </dlentry>
                        <dlentry>
                              <dt>Whole file processed</dt>
                              <dd>The destination generates an event record when it completes
                                    streaming a whole file. Whole file event records have the
                                          <codeph>sdc.event.type</codeph> record header attribute
                                    set to <codeph>wholeFileProcessed</codeph> and have the
                                    following fields:<table frame="all" rowsep="1" colsep="1"
                                          id="table_eh2_5gc_rx">
                                          <tgroup cols="2">
                                                <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                                                <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                                                <thead>
                                                  <row>
                                                  <entry>Field</entry>
                                                  <entry>Description</entry>
                                                  </row>
                                                </thead>
                                                <tbody>
                                                  <row>
                                                  <entry>sourceFileInfo</entry>
                                                  <entry>A map of attributes about the original
                                                  whole file that was processed. The attributes
                                                  include:<ul id="ul_nbd_cmc_rx">
                                                  <li>size - Size of the whole file in bytes. </li>
                                                  </ul><p>Additional attributes depend on the
                                                  information provided by the origin system.
                                                  </p></entry>
                                                  </row>
                                                  <row>
                                                  <entry>targetFileInfo</entry>
                                                  <entry>A map of attributes about the whole file
                                                  written to the destination. The attributes
                                                  include:<ul id="ul_kx1_klc_rx">
                                                  <li>path - An absolute path the processed whole
                                                  file.</li>
                                                  </ul></entry>
                                                  </row>
                                                  <row id="row-D-Event-Checksum">
                                                  <entry>checksum</entry>
                                                  <entry>Checksum generated for the written file.
                                                  <p>Included only when you configure the
                                                  destination to include checksums in the event
                                                  record. </p></entry>
                                                  </row>
                                                  <row id="row-D-Event-CheckAlgo">
                                                  <entry>checksumAlgorithm</entry>
                                                  <entry>Algorithm used to generate the checksum.
                                                  <p>Included only when you configure the
                                                  destination to include checksums in the event
                                                  record. </p></entry>
                                                  </row>
                                                </tbody>
                                          </tgroup>
                                    </table></dd>
                        </dlentry>
                  </dl></p>
            <p>
                  <draft-comment author="Loretta"><uicontrol>code-HiveDrift-DB.table</uicontrol>,
                              <uicontrol>ph-code-HiveDrift-DB.table</uicontrol> - used in
                        Hive-Impala case study and Hive Query executor > Impala
                        queries</draft-comment>
            </p>
            <p>
                  <codeblock id="code-HiveDrift-DB.table"><ph id="ph-code-HiveDrift-DB.table">`${file:pathElement(record:value('/filepath'), -3)}`.`${file:pathElement(record:value('/filepath'), -2)}`</ph></codeblock>
            </p>
            <p>
                  <draft-comment author="Loretta">ul-KERBprops - Kerberos properties in Data
                        Collector config file:</draft-comment>
            </p>
            <p>
                  <ul id="ul-KERBprops">
                        <li>kerberos.client.enabled</li>
                        <li>kerberos.client.principal</li>
                        <li>kerberos.client.keytab</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">UseDefaults: Using this phrase throughout the tutorial
                  as a disclaimer:</draft-comment>
            <p id="UseDefaults">Use the defaults for properties that aren't listed:</p>
            <p>
                  <draft-comment author="Loretta">MapR-Prereq - Using in all MapR stages to make
                        sure they configure SDC.</draft-comment>
            </p>
            <p id="MapRPrereq">Before you use any MapR stage in a pipeline, you must perform
                  additional steps to enable <ph conref="#concept_vhs_5tz_xp/pName-long"/> to
                  process MapR data. <ph conref="#concept_vhs_5tz_xp/SDCDPM_MapRPrereq"/></p>
            <draft-comment author="alisontaylor">MapRPrereq-Stats - Used in Write Statistics to MapR
                  Streams.</draft-comment>
            <p id="MapRPrereq-Stats">Before you can write statistics to MapR Streams, you must
                  perform additional steps to enable <ph conref="#concept_vhs_5tz_xp/pName-long"/>
                  to process MapR data. <ph conref="#concept_vhs_5tz_xp/SDCDPM_MapRPrereq"/></p>
            <draft-comment author="alisontaylor">MapR-PrereqHiveStream - Use in Hive Streaming
                  destination for MapR library to make sure they configure SDC.</draft-comment>
            <p id="MapRPrereqHiveStream">Before you use the Hive Streaming destination with the MapR
                  library in a pipeline, you must perform additional steps to enable <ph
                        conref="#concept_vhs_5tz_xp/pName-long"/> to process MapR data. <ph
                        conref="#concept_vhs_5tz_xp/SDCDPM_MapRPrereq"/>
            </p>
            <draft-comment author="alisontaylor">Two versions of Lookup Cache below. Be sure to make
                  the same updates to both.</draft-comment>
            <draft-comment author="alisontaylor">Lookup Cache - used for Redis Lookup and HBase
                  Lookup processors</draft-comment>
            <p id="LookupCacheOverview">The processor caches key-value pairs until the cache reaches
                  the maximum size or the expiration time. When the first limit is reached, the
                  processor evicts key-value pairs from the cache.</p>
            <p id="LookupCacheOptions">You can configure the following ways to evict key-value pairs
                  from the cache:<dl>
                        <dlentry>
                              <dt>Size-based eviction</dt>
                              <dd>Configure the maximum number of key-value pairs that the processor
                                    caches. When the maximum number is reached, the processor evicts
                                    the oldest key-value pairs from the cache.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>Time-based eviction</dt>
                              <dd>Configure the amount of time that a key-value pair can remain in
                                    the cache without being written to or accessed. When the
                                    expiration time is reached, the processor evicts the key from
                                    the cache. The eviction policy determines whether the processor
                                    measures the expiration time since the last write of the value
                                    or since the last access of the value.</dd>
                              <dd>For example, you set the eviction policy to expire after the last
                                    access and set the expiration time to 60 seconds. After the
                                    processor does not access a key-value pair for 60 seconds, the
                                    processor evicts the key-value pair from the cache.</dd>
                        </dlentry>
                  </dl></p>
            <p id="LookupCacheSummary">When you stop the pipeline, the processor clears the
                  cache.</p>
            <draft-comment author="alisontaylor">Lookup Cache - used for JDBC Lookup, Salesforce
                  Lookup, and Kudu Lookup processors</draft-comment>
            <p id="JDBCLookupCacheOverview">The processor caches values until the cache reaches the
                  maximum size or the expiration time. When the first limit is reached, the
                  processor evicts values from the cache.</p>
            <p id="JDBCLookupCacheOptions">You can configure the following ways to evict values from
                  the cache:<dl>
                        <dlentry>
                              <dt>Size-based eviction</dt>
                              <dd>Configure the maximum number of values that the processor caches.
                                    When the maximum number is reached, the processor evicts the
                                    oldest values from the cache.</dd>
                        </dlentry>
                        <dlentry>
                              <dt>Time-based eviction</dt>
                              <dd>Configure the amount of time that a value can remain in the cache
                                    without being written to or accessed. When the expiration time
                                    is reached, the processor evicts the value from the cache. The
                                    eviction policy determines whether the processor measures the
                                    expiration time since the last write of the value or since the
                                    last access of the value.</dd>
                              <dd>For example, you set the eviction policy to expire after the last
                                    access and set the expiration time to 60 seconds. After the
                                    processor does not access a value for 60 seconds, the processor
                                    evicts the value from the cache.</dd>
                        </dlentry>
                  </dl></p>
            <p id="JDBCLookupCacheSummary">When you stop the pipeline, the processor clears the
                  cache.</p>
            <draft-comment author="alisontaylor">Used in Field Type Converter chapter and in Data
                  Preview chapter.</draft-comment>
            <note id="PreviewDateFormat"><ph id="ph-PreviewDateFormat">Data preview displays date,
                        datetime, and time data using the default format of the browser locale. For
                        example, if the browser uses the en_US locale, preview displays dates using
                        the following format: MMM d, y h:mm:ss a.</ph></note>
            <draft-comment author="alisontaylor">Used in the Function types sections in the
                  Expression Language chapter.</draft-comment>
            <p id="FunctionArgument">You can replace any argument with a literal or an expression
                  that evaluates to the argument. String literals must be enclosed in single or
                  double quotation marks.</p>
            <p>
                  <draft-comment author="alisontaylor">AWS Credentials for destinations: Amazon S3,
                        Kinesis Firehose, and Kinesis Producer. </draft-comment>
            </p>
            <dl id="AWSCredentials-Destinations">
                  <dlentry>
                        <dt>IAM roles</dt>
                        <dd>When <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> runs on an Amazon EC2 instance, you can use the AWS Management
                              Console to configure an IAM role for the EC2 instance. <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> uses the IAM instance profile credentials to automatically connect
                              to AWS. </dd>
                        <dd>When you use IAM roles, you do not need to specify the Access Key ID and
                              Secret Access Key properties in the destination. </dd>
                        <dd>For more information about assigning an IAM role to an EC2 instance, see
                              the Amazon EC2 documentation.</dd>
                  </dlentry>
                  <dlentry>
                        <dt>AWS access key pairs</dt>
                        <dd>
                              <p>When <ph
                                          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                    /> does not run on an Amazon EC2 instance or when the EC2
                                    instance doesn’t have an IAM role, you must specify the
                                          <uicontrol>Access Key ID</uicontrol> and <uicontrol>Secret
                                          Access Key</uicontrol> properties in the destination.</p>
                        </dd>
                  </dlentry>
            </dl>
            <draft-comment author="alisontaylor">AWS Credentials for origins: Amazon S3 and Kinesis
                  Consumer</draft-comment>
            <dl id="AWSCredentials-Origins">
                  <dlentry>
                        <dt>IAM roles</dt>
                        <dd>When <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> runs on an Amazon EC2 instance, you can use the AWS Management
                              Console to configure an IAM role for the EC2 instance. <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> uses the IAM instance profile credentials to automatically connect
                              to AWS. </dd>
                        <dd>When you use IAM roles, you do not need to specify the Access Key ID and
                              Secret Access Key properties in the origin.</dd>
                        <dd>For more information about assigning an IAM role to an EC2 instance, see
                              the Amazon EC2 documentation.</dd>
                  </dlentry>
                  <dlentry>
                        <dt>AWS access key pairs</dt>
                        <dd>When <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> does not run on an Amazon EC2 instance or when the EC2 instance
                              doesn’t have an IAM role, you must specify the <uicontrol>Access Key
                                    ID</uicontrol> and <uicontrol>Secret Access Key</uicontrol>
                              properties in the origin.</dd>
                  </dlentry>
            </dl>
            <draft-comment author="alisontaylor"><uicontrol>Tip_Usernames_JDBC</uicontrol> used in
                  username/password properties for JDBC stages. <uicontrol>Tip_Usernames</uicontrol>
                  Used in username/password properties for all other stages.
                        <uicontrol>Tip_AccessKeys</uicontrol> used in AWS Credentials topic for AWS
                  stages. <uicontrol>Tip_Passwords</uicontrol> used in keystore/truststore password
                  properties for all stages.</draft-comment>
            <note type="tip" id="SDCDPM_Tip_Usernames">To secure sensitive information such as
                  usernames and passwords, you can use <xref
                        href="../Pipeline_Configuration/RuntimeResources.dita#concept_bs4_5nm_2s"
                        >runtime resources</xref> or <ph product="SDC"><xref
                              href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                              >credential stores.</xref></ph><ph product="DPM">credential stores.
                        For more information about credential stores, see <xref
                              href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/CredentialStores.html"
                              format="html" scope="external">Credential Stores</xref> in the Data
                        Collector documentation.</ph></note>
            <note type="tip" id="SDCDPM_Tip_AccessKeys">To secure sensitive information such as
                  access key pairs, you can use <xref
                        href="../Pipeline_Configuration/RuntimeResources.dita#concept_bs4_5nm_2s"
                        >runtime resources</xref> or <ph product="SDC"><xref
                              href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                              >credential stores.</xref></ph><ph product="DPM">credential stores.
                        For more information about credential stores, see <xref
                              href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/CredentialStores.html"
                              format="html" scope="external">Credential Stores</xref> in the Data
                        Collector documentation.</ph></note>
            <note type="tip" id="SDCDPM_Tip_Passwords">To secure sensitive information such as
                  passwords, you can use <xref
                        href="../Pipeline_Configuration/RuntimeResources.dita#concept_bs4_5nm_2s"
                        >runtime resources</xref> or <ph product="SDC"><xref
                              href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                              >credential stores.</xref></ph><ph product="DPM">credential stores.
                        For more information about credential stores, see <xref
                              href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/CredentialStores.html"
                              format="html" scope="external">Credential Stores</xref> in the Data
                        Collector documentation.</ph></note>
            <note type="tip" id="SDCDPM_Tip_ConsumerKey">To secure sensitive information such as the
                  consumer key and secret, you can use <xref
                        href="../Pipeline_Configuration/RuntimeResources.dita#concept_bs4_5nm_2s"
                        >runtime resources</xref> or <ph product="SDC"><xref
                              href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                              >credential stores.</xref></ph><ph product="DPM">credential stores.
                        For more information about credential stores, see <xref
                              href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/CredentialStores.html"
                              format="html" scope="external">Credential Stores</xref> in the Data
                        Collector documentation.</ph></note>
            <note type="tip" id="SDCDPM_Tip_ClientID">To secure sensitive information such as the
                  client ID and secret, you can use <xref
                        href="../Pipeline_Configuration/RuntimeResources.dita#concept_bs4_5nm_2s"
                        >runtime resources</xref> or <ph product="SDC"><xref
                              href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                              >credential stores.</xref></ph><ph product="DPM">credential stores.
                        For more information about credential stores, see <xref
                              href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/CredentialStores.html"
                              format="html" scope="external">Credential Stores</xref> in the Data
                        Collector documentation.</ph></note>
            <note type="tip" id="SDCDPM_Tip_JWTKey">To secure sensitive information such as the JWT
                  signing key, you can use <xref
                        href="../Pipeline_Configuration/RuntimeResources.dita#concept_bs4_5nm_2s"
                        >runtime resources</xref> or <ph product="SDC"><xref
                              href="../Configuration/CredentialStores.dita#concept_bt1_bpj_r1b"
                              >credential stores.</xref></ph><ph product="DPM">credential stores.
                        For more information about credential stores, see <xref
                              href="https://streamsets.com/documentation/datacollector/latest/help/#Configuration/CredentialStores.html"
                              format="html" scope="external">Credential Stores</xref> in the Data
                        Collector documentation.</ph></note>
            <p>
                  <draft-comment author="Loretta">Cluster mode: MapR Streaming mode and Kafka (Yarn)
                        requirements:</draft-comment>
            </p>
            <p>
                  <ol id="ol-Clust-Stream-loglevel">
                        <li>Edit the log4j.properties file, located in the following directory:
                              <codeblock>&lt;spark-home>/conf/log4j.properties</codeblock></li>
                        <li>Set the <uicontrol>log4j.rootCategory</uicontrol> property to a severity
                              of INFO or lower, such as DEBUG or TRACE.</li>
                  </ol>
            </p>
            <p>
                  <draft-comment author="Loretta">EXECUTORS</draft-comment>
            </p>
            <p>
                  <draft-comment author="Loretta">The following is used by the S3 executor overview
                        and the Dataflow Triggers > Executors topic.</draft-comment>
            </p>
            <p><ph id="ph-S3usage">You can use the executor in any logical way, such as writing
                        information from an event record to an S3 object or tagging objects after
                        they are written by the Amazon S3 destination.</ph>
            </p>
            <p>
                  <draft-comment author="Loretta">The following is used by the S3 executor Tags and
                        Configuring topics. </draft-comment>
            </p>
            <p><ph id="ph-S3keyvalues">You can configure multiple tags. When you configure a tag,
                        you can define a tag with just the key or specify a key and value. You can
                        also use expressions to define tag values.</ph></p>
            <p>
                  <draft-comment author="Loretta">The note is used in HDFS File Metadata executor
                        File Path topic and Configuring</draft-comment>
            </p>
            <p>
                  <note id="FileMetadataEx-BadDefault">In most cases, you will not want to use the
                        default expression. The default expression is more appropriate for changing
                        file metadata.</note>
            </p>
            <p>
                  <draft-comment author="Loretta">note and ph- <uicontrol>HiveQuery</uicontrol>-
                        Used in Hive Query overview and Query topics. </draft-comment>
            </p>
            <p>
                  <note id="note-HiveQuery"><ph id="ph-HiveQuery">The Hive Query executor waits for
                              each query to complete before continuing with the next query for the
                              same event record. It also waits for all queries to complete before
                              starting the queries for the next event record. Depending on the speed
                              of the pipeline and the complexity of the queries, the wait for query
                              completion can slow pipeline performance.</ph></note>
            </p>
            <p>
                  <draft-comment author="Loretta">Pipeline Finisher > Event Generating Stages and
                        Case Study: Stop the Pipeline include this paragraph w/list.</draft-comment>
            </p>
            <p id="p-PFinisher-nomoredataOrigins">The following origins generate no-more-data
                        events:<ul id="ul_l51_mvd_nx">
                        <li>Amazon S3 origin</li>
                        <li>Directory origin</li>
                        <li>Google Cloud Storage origin</li>
                        <li>JDBC Query Consumer origin</li>
                        <li>JDBC Multitable Consumer origin</li>
                        <li>Salesforce origin</li>
                        <li>SQL Server CDC Client origin</li>
                        <li>SQL Server Change Tracking origin</li>
                  </ul></p>
            <p>
                  <draft-comment author="Loretta">Spark executor, Spark Evaluator, Cluster mode -
                        supported Spark version!</draft-comment>
            </p>
            <p><ph id="ph-SparkVersions">Spark versions 1.3 through 2.1</ph></p>
            <p>
                  <draft-comment author="Loretta">Phrases below used in Spark executor, Spark
                        evaluator > Spark versions &amp; stage libraries topics</draft-comment>
            </p>
            <p><ph id="Spark-AppVersion-Stagelib">the Spark version used in the selected stage
                        library matches the Spark version used to build the application.</ph></p>
            <p><ph id="Spark-Stagelib-Cluster">in a cluster streaming pipeline, the Spark version in
                        the selected stage library must also match the Spark version used by the
                        cluster.</ph>
            </p>
            <p><ph id="spark-clusterExample">For example, if your cluster uses Spark 1.6, use a
                        stage library that includes Spark 1.6.</ph>
            </p>
            <p><ph id="Spark-xrefs">available in several CDH and MapR stage libraries. To verify the
                        Spark version that a stage library includes, see the CDH or MapR
                        documentation. For more information about the stage libraries that include
                        the Spark Evaluator, see <xref
                              href="../Installation/AvailableStageLibraries.dita#concept_evs_xkm_s5"
                              product="SDC"/><ph product="DPM"><xref
                                    href="https://streamsets.com/documentation/datacollector/latest/help/#Installation/AddtionalStageLibs.html%23concept_evs_xkm_s5"
                                    format="html" scope="external">Available Stage Libraries</xref>
                              in the <ph conref="#concept_vhs_5tz_xp/pName-long"/>
                              documentation</ph>.</ph></p>
            <p>
                  <draft-comment author="Loretta">Used by Spark Evaluator > Installing the
                        Application and Config > Install External Libraries.</draft-comment>
            </p>
            <p><ph id="ph-SparkLibraries">streamsets-datacollector-api,
                        streamsets-datacollector-spark-api, and spark-core libraries</ph></p>
            <p>
                  <draft-comment author="Loretta">YARN prerequisites - Conref the first two
                        DLentries - and adopt the 3rd as needed.</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="YARNminUserID-DLentry">
                              <dt>Configure the YARN Minimum User ID property, min.user.id</dt>
                              <dd>The min.user.id property is set to 1000 by default. To allow job
                                          submission:<ol id="ol_wpb_bcm_zx">
                                          <li>Verify the user ID being used by the <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> user, typically named "sdc".</li>
                                          <li>In Hadoop, configure the YARN min.user.id property.
                                                  <p> Set the property to equal to or lower than the
                                                  <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                  /> user ID.</p></li>
                                    </ol></dd>
                        </dlentry>
                        <dlentry id="YARNallowedSysUser-DLentry">
                              <dt>Configure the YARN Allowed System Users property,
                                    allowed.system.users</dt>
                              <dd>The allowed.system.users property lists allowed user names. To
                                    allow job submission: <ol id="ol_hyg_zdw_cy">
                                          <li> In Hadoop, configure the YARN allowed.system.users
                                                property. <p>Add the <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                  /> user name, typically "sdc", to the list of
                                                  allowed users.</p></li>
                                    </ol></dd>
                        </dlentry>
                        <dlentry>
                              <dt id="YarnMinUserID-DLentry">Configure the &lt;&lt;executor
                                    stage>>Proxy User property</dt>
                              <dd>In the &lt;&lt;executor stage>>, the Proxy User property allows
                                    you to enter a user name for the stage to use when submitting
                                    applications. To allow application submission:<ol
                                          id="ol_f3h_f2w_cy">
                                          <li>In the Spark executor stage, on the
                                                  <wintitle>Spark</wintitle> tab, configure the
                                                  <uicontrol>Proxy User</uicontrol> property.
                                                  <p>Enter a user with an ID that is higher than the
                                                  min.user.id property, or with a user name that is
                                                  listed in the allowed.system.users property.
                                                </p></li>
                                    </ol></dd>
                              <dd>For information about using a Hadoop User, see &lt;&lt;x-ref to
                                    local topic>></dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta">Spark executor tip</draft-comment>
            <p id="SparkAppName-Tip">
                  <note type="tip">Use a name that distinguishes the application from those started
                        by other processes and other pipelines, such as <ph id="SparkAppName-ph"
                              >SDC_&lt;pipeline name>_&lt;app_type></ph>.</note>
            </p>
            <p>
                  <draft-comment author="Loretta">Using CDCoverview in CDC origin
                        overviews.</draft-comment>
            </p>
            <p><ph id="ph-CDCoverview">includes the CRUD operation type in a record header attribute
                        so generated records can be easily processed by CRUD-enabled destinations.
                        For an overview of <ph conref="#concept_vhs_5tz_xp/pName-long"/> changed
                        data processing and a list of CRUD-enabled destinations, see <xref
                              href="../Pipeline_Design/CDC-Overview.dita#concept_apw_l2c_ty"
                  />.</ph></p>
            <p>
                  <draft-comment author="Loretta">CDC reusable content.
                              <uicontrol>CDC-useCDCdest</uicontrol> used in MongoDBOplog > CDC
                        header, JDBC Query Consumer > CDC header, </draft-comment>
            </p>
            <p id="CDC-UseCDCdest">If you use a CRUD-enabled destination in the pipeline such as
                  JDBC Producer or Elasticsearch, the destination can use the operation type when
                  writing to destination systems. When necessary, you can use an Expression
                  Evaluator or scripting processors to manipulate the value in the
                  sdc.operation.type header attribute. For an overview of <ph
                        conref="#concept_vhs_5tz_xp/pName-long"/> changed data processing and a list
                  of CRUD-enabled destinations, see <xref
                        href="../Pipeline_Design/CDC-Overview.dita#concept_apw_l2c_ty"/>.</p>
            <p/>
            <draft-comment author="Loretta">CRUD-dest-def is used in Processing Changed Data
                  overview &amp; sdc.operation.type overview</draft-comment>
            <p id="CRUD-dest-def">CRUD-enabled processors and destinations can use the CRUD
                  operation type in the sdc.operation.type header attribute when writing records,
                  enabling the external system to perform the appropriate operation. </p>
            <draft-comment author="alisontaylor">dd's are used in the Define the Operation topic for
                  Elasticsearch and Kudu destinations – Also in JDBC Producer - - make all changes
                  here to JDBC Tee > Define the CRUD Operation</draft-comment>
            <dl>
                  <dlentry>
                        <dt>Record header attribute ** dlentries not used ** </dt>
                        <dd id="RecordHeaderAtt">You can define the CRUD operation in a CRUD
                              operation record header attribute. The destination looks for the CRUD
                              operation to use in the sdc.operation.type record header attribute. </dd>
                        <dd id="DefineRecordHeaderAtt">If your pipeline includes a CRUD-enabled
                              origin that processes changed data, the destination simply reads the
                              operation type from the sdc.operation.type header attribute that the
                              origin generates. If your pipeline uses a non-CDC origin, you can use
                              the Expression Evaluator or a scripting processor to define the record
                              header attribute. For more information about <ph
                                    conref="#concept_vhs_5tz_xp/pName-long"/> changed data
                              processing and a list of CDC-enabled origins, see <xref
                                    href="../Pipeline_Design/CDC-Overview.dita#concept_apw_l2c_ty"
                              />.</dd>
                  </dlentry>
                  <dlentry>
                        <dt>Default operation ** dlentries not used ** </dt>
                        <dd id="DefaultOperation">You define a default operation in the destination
                              properties. The destination uses the default operation when the
                              sdc.operation.type record header attribute is not set.</dd>
                        <dd id="UnsupportedOp">You can also define how to handle records with
                              unsupported operations defined in the sdc.operation.type header
                              attribute. The destination can discard them, send them to error, or
                              use the default operation. </dd>
                  </dlentry>
            </dl>
            <p>
                  <draft-comment author="Loretta">These paragraphs used in JDBC Tee and JDBC
                        Producer overviews</draft-comment>
            </p>
            <p><ph id="Dest-CRUDoverview">can use CRUD operations defined in the sdc.operation.type
                        record header attribute to write data. You can define a default operation
                        for records without the header attribute or value. You can also configure
                        whether to use multi-row operations for inserts and deletes, and how to
                        handle records with unsupported operations.</ph></p>
            <p><ph id="Dest-ChangeLog">When processing data from a CDC-enabled origin, you can
                        specify the origin change log to aid record processing. </ph></p>
            <p><ph id="CDC-xref">For information about <ph conref="#concept_vhs_5tz_xp/pName-long"/>
                        change data processing and a list of CDC-enabled origins, see <xref
                              href="../Pipeline_Design/CDC-Overview.dita#concept_apw_l2c_ty"
                  />.</ph></p>
            <p/>
            <draft-comment author="Loretta">used in references to field attributes - XML parser, XML
                  data format descriptions...</draft-comment>
            <p><ph id="ph-FieldAtts-SDCRecord">Field attributes are automatically included in
                        records written to destination systems only when you use the SDC RPC data
                        format in the destination.</ph>
            </p>
            <p id="p-FieldAtts-Include">To include field attributes in the record data or to use
                  field attributes in calculations, use the <codeph>record:fieldAttribute</codeph>
                  and <codeph>record:fieldAttributeOrDefault</codeph> functions. For more
                  information about field attribute functions, see <xref
                        href="../Expression_Language/RecordFunctions.dita#concept_p1z_ggv_1r"/>.</p>
            <p><ph id="warn_FieldRecHeaderAtts">Field attributes and record header attributes are
                        written to destination systems automatically only when you use the SDC RPC
                        data format in destinations. For more information about working with field
                        attributes and record header attributes, and how to include them in records,
                        see <xref href="../Pipeline_Design/FieldAttributes.dita#concept_xfm_wtp_1z"
                        /> and <xref
                              href="../Pipeline_Design/RecordHeaderAttributes.dita#concept_wn2_jcz_dz"
                        />.</ph></p>
            <draft-comment author="alisontaylor">Used in the OAuth 2 Authorization topic for HTTP
                  Client origin, processor, and destination</draft-comment>
            <p id="OAuth2_Credentials">The credentials that you enter to request an access token
                  depend on the credentials grant type required by the HTTP service. You can define
                  the following OAuth 2 credentials grant types for HTTP Client: <dl>
                        <dlentry>
                              <dt>Client credentials grant</dt>
                              <dd>
                                    <p>HTTP Client sends its own credentials - the client ID and
                                          client secret or the basic, digest, or universal
                                          authentication credentials - to the HTTP service. For
                                          example, use the client credentials grant to process data
                                          from the Twitter API or from the Microsoft Azure Active
                                          Directory (Azure AD) API.</p>
                                    <p>For more information about the client credentials grant, see
                                                <xref
                                                href="https://tools.ietf.org/html/rfc6749#section-4.4"
                                                format="html" scope="external"/>.</p>
                              </dd>
                        </dlentry>
                        <dlentry>
                              <dt>Resource owner password credentials grant</dt>
                              <dd>
                                    <p>HTTP Client sends the credentials for the resource owner -
                                          the resource owner username and password - to the HTTP
                                          service. Or, you can use this grant type to migrate
                                          existing clients using basic, digest, or universal
                                          authentication to OAuth 2 by converting the stored
                                          credentials to an access token.</p>
                                    <p>For example, use this grant to process data from the Getty
                                          Images API. For more information about using OAuth 2 to
                                          connect to the Getty Images API, see <xref
                                                href="http://developers.gettyimages.com/api/docs/v3/oauth2.html"
                                                format="html" scope="external"
                                                >http://developers.gettyimages.com/api/docs/v3/oauth2.html</xref>.</p>
                                    <p>For more information about the resource owner password
                                          credentials grant, see <xref
                                                href="https://tools.ietf.org/html/rfc6749#section-4.3"
                                                format="html" scope="external"/>.</p>
                              </dd>
                        </dlentry>
                        <dlentry>
                              <dt>JSON Web Tokens (JWT)</dt>
                              <dd>
                                    <p>HTTP Client sends a JSON-based security token encoding to the
                                          HTTP service. For example, use JSON Web Tokens to process
                                          data from the Google API.</p>
                              </dd>
                        </dlentry>
                  </dl></p>
            <p id="OAuth2_Examples">Let’s look at some examples of how to configure authentication
                  and OAuth 2 authorization to process data from Twitter, Microsoft Azure AD, and
                  Google APIs.</p>
            <draft-comment author="alisontaylor">Note used 2 times in Aggregated Statistics for
                  Pipelines in DPM chapter</draft-comment>
            <note type="important" id="AggStatsNote">For a production environment, use a Kafka
                  cluster, Amazon Kinesis Streams, or MapR Streams to aggregate statistics. Using
                  SDC RPC to aggregate statistics is not highly available and might cause the loss
                  of some data. It should be used for development purposes only.</note>
            <p>
                  <draft-comment author="Loretta">This ph-PermissionsDisabled is used in Permissions
                        overview and Config the Data Collector.</draft-comment>
            </p>
            <p><ph id="ph-PermissionsDisabled">When pipeline permissions are disabled, access to
                        pipelines is based on the roles assigned to the user and its
                  groups.</ph></p>
            <draft-comment author="alisontaylor">This ul is the list of stages that need external
                  libs/drivers to use a JDBC connection. Used in Install External Libs in Config
                  chapter and in JDBC Connections in Troubleshooting chapter</draft-comment>
            <ul id="ul_JDBCStages">
                  <li>JDBC Multitable Consumer origin</li>
                  <li>JDBC Query Consumer origin</li>
                  <li>JMS Consumer origin</li>
                  <li>MySQL Binary Log origin</li>
                  <li>Oracle CDC Client origin</li>
                  <li>SQL Server CDC Client origin</li>
                  <li>SQL Server Change Tracking origin</li>
                  <li>JDBC Lookup processor</li>
                  <li>JDBC Tee processor</li>
                  <li>JDBC Producer destination</li>
                  <li>JMS Producer destination</li>
                  <li>JDBC Query executor</li>
            </ul>
            <draft-comment author="alisontaylor">Use this for any location in the help that mentions
                  these environment variable files.</draft-comment>
            <p id="EnvFileLocation">Modify the <xref
                        href="../Configuration/DCEnvironmentConfig.dita#concept_rng_qym_qr"
                        >environment configuration file</xref> used by your installation type.</p>
            <draft-comment author="alisontaylor">Use this for the pipeline --name option for any
                  commands in the CLI docs</draft-comment>
            <p id="CliPipelineName"><ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                  /> generates the ID when the pipeline is created. <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                  /> uses the alphanumeric characters entered for the pipeline title as a prefix for
                  the generated pipeline ID.</p>
            <draft-comment author="alisontaylor">Used in the Credentials topic for MongoDB origin,
                  MongoDB Oplog origin, and MongoDB destination</draft-comment>
            <p id="MongoDBCredentials">To use username/password or LDAP authentication, enter the
                  required credentials in one of the following ways:<dl>
                        <dlentry>
                              <dt>Connection string in the MongoDB tab</dt>
                              <dd>Enter credentials in the connection string in the MongoDB
                                    tab.</dd>
                              <dd>To enter credentials for username/password authentication, enter
                                    the username and password before the host name. Use the
                                    following
                                    format:<codeblock>mongodb://<b>username:password@</b>host[:port][/[database][?options]]</codeblock></dd>
                              <dd>To enter credentials for LDAP authentication, enter the username
                                    and password before the host name, and set the authMechanism
                                    option to PLAIN. Use the following
                                    format:<codeblock>mongodb://<b>username:password@</b>host[:port][/[database]<b>?authMechanism=PLAIN</b></codeblock></dd>
                        </dlentry>
                        <dlentry>
                              <dt>Credentials tab</dt>
                              <dd>Select either the Username/Password or LDAP authentication type in
                                    the Credentials tab. Then enter the username and password for
                                    the authentication type. </dd>
                        </dlentry>
                  </dl></p>
            <p id="MongoDBCredentialsConclusion">If you enter credentials in both the connection
                  string and in the Credentials tab, the Credentials tab takes precedence.</p>
            <draft-comment author="alisontaylor">This text is used in Configuring a Pipeline and in
                  Adding Labels to a Pipeline</draft-comment>
            <p id="NestedLabels">You can use nested labels to create a hierarchy of pipeline
                  groupings. Enter nested labels using the following
                  format:<codeblock>&lt;label1>/&lt;label2>/&lt;label3></codeblock>For example, you
                  might want to group pipelines in the test environment by the origin system. You
                  add the labels Test/HDFS and Test/Elasticsearch to the appropriate pipelines.</p>
            <draft-comment author="alisontaylor">The following dl is used in the Credentials topic
                  for Google BigQuery origin, Google Pub/Sub Publisher origin, and Google Pub/Sub
                  Subscriber destination</draft-comment>
            <dl id="dl_GoogleCredentials">
                  <dlentry>
                        <dt>Default credentials provider</dt>
                        <dd>Use the Google Application Default Credentials. The stage checks for the
                              credentials file defined in the
                                    <codeph>GOOGLE_APPLICATION_CREDENTIALS</codeph> environment
                              variable. If the environment variable doesn't exist and <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                              /> is running as an image in Google Cloud, the stage uses the built-in
                              service account associated with the virtual machine instance. </dd>
                        <dd>
                              <p>For more information about the default credentials, see <xref
                                          href="https://developers.google.com/identity/protocols/application-default-credentials"
                                          format="html" scope="external">Google Application Default
                                          Credentials</xref> in the Google Developer
                                    documentation.</p>
                        </dd>
                        <dd>Complete the following steps to define the credentials file in the
                              environment variable:<ol id="ol_cvy_djq_v1b">
                                    <li>Use the Google Cloud Platform Console or the
                                                <codeph>gcloud</codeph> command-line tool to create
                                          a Google service account and have your application use it
                                          for API access.<p>For example, to use the command line
                                                tool, run the following
                                                commands:<codeblock>gcloud iam service-accounts create my-account
gcloud iam service-accounts keys create key.json --iam-account=my-account@my-project.iam.gserviceaccount.com</codeblock></p></li>
                                    <li>Store the generated credentials file on the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                          /> machine. </li>
                                    <li>In the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                          /> environment configuration file located in the
                                                <codeph>$SDC_DIST/libexec</codeph> directory, add
                                          the <codeph>GOOGLE_APPLICATION_CREDENTIALS</codeph>
                                          environment variable and point it to the credentials
                                                file.<p
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/EnvFileLocation"
                                                /><p>Set the environment variable as
                                          follows:</p><codeblock>export GOOGLE_APPLICATION_CREDENTIALS="/var/lib/sdc-resources/keyfile.json"</codeblock></li>
                                    <li>Restart <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                          /> to enable the changes.</li>
                                    <li>On the <uicontrol>Credentials</uicontrol> tab for the stage,
                                          select <uicontrol>Default Credentials Provider</uicontrol>
                                          for the credentials provider. </li>
                              </ol></dd>
                  </dlentry>
                  <dlentry>
                        <dt>Service account credentials file (JSON)</dt>
                        <dd>Use the Google Cloud service account credentials file and define the
                              path to the file in the stage properties. Complete the following steps
                              to use the service account credentials file:</dd>
                        <dd>
                              <ol id="ol_GoogleCredentialsFile">
                                    <li>Generate a service account credentials file in JSON
                                                format.<p>Use the Google Cloud Platform Console or
                                                the <codeph>gcloud</codeph> command-line tool to
                                                generate and download the credentials file. For more
                                                information, see <xref
                                                  href="https://cloud.google.com/storage/docs/authentication#generating-a-private-key"
                                                  format="html" scope="external">generating a
                                                  service account credential</xref> in the Google
                                                Cloud Platform documentation.</p></li>
                                    <li>Store the generated credentials file on the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                          /> machine. <p>As a best practice, store the file in the
                                                  <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> resources directory,
                                                  <codeph>$SDC_RESOURCES</codeph>. </p></li>
                                    <li>On the <uicontrol>Credentials</uicontrol> tab for the stage,
                                          select <uicontrol>Service Account Credentials
                                                File</uicontrol> for the credentials provider and
                                          enter the path to the credentials file.</li>
                              </ol>
                        </dd>
                  </dlentry>
            </dl>
      </conbody>
</concept>
