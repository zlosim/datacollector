
<!DOCTYPE html
  PUBLIC "" "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:whc="http://www.oxygenxml.com/webhelp/components" xml:lang="en-us" lang="en-us">
    <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><link rel="shortcut icon" href="../../../oxygen-webhelp/template/images/favicon.png"><!----></link><link rel="icon" href="../../../oxygen-webhelp/template/images/favicon.png"><!----></link>        
      <meta name="copyright" content="(C) Copyright 2018" /><meta name="DC.rights.owner" content="(C) Copyright 2018" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="What's New" /><meta name="abstract" content="" /><meta name="description" content="Data Collector version 3.4.0 includes the following new features and enhancements: Origins New PostgreSQL CDC Client origin - Use the PostgreSQL CDC Client origin to process change data capture ..." /><meta name="DC.Relation" scheme="URI" content="../../../datacollector/UserGuide/Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" /><meta name="DC.Relation" scheme="URI" content="../../../datacollector/UserGuide/Installation/Install_title.html" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="prodname" content="Data Collector" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="version" content="1" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="release" content="0" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="modification" content="0 Beta 2" /><meta name="DC.Date.Created" content="2014-10-31" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_hz3_5fk_fy" /><title>What's New</title><!--  Generated with Oxygen version 20.0-SNAPSHOT, build number 2018042310.  --><meta name="wh-path2root" content="../../../" /><meta name="wh-toc-id" content="concept_hz3_5fk_fy-d46e557" />         
        
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!-- Latest compiled and minified Bootstrap CSS -->
        <link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/lib/bootstrap/css/bootstrap.min.css" />

        <!-- Bootstrap Optional theme -->
        <link rel="stylesheet" href="../../../oxygen-webhelp/lib/bootstrap/css/bootstrap-theme.min.css" />
        <link rel="stylesheet" href="../../../oxygen-webhelp/lib/jquery-ui/jquery-ui.min.css" />

        <!-- Template default styles  -->
        <link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/app/topic-page.css?buildId=2018042310" />
        

        <script type="text/javascript" src="../../../oxygen-webhelp/lib/jquery/jquery-3.1.1.min.js"><!----></script>

        <script data-main="../../../oxygen-webhelp/app/topic-page.js" src="../../../oxygen-webhelp/lib/requirejs/require.js"></script>
        
        <!-- Skin resources -->
        <link rel="stylesheet" type="text/css" href="../../../oxygen-webhelp/template/light.css?buildId=2018042310" />
        <!-- EXM-36950 - Expand the args.hdf parameter here -->
        
        
    <link rel="stylesheet" type="text/css" href="../../../skin.css" /></head>

    <body class="wh_topic_page frmBody">
        <!-- EXM-36950 - Expand the args.hdr parameter here -->
        
        
        
<nav class="navbar navbar-default wh_header">
    <div class="container-fluid">
        <div class="wh_header_flex_container">
            <div class="wh_logo_and_publication_title_container">
                <div class="wh_logo_and_publication_title">
                    
                    <!--
                            This component will be generated when the next parameters are specified in the transformation scenario:
                            'webhelp.logo.image' and 'webhelp.logo.image.target.url'.
                            See: http://oxygenxml.com/doc/versions/17.1/ug-editor/#topics/dita_webhelp_output.html.
                    -->
                    <a href="../../../index.html" class=" wh_logo hidden-xs "></a>
                    <div class=" wh_publication_title "><a href="../../../index.html"><span class="booktitle">  <span class="ph mainbooktitle"><span class="ph">Data Collector</span> User Guide</span>  </span></a></div>
                    
                </div>
                
                <!-- The menu button for mobile devices is copied in the output only when the 'webhelp.show.top.menu' parameter is set to 'yes' -->
                
            </div>

            <div class="wh_top_menu_and_indexterms_link collapse navbar-collapse">
                
                
                <div class=" wh_indexterms_link "><a href="../../../indexTerms.html" title="Index"><span>Index</span></a></div>
                
            </div>
        </div>
    </div>
</nav>

        <div class=" wh_search_input "><form id="searchForm" method="get" action="../../../search.html"><div><input type="search" placeholder="Search " class="wh_search_textfield" id="textToSearch" name="searchQuery" /><button type="submit" class="wh_search_button"><span>Search</span></button></div><script><!--
                                    $(document).ready(function () {
                                        $('#searchForm').submit(function (e) {
                                            if ($('.wh_search_textfield').val().length < 1) {
                                                e.preventDefault();
                                            }
                                        });
                                    });
                                --></script></form></div>
        
        <div class="container-fluid">
            <div class="row">

                <nav class="wh_tools hidden-print">
                    <div data-tooltip-position="bottom" class=" wh_breadcrumb "><ol xmlns:html="http://www.w3.org/1999/xhtml" class="hidden-print"><li><span class="home"><a href="../../../index.html"><span>Home</span></a></span></li>
   <li class="active"><span class="topicref" data-id="concept_hz3_5fk_fy"><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_hz3_5fk_fy">What's New</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
</ol></div>

                    <div class="wh_right_tools hidden-sm hidden-xs">
                        <div class=" wh_navigation_links "><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../../../datacollector/UserGuide/Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"></a></span>  
<span class="navnext"><a class="link" href="../../../datacollector/UserGuide/Installation/Install_title.html" title="Installation"></a></span>  </span></div>
                        <button class="wh_hide_highlight" title="Toggle search highlights"></button>
                        <button class="webhelp_expand_collapse_sections" data-next-state="collapsed" title="Collapse sections"></button>
                        <div class=" wh_print_link print "><a href="javascript:window.print();" title="Print this page"></a></div>
                    </div>
                </nav>
            </div>

            <div class="wh_content_area">
                <div class="row">
                    
                        <nav role="navigation" id="wh_publication_toc" class="col-lg-3 col-md-3 col-sm-3 hidden-xs navbar hidden-print">
                            <div class=" wh_publication_toc " data-tooltip-position="right"><ul>
   <li><span data-tocid="concept_htw_ghg_jq-d46e54" class="topicref" data-id="concept_htw_ghg_jq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq">Getting Started</a></span></span></li>
   <li class="active"><span data-tocid="concept_hz3_5fk_fy-d46e557" class="topicref" data-id="concept_hz3_5fk_fy" data-state="expanded"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_hz3_5fk_fy">What's New</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span><ul class="nav nav-list">
         <li><span data-tocid="concept_ynr_5lt_m2b-d46e833" class="topicref" data-id="concept_ynr_5lt_m2b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_ynr_5lt_m2b">What's New in 3.4.0</a></span></span></li>
         <li><span data-tocid="concept_gbv_rcr_h2b-d46e855" class="topicref" data-id="concept_gbv_rcr_h2b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_gbv_rcr_h2b">What's New in 3.3.1</a></span></span></li>
         <li><span data-tocid="concept_k42_pbc_xdb-d46e887" class="topicref" data-id="concept_k42_pbc_xdb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_k42_pbc_xdb">What's New in 3.3.0</a></span></span></li>
         <li><span data-tocid="concept_yv1_cm2_pdb-d46e929" class="topicref" data-id="concept_yv1_cm2_pdb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_yv1_cm2_pdb">What's New in 3.2.0.0</a></span></span></li>
         <li><span data-tocid="concept_ozm_kxk_fdb-d46e981" class="topicref" data-id="concept_ozm_kxk_fdb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_ozm_kxk_fdb">What's New in 3.1.2.0</a></span></span></li>
         <li><span data-tocid="concept_g3c_fn2_ycb-d46e1043" class="topicref" data-id="concept_g3c_fn2_ycb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_g3c_fn2_ycb">What's New in 3.1.1.0</a></span></span></li>
         <li><span data-tocid="concept_agl_4tw_scb-d46e1115" class="topicref" data-id="concept_agl_4tw_scb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_agl_4tw_scb">What's New in 3.1.0.0</a></span></span></li>
         <li><span data-tocid="concept_z4b_qrb_4cb-d46e1198" class="topicref" data-id="concept_z4b_qrb_4cb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_z4b_qrb_4cb">What's New in 3.0.3.0</a></span></span></li>
         <li><span data-tocid="concept_lhn_cvv_lcb-d46e1291" class="topicref" data-id="concept_lhn_cvv_lcb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_lhn_cvv_lcb">What's New in 3.0.2.0</a></span></span></li>
         <li><span data-tocid="concept_lgz_yp4_gcb-d46e1394" class="topicref" data-id="concept_lgz_yp4_gcb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_lgz_yp4_gcb">What's New in 3.0.1.0</a></span></span></li>
         <li><span data-tocid="concept_cjx_y4k_wbb-d46e1508" class="topicref" data-id="concept_cjx_y4k_wbb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_cjx_y4k_wbb">What's New in 3.0.0.0</a></span></span></li>
         <li><span data-tocid="concept_rrq_v3k_kbb-d46e1631" class="topicref" data-id="concept_rrq_v3k_kbb" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_rrq_v3k_kbb">What's New in 2.7.2.0</a></span></span></li>
         <li><span data-tocid="concept_rr2_mbz_w1b-d46e1764" class="topicref" data-id="concept_rr2_mbz_w1b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_rr2_mbz_w1b">What's New in 2.7.1.1</a></span></span></li>
         <li><span data-tocid="unique_887908705-d46e1907" class="topicref" data-id="unique_887908705" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#unique_887908705">What's New in 2.7.1.0</a></span></span></li>
         <li><span data-tocid="unique_468165943-d46e2060" class="topicref" data-id="unique_468165943" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#unique_468165943">What's New in 2.7.0.0</a></span></span></li>
         <li><span data-tocid="concept_avm_x1y_h1b-d46e2223" class="topicref" data-id="concept_avm_x1y_h1b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_avm_x1y_h1b">What's New in 2.6.0.1</a></span></span></li>
         <li><span data-tocid="concept_bsw_cky_11b-d46e2396" class="topicref" data-id="concept_bsw_cky_11b" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_bsw_cky_11b">What's New in 2.6.0.0</a></span></span></li>
         <li><span data-tocid="concept_afr_hly_tz-d46e2579" class="topicref" data-id="concept_afr_hly_tz" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_afr_hly_tz">What's New in 2.5.1.0</a></span></span></li>
         <li><span data-tocid="concept_ddx_bpm_pz-d46e2773" class="topicref" data-id="concept_ddx_bpm_pz" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_ddx_bpm_pz">What's New in 2.5.0.0</a></span></span></li>
         <li><span data-tocid="concept_cf2_sdz_fz-d46e2977" class="topicref" data-id="concept_cf2_sdz_fz" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_cf2_sdz_fz">What's New in 2.4.1.0</a></span></span></li>
         <li><span data-tocid="concept_kzc_4sd_yy-d46e3191" class="topicref" data-id="concept_kzc_4sd_yy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_kzc_4sd_yy">What's New in 2.4.0.0</a></span></span></li>
         <li><span data-tocid="concept_bml_dbt_wy-d46e3416" class="topicref" data-id="concept_bml_dbt_wy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_bml_dbt_wy">What's New in 2.3.0.1</a></span></span></li>
         <li><span data-tocid="concept_yym_xqt_5y-d46e3650" class="topicref" data-id="concept_yym_xqt_5y" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_yym_xqt_5y">What's New in 2.3.0.0</a></span></span></li>
         <li><span data-tocid="concept_wbf_dgk_fy-d46e3894" class="topicref" data-id="concept_wbf_dgk_fy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_wbf_dgk_fy">What's New in 2.2.1.0</a></span></span></li>
         <li><span data-tocid="concept_oyv_zfk_fy-d46e4148" class="topicref" data-id="concept_oyv_zfk_fy" data-state="leaf"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/WhatsNew/WhatsNew_Title.html#concept_oyv_zfk_fy">What's New in 2.2.0.0</a></span></span></li>
      </ul>
   </li>
   <li><span data-tocid="concept_l4q_flb_kr-d46e4414" class="topicref" data-id="concept_l4q_flb_kr" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Installation/Install_title.html">Installation</a></span></span></li>
   <li><span data-tocid="concept_ylh_yyz_ky-d46e6481" class="topicref" data-id="concept_ylh_yyz_ky" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Configuration/Config_title.html">Configuration</a></span></span></li>
   <li><span data-tocid="concept_ejk_f1f_5v-d46e13895" class="topicref" data-id="concept_ejk_f1f_5v" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Upgrade/Upgrade_title.html">Upgrade</a></span></span></li>
   <li><span data-tocid="concept_qsw_cjy_bt-d46e18499" class="topicref" data-id="concept_qsw_cjy_bt" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Design/PipelineDesign_title.html">Pipeline Concepts and Design</a></span></span></li>
   <li><span data-tocid="concept_qn1_wn4_kq-d46e20154" class="topicref" data-id="concept_qn1_wn4_kq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Configuration/PipelineConfiguration_title.html">Pipeline Configuration</a></span></span></li>
   <li><span data-tocid="concept_hdr_gyw_41b-d46e22651" class="topicref" data-id="concept_hdr_gyw_41b" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Data_Formats/DataFormats-Title.html">Data Formats</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_yjl_nc5_jq-d46e24466" class="topicref" data-id="concept_yjl_nc5_jq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Origins/Origins_title.html">Origins</a></span></span></li>
   <li><span data-tocid="concept_yjl_nc5_jq-d46e64169" class="topicref" data-id="concept_yjl_nc5_jq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Processors/Processors_title.html">Processors</a></span></span></li>
   <li><span data-tocid="concept_agj_cfj_br-d46e77391" class="topicref" data-id="concept_agj_cfj_br" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Destinations/Destinations-title.html">Destinations</a></span></span></li>
   <li><span data-tocid="concept_umc_1lk_fx-d46e92871" class="topicref" data-id="concept_umc_1lk_fx" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Executors/Executors-title.html">Executors</a></span></span></li>
   <li><span data-tocid="concept_ugp_kwf_xw-d46e98772" class="topicref" data-id="concept_ugp_kwf_xw" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/DPM/DPM_title.html">StreamSets Control Hub</a></span></span></li>
   <li><span data-tocid="concept_xxd_f5r_kx-d46e102096" class="topicref" data-id="concept_xxd_f5r_kx" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx">Dataflow Triggers</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_fjj_zcf_2w-d46e106011" class="topicref" data-id="concept_fjj_zcf_2w" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Hive_Drift_Solution/HiveDriftSolution_title.html#concept_fjj_zcf_2w">Drift Synchronization Solution for Hive</a></span></span></li>
   <li><span data-tocid="concept_kgt_pnr_4cb-d46e108837" class="topicref" data-id="concept_kgt_pnr_4cb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/JDBC_DriftSolution/JDBC_DriftSyncSolution_title.html#concept_kgt_pnr_4cb">Drift Synchronization Solution for PostgreSQL</a></span></span></li>
   <li><span data-tocid="concept_wwq_gxc_py-d46e109636" class="topicref" data-id="concept_wwq_gxc_py" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py">Multithreaded Pipelines</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_fyf_gkq_4bb-d46e110218" class="topicref" data-id="concept_fyf_gkq_4bb" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Edge_Mode/EdgePipelines_title.html">Edge Pipelines</a></span></span></li>
   <li><span data-tocid="concept_wr1_ktz_bt-d46e111818" class="topicref" data-id="concept_wr1_ktz_bt" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/RPC_Pipelines/SDC_RPCpipelines_title.html#concept_wr1_ktz_bt">SDC RPC Pipelines</a></span></span></li>
   <li><span data-tocid="concept_fpz_5r4_vs-d46e112299" class="topicref" data-id="concept_fpz_5r4_vs" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Cluster_Mode/ClusterPipelines_title.html">Cluster Pipelines</a></span></span></li>
   <li><span data-tocid="concept_jjk_23z_sq-d46e113641" class="topicref" data-id="concept_jjk_23z_sq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Data_Preview/DataPreview_Title.html#concept_jjk_23z_sq">Data Preview</a></span></span></li>
   <li><span data-tocid="concept_pgk_brx_rr-d46e114610" class="topicref" data-id="concept_pgk_brx_rr" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Alerts/RulesAlerts_title.html#concept_pgk_brx_rr">Rules and Alerts</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_asx_fdz_sq-d46e117238" class="topicref" data-id="concept_asx_fdz_sq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Monitoring/PipelineMonitoring_title.html#concept_asx_fdz_sq">Pipeline Monitoring</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_o3l_dtr_5q-d46e118495" class="topicref" data-id="concept_o3l_dtr_5q" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Pipeline_Maintenance/PipelineMaintenance_title.html#concept_o3l_dtr_5q">Pipeline Maintenance</a></span></span></li>
   <li><span data-tocid="concept_yms_ftm_sq-d46e120286" class="topicref" data-id="concept_yms_ftm_sq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Administration/Administration_title.html#concept_yms_ftm_sq">Administration</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_nls_w1r_ks-d46e124782" class="topicref" data-id="concept_nls_w1r_ks" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Tutorial/Tutorial-title.html">Tutorial</a></span></span></li>
   <li><span data-tocid="concept_sh3_frm_tq-d46e125996" class="topicref" data-id="concept_sh3_frm_tq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Troubleshooting/Troubleshooting_title.html#concept_sh3_frm_tq">Troubleshooting</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_xbx_rs1_tq-d46e129904" class="topicref" data-id="concept_xbx_rs1_tq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Glossary/Glossary_title.html#concept_xbx_rs1_tq">Glossary</a></span></span></li>
   <li><span data-tocid="concept_jn1_nzb_kv-d46e129959" class="topicref" data-id="concept_jn1_nzb_kv" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Apx-DataFormats/DataFormat_Title.html#concept_jn1_nzb_kv">Data Formats by Stage</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_pvm_yt3_wq-d46e130119" class="topicref" data-id="concept_pvm_yt3_wq" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Expression_Language/ExpressionLanguage_title.html">Expression Language</a></span></span></li>
   <li><span data-tocid="concept_vcj_1ws_js-d46e131600" class="topicref" data-id="concept_vcj_1ws_js" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Apx-RegEx/RegEx-Title.html#concept_vcj_1ws_js">Regular Expressions</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
   <li><span data-tocid="concept_chv_vmj_wr-d46e131822" class="topicref" data-id="concept_chv_vmj_wr" data-state="not-ready"><span class="wh-expand-btn"></span><span class="title"><a href="../../../datacollector/UserGuide/Apx-GrokPatterns/GrokPatterns_title.html#concept_chv_vmj_wr">Grok Patterns</a><span class="wh-tooltip">
               
               <p xmlns:toc="http://www.oxygenxml.com/ns/webhelp/toc" xmlns:xhtml="http://www.w3.org/1999/xhtml" class="shortdesc"></p>
               </span></span></span></li>
</ul></div>
                        </nav>
                    
                    
                    <div class="col-lg-9 col-md-9 col-sm-9 col-xs-12" id="wh_topic_body">
                        <div class=" wh_topic_content body "><main role="main"><article role="article" aria-labelledby="ariaid-title1"><article class="nested0" aria-labelledby="ariaid-title1" id="concept_hz3_5fk_fy">
 <h1 class="title topictitle1" id="ariaid-title1">What's New</h1>

 
 <div class="body conbody"><p class="shortdesc"></p>

  <p class="p"></p>

 </div>

<article class="topic concept nested1" aria-labelledby="ariaid-title2" id="concept_ynr_5lt_m2b">
    <h2 class="title topictitle2" id="ariaid-title2">What's New in 3.4.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.4.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_ohb_jmt_m2b">
                        <li class="li"><a class="xref" href="../Origins/PostgreSQL.html#concept_cfs_4m4_n2b" title="The PostgreSQL CDC Client origin processes Write-Ahead Logging (WAL) data to generate change data capture records for a PostgreSQL database. Use the PostgreSQL CDC Client origin to process WAL data from PostgreSQL 9.4 or later. Earlier versions do not support WAL.">New
                                PostgreSQL CDC Client origin</a> - Use the PostgreSQL CDC Client
                            origin to process change data capture information for a PostgreSQL
                            database.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/TestOrigin.html#concept_sgt_s5v_g2b" title="A test origin can provide test data for data preview to aid in pipeline development. In Control Hub, you can also use test origins when developing pipeline fragments. Test origins are not used when running a pipeline.">New
                                Test origin</a>- You can now configure a virtual test origin to
                            provide test data for data preview to aid in pipeline development. In
                                <span class="ph">Control Hub</span>, you can also use test origins when developing pipeline fragments. </li>

                        <li class="li">Amazon S3, Directory, SFTP/FTP Client, Google Cloud Storage enhancements
                            - The listed origins can now process <a class="xref" href="../Data_Formats/Excel.html#concept_w1z_zc1_22b">Microsoft
                                Excel files</a>.</li>

                        <li class="li">Dev Data Generator origin enhancement - The development origin can now
                            generate additional types of data for testing purposes - such as sample
                            address data, names, or prices.</li>

                        <li class="li">Hadoop FS origin enhancements - The origin includes the following
                                enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_ssl_ymt_m2b">
                                <li class="li">Process Amazon S3 data in <a class="xref" href="../Cluster_Mode/AmazonS3Requirements.html#concept_opj_jmf_f2b" title="Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3.">cluster EMR batch mode</a> - Use the origin in a cluster
                                    EMR batch pipeline that runs on an Amazon EMR cluster to process
                                    data from Amazon S3. </li>

                                <li class="li">Process Amazon S3 data in <a class="xref" href="../Cluster_Mode/AmazonS3Requirements.html#concept_opj_jmf_f2b" title="Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3.">cluster batch mode</a> - Use the origin in a cluster
                                    batch pipeline that runs on a Cloudera distribution of Hadoop
                                    (CDH) or Hortonworks Data Platform (HDP) cluster to process data
                                    from Amazon S3.</li>

                            </ul>
</li>

                        <li class="li">HTTP Client origin enhancements - The origin includes the following
                            changes and enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_off_lnt_m2b">
                                <li class="li">The origin now uses <a class="xref" href="../Origins/HTTPClient.html#task_akl_rkz_5r">buffered
                                        request transfer encoding</a> by default. Upgraded
                                    pipelines retain their previous configuration.</li>

                                <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_etl_bsh_l2b" title="The HTTP Client origin generates records based on the responses it receives.">HEAD
                                        request responses create an empty record</a>. Information
                                    returned from the HEAD appear in record header attributes.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_rvj_5qy_4cb">HTTP Server
                                origin enhancement</a> - The origin now includes the name of the
                            client or proxy that made the request in the remoteHost record header
                            attribute.</li>

                        <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_kx3_zrs_ns">MongoDB origin enhancement</a> - You can now use a date field as
                            the offset field.</li>

                        <li class="li">Oracle CDC Client origin enhancements - The origin includes the
                            following changes and enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_esg_f4t_m2b">
                                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_syx_pjy_12b" title="When you configure the origin to use local buffering and to parse the SQL query, you can configure the Oracle CDC Client origin to use multiple threads to parse transactions. You can use multithreaded parsing with both the default Oracle CDC Client parser and the alternate PEG parser.">Multithreaded parsing</a> - When using local caching and
                                    parsing the SQL query, the origin can now use multiple threads
                                    to parse transactions. </li>

                                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_jy5_dyd_12b" title="The Oracle CDC Client origin provides an alternate PEG parser that you can try when concerned about pipeline performance.">PEG Parser</a> - To improve performance for very wide
                                    tables, you can try our experimental PEG parser.</li>

                                <li class="li">With this release, the Query Timeout property has been removed.
                                    You can no longer configure a query to timeout before the end of
                                    a LogMiner session. The existing LogMiner Session Window
                                    property defines how long the session lasts. </li>

                            </ul>
</li>

                        <li class="li">Salesforce origin enhancement - When using the SOAP API, the origin can
                            now execute an SOQL query that includes one or more subqueries. Support
                            for subqueries using the Bulk API will be added in a future release.
                        </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_fxf_s4t_m2b">
                        <li class="li"><a class="xref" href="../Processors/WholeFileTransformer.html#concept_nwg_rx4_l2b" title="The Whole File Transformer processor transforms fully written Avro files to highly efficient, columnar Parquet files. Use the Whole File Transformer in a pipeline that reads Avro files as whole files and writes the transformed Parquet files as whole files.">New Whole File Transformer processor</a> - Use the Whole File
                            Transformer processor to convert fully written Avro files to Parquet in
                            a whole file pipeline. </li>

                        <li class="li"><a class="xref" href="../Processors/FieldHasher.html#concept_pmb_sws_f2b" title="You can configure the Field Hasher processor to add a field separator character to the end of all fields to be hashed. You might want to add a field separator character when you hash multiple fields to a single field or when you hash an entire record.">Field Hasher processor enhancement</a> - The processor can now
                            add a user-defined field separator to fields before hashing. </li>

                        <li class="li">HTTP Client processor enhancements - The processor includes the
                            following changes and enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_rhv_y4t_m2b">
                                <li class="li">The processor now uses <a class="xref" href="../Processors/HTTPClient.html#task_z54_1qr_fw" title="Configure an HTTP Client processor to perform requests against a resource URL.">buffered
                                        request transfer encoding</a> by default. Upgraded
                                    pipelines retain their previous configuration.</li>

                                <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_ry4_nx3_l2b" title="The HTTP Client processor generates records based on the responses it receives.">HEAD
                                        request responses create an empty record</a>. Information
                                    returned from the HEAD appear in record header attributes.</li>

                                <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_kjb_gfz_m2b" title="You can write the resolved resource URL to the Data Collector log.">The
                                        resolved request URL</a> is now written to the <span class="ph">Data Collector</span> log when <span class="ph">Data Collector</span> logging is set to debug or higher.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#concept_vsl_dvt_d2b" title="When using local caching, you can increase the number of threads that the JDBC Lookup processor uses to prepopulate the lookup cache. After the cache is populated, the additional threads are released. This can substantially increase the performance of the processor.">JDBC Lookup processor enhancement</a> - When using local
                            caching, the processor can now use additional cores to prepopulate the
                            cache to enhance pipeline performance. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_ekt_gpt_m2b">
                        <li class="li"><a class="xref" href="../Destinations/Couchbase.html#concept_ahq_1wq_h2b" title="The Couchbase destination writes data to Couchbase Server. Couchbase Server is a distributed NoSQL document-oriented database.">New
                                Couchbase destination</a> - A new destination that writes data to
                            a Couchbase database. </li>

                        <li class="li"><a class="xref" href="../Destinations/Splunk.html#concept_zzr_pqn_xdb" title="The Splunk destination writes data to Splunk using the Splunk HTTP Event Collector (HEC).">New Splunk
                                destination</a> - A new destination that writes data to Splunk
                            using the Splunk HTTP Event Collector (HEC).</li>

                        <li class="li"><a class="xref" href="../Destinations/Cassandra.html#task_t1d_z3l_sr">Cassandra destination enhancement</a> - You can now use SSL/TLS
                            to connect to Cassandra.</li>

                        <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#task_bdf_fk5_lz">HTTP Client
                                destination enhancement</a> - The destination now uses buffered
                            request transfer encoding by default. Upgraded pipelines retain their
                            previous configuration. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_qgm_ppt_m2b">
                        <li class="li">Amazon S3 executor enhancements - The executor includes the following
                                enhancements:<ul class="ul" id="concept_ynr_5lt_m2b__ul_fbw_ppt_m2b">
                                <li class="li">The executor can now <a class="xref" href="../Executors/AmazonS3.html#concept_v5j_pjr_f2b" title="You can use the Amazon S3 executor to copy an object to another location within the same bucket when the executor receives an event record. You can optionally delete the original object after the copy. The object must be under 5 GB in size.">copy objects</a> to a new location and optionally delete
                                    the original object.</li>

                                <li class="li">The executor can now generate event records each time the
                                    executor creates a new object, adds tags to an existing object,
                                    or completes copying an object to a new location.</li>

                            </ul>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm"><span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>)</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_osl_5pt_m2b">
                        <li class="li"><a class="xref" href="../Origins/SystemMetrics.html#concept_gzy_gmv_32b" title="The System Metrics origin reads system metrics from the edge device where StreamSets Data Collector Edge (SDC Edge) is installed. Use the System Metrics origin only in pipelines configured for edge execution mode.">New
                                System Metrics origin</a> - A new origin that reads system
                            metrics - such as CPU and memory usage - from the edge device where <span class="ph">SDC Edge</span> is installed.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_kly_qtq_4bb" title="An edge sending pipeline uses an origin specific to the edge device to read local data residing on the device. The pipeline can perform minimal processing on the data before sending the data to a Data Collector receiving pipeline.">HTTP Client origin supported</a> - Edge sending pipelines​​ now
                            support the HTTP Client origin. However, the origin does not currently
                            support batch processing mode, pagination, or OAuth2 authorization in
                            edge pipelines.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_kly_qtq_4bb" title="An edge sending pipeline uses an origin specific to the edge device to read local data residing on the device. The pipeline can perform minimal processing on the data before sending the data to a Data Collector receiving pipeline.">WebSocket Client origin supported</a> - Edge sending pipelines​​
                            now support the WebSocket Client origin.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_pbm_q4r_4bb" title="Edge pipelines run on SDC Edge which is a lightweight agent without a UI. As a result, some features available for standalone pipelines are not available for edge pipelines at this time. We will provide support for some of these features in edge pipelines in a future release.">Pipeline functions</a> - Edge pipelines now support the
                            following pipeline functions:<ul class="ul" id="concept_ynr_5lt_m2b__ul_ezh_3qt_m2b">
                                <li class="li">pipeline:id()</li>

                                <li class="li">pipeline:title()</li>

                                <li class="li">pipeline:user()</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Manage.html#concept_tqk_dbb_4db" title="After designing edge pipelines in Data Collector and then deploying the edge pipelines to SDC Edge, you can manage the pipelines on SDC Edge. Managing edge pipelines includes previewing, validating, starting, stopping, resetting the origin, and monitoring the pipelines.">Preview and validate edge pipelines</a> - You can now use the
                                <span class="ph">Data Collector</span> UI or the command line and REST API to preview and validate edge
                            pipelines.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Deploy.html#task_obt_d25_32b">Publish
                                multiple edge pipelines to <span class="ph">SDC Edge</span></a> - You can now use the <span class="ph">Data Collector</span> Home page to directly publish multiple edge pipelines at one time to
                            an <span class="ph">SDC Edge</span> that is running. Previously, you could only publish a single edge
                            pipeline at a time.</li>

                        <li class="li"><a class="xref" href="../Edge_Mode/DownloadPipelines.html#task_vsg_wf5_32b" title="When SDC Edge is running and is accessible by the Data Collector machine, you can download edge pipelines from SDC Edge into Data Collector. When you download pipelines from SDC Edge, you download all edge pipelines deployed to that SDC Edge in addition to all sample edge pipelines included with SDC Edge.">Download edge pipelines from <span class="ph">SDC Edge</span></a> - You can now use the <span class="ph">Data Collector</span> UI to download all edge pipelines deployed to an <span class="ph">SDC Edge</span> in addition to all sample edge pipelines included with <span class="ph">SDC Edge</span>.</li>

                        <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#concept_wc5_ysp_1w" title="Data Collector displays a list of all available pipelines and related information on the Home page. You can select a category of pipelines, such as Running Pipelines, to view a subset of all available pipelines.">Filter the Home page by edge pipelines</a> - You can now select
                            Edge Pipelines as a category on the <span class="ph">Data Collector</span> Home page to view all available edge pipelines. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_nyy_rrt_m2b">
                        <li class="li">Microservices pipelines - You can now create Microservices pipelines
                            using a new REST Service origin and new Send to Origin Response
                            destination to build a microservice.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/Notifications.html#concept_mtn_k4j_rz">Notifications​</a> - You can now configure a pipeline to send an
                            email or webhook when the pipeline changes to the Running_Error
                            state.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_itf_55z_dz">Error records</a> - Error records now include an errorJobID
                            internal header attribute when the pipeline that generated the error
                            record was started by a <span class="ph">Control Hub</span> job.</li>

                        <li class="li">Install external libraries from the properties panel - You can now
                            select a stage in the pipeline canvas and then install external
                            libraries for that stage from the properties panel. Previously, you had
                            to navigate to the Package Manager page to install external
                            libraries.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Cluster Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_hp3_bst_m2b">
                        <li class="li"><a class="xref" href="../Cluster_Mode/AmazonS3Requirements.html#concept_opj_jmf_f2b" title="Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3.">New cluster EMR batch mode</a> - <span class="ph">Data Collector</span> can now use the cluster EMR batch mode to run on an Amazon EMR
                            cluster to process data from Amazon S3. <span class="ph">Data Collector</span> runs as an application on top of MapReduce in the EMR cluster.<p class="p"><span class="ph">Data Collector</span> can run on an existing EMR cluster or on a new EMR cluster that
                                is provisioned when the cluster pipeline starts. When you provision
                                a new EMR cluster, you can configure whether the cluster remains
                                active or terminates when the pipeline stops.</p>
<p class="p">Use the Hadoop
                                FS origin to process data from Amazon S3 in cluster EMR batch mode.
                            </p>
</li>

                        <li class="li"><a class="xref" href="../Cluster_Mode/ClusterPipelines.html#concept_iyx_23c_j2b" title="Because cluster pipelines run as either MapReduce or Spark applications, each Data Collector worker in the cluster manages its own log. ">Logs</a> -
                            You can now configure the <span class="ph">Data Collector</span> on the master gateway node to use the log4j rolling file appender to
                            write log messages to an sdc.log file. This configuration is propagated
                            to the worker nodes such that each <span class="ph">Data Collector</span> worker writes log messages to an sdc.log file within the YARN
                            application directory.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Data Formats</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_e3w_hst_m2b">
                        <li class="li"><a class="xref" href="../Data_Formats/Excel.html#concept_w1z_zc1_22b">New Excel
                                data format</a> - You can now use the following file-based
                            origins to process Microsoft Excel files:<ul class="ul" id="concept_ynr_5lt_m2b__ul_yyf_jst_m2b">
                                <li class="li">Amazon S3 origin</li>

                                <li class="li">Directory origin</li>

                                <li class="li">Google Cloud Storage origin</li>

                                <li class="li">SFTP/FTP Client origin</li>

                            </ul>
</li>

                        <li class="li">Avro and Protobuf data formats - To preserve the ordering of fields, the
                            Avro and Protobuf data formats now use the list-map root field type
                            instead of the map root field type. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <div class="p">This version of <span class="ph">Data Collector</span> includes the following new <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage libraries</a>:<ul class="ul" id="concept_ynr_5lt_m2b__ul_j2y_4st_m2b">
                            <li class="li">streamsets-datacollector-cdh_5_15-lib - The Cloudera CDH 5.5
                                distribution of Hadoop.</li>

                            <li class="li">streamsets-datacollector-emr_hadoop_2_8_3-lib - Includes the Hadoop
                                FS origin for cluster EMR batch mode pipelines that run on an Amazon
                                EMR cluster to process data from Amazon S3.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ynr_5lt_m2b__ul_osx_rst_m2b">
                        <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_xt5_pyq_j2b" title="When you have a paid subscription for StreamSets, configure Data Collector to include your StreamSets customer ID in support bundles before submitting a bundle. This enables the StreamSets support team to easily locate and prioritize your bundles.">Cloudera Manager CSD enhancement</a> - The Cloudera Manager CSD
                            now enables specifying a StreamSets Customer ID, used when generating
                            support bundles. The customer ID is generated by the StreamSets Support
                            team for users with a paid subscription.</li>

                        <li class="li">Postgres rename - Postgres CSV and Postgres Text delimited format types
                            are now known as PostgreSQL CSV and PostgreSQL Text, respectively The
                            Postgres Metadata processor is now known as the PostgreSQL Metadata
                            processor. And the Drift Synchronization Solution for Postgres is now
                            known as the Drift Synchronization Solution for PostgreSQL.</li>

                        <li class="li">Documentation enhancement - The online help has a new look and feel. All
                            of the previous documentation remains exactly where you expect it, but
                            it is now easier to view and navigate on smaller devices like your
                            tablet or mobile phone. </li>

                    </ul>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title3" id="concept_gbv_rcr_h2b">
    <h2 class="title topictitle2" id="ariaid-title3">What's New in 3.3.1</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.3.1 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origin</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_err_zcr_h2b">
                            <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_rx3_3hx_4y">JDBC Multitable Consumer origin enhancement</a> - You can
                                now optionally define a schema exclusion pattern to exclude some
                                schemas from being read. The schema exclusion pattern uses a
                                Java-based regular expression, or regex.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_jky_gdr_h2b">
                            <li class="li"><a class="xref" href="../Processors/KuduLookup.html#task_b5b_dyl_p1b">Kudu Lookup processor enhancements</a>:<ul class="ul" id="concept_gbv_rcr_h2b__ul_j5j_mdr_h2b">
                                    <li class="li">You can now configure the Maximum Number of Worker Threads
                                        property to limit the number of threads that the processor
                                        uses. </li>

                                    <li class="li">You can now configure an Admin Operation Timeout property to
                                        determine how many milliseconds to allow for admin-type
                                        operations, such as opening a table or getting a table
                                        schema.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_chx_4dr_h2b">
                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#task_c4x_tmh_4v">Kudu destination enhancements:</a><ul class="ul" id="concept_gbv_rcr_h2b__ul_rnl_pdr_h2b">
                                    <li class="li">You can now configure the Maximum Number of Worker Threads
                                        property to limit the number of threads that the destination
                                        uses. </li>

                                    <li class="li">You can now configure an Admin Operation Timeout property to
                                        determine how many milliseconds to allow for admin-type
                                        operations, such as opening a table or getting a table
                                        schema.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Environment Variables</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_gbv_rcr_h2b__ul_mhd_pkr_h2b">
                            <li class="li">Data Collector now includes a SPARK_KAFKA_VERSION environment
                                variable that is set to 0.10 by default in the Data Collector
                                environment configuration file - <code class="ph codeph">sdc.env.sh</code> or
                                    <code class="ph codeph">sdcd.env.sh</code>. Do not change this environment
                                variable value. This variable is used only when you run <a class="xref" href="../Cluster_Mode/KafkaRequirements.html#task_gmd_msw_yr">cluster streaming mode pipelines</a> on a Cloudera CDH
                                cluster.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title4" id="concept_k42_pbc_xdb">
    <h2 class="title topictitle2" id="ariaid-title4">What's New in 3.3.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.3.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Cluster Pipelines</dt>

                <dd class="dd">
                    <div class="p">
                        <ul class="ul" id="concept_k42_pbc_xdb__ul_nvg_jdr_h2b">
                            <li class="li">When using Spark 2.1 or later and Kafka 0.10.0.0 or later in a
                                cluster pipeline that reads from a Kafka cluster on YARN, you can
                                now enable the pipeline to use <a class="xref" href="../Cluster_Mode/KafkaRequirements.html#concept_bb2_m5p_tdb">Kafka security features</a> such as SSL/TLS and Kerberos
                                authentication.</li>

                        </ul>

                    </div>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_zr1_tcc_xdb">
                        <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#task_u4n_rzk_fbb">WebSocket Client Origin enhancement</a> - You can now configure
                            the origin to send an initial message or command after connecting to the
                            WebSocket server.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_od1_bdc_xdb">
                        <li class="li">New SQL Parser processor - A processor that parses SQL queries. For
                            example, if you set the Parse SQL Query property to false in the Oracle
                            CDC origin, the origin writes the SQL query to an “sql” field that can
                            be parsed by the SQL Parser.</li>

                        <li class="li"><a class="xref" href="../Processors/FieldZip.html#task_nqj_51k_yx">Field Zip processor enhancement</a> - The Continue option for
                            the Field Does Not Exist property is now named Include without
                            Processing.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_bpq_ndc_xdb">
                        <li class="li"><a class="xref" href="../Pipeline_Configuration/Notifications.html#concept_mtn_k4j_rz">Notifications</a> - You can now configure a pipeline to send an
                            email or webhook when the pipeline changes to the Stop_Error state.</li>

                        <li class="li">Preview - The default value of the <a class="xref" href="../Data_Preview/DataPreview_Title.html#task_cxd_p25_qq">Preview Timeout property</a> has been increased to 30,000
                            milliseconds. Previously the default was 10,000 milliseconds.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Edge Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_k42_pbc_xdb__ul_vqx_hdc_xdb">
                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Sensor
                                Reader origin enhancement</a> - This development stage can now
                            generate records with thermal data such as that generated by BCM2835
                            onboard thermal sensors.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <div class="p">
                        <ul class="ul" id="concept_k42_pbc_xdb__ul_xqt_jdr_h2b">
                            <li class="li">
                                <p class="p">This version of Data Collector includes several new, changed, and
                                    removed stage libraries because of the introduction of cluster
                                    streaming mode with support for Kafka security features using
                                    Spark 2.1 or later and Kafka 0.10.0.0 or later. </p>

                                <p class="p">For more information about the changed stage libraries, see <a class="xref" href="../Upgrade/PreUpgrade.html#concept_zgm_vj2_mdb" title="Data Collector version 3.3.0 introduces cluster streaming mode with support for Kafka security features such as SSL/TLS and Kerberos authentication using Spark 2.1 or later and Kafka 0.10.0.0 or later.">Upgrade to Spark 2.1 or Later</a>.</p>

                            </li>

                        </ul>

                    </div>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title5" id="concept_yv1_cm2_pdb">
    <h2 class="title topictitle2" id="ariaid-title5">What's New in 3.2.0.0</h2>

    <div class="body conbody">
        <p class="p"><span class="ph">Data Collector</span>
            version 3.2.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_irn_3m2_pdb">
                        <li class="li"><a class="xref" href="../Origins/HDFSStandalone.html#concept_djz_pdm_hdb" title="The Hadoop FS Standalone origin reads files in HDFS. The origin can use multiple threads to enable the parallel processing of files. The files to be processed must all share a file name pattern and be fully written. You can also configure the origin to read from Azure HDInsight.">New
                                Hadoop FS Standalone origin</a> - Similar to the Directory
                            origin, the Hadoop FS Standalone origin can use multiple threads to read
                            fully-written files. Use this origin in standalone execution mode
                            pipelines to read files in HDFS.</li>

                        <li class="li"><a class="xref" href="../Origins/MapRFSStandalone.html#concept_b43_3qc_mdb" title="The MapR FS Standalone origin reads files in MapR. The origin can use multiple threads to enable the parallel processing of files. The files to be processed must all share a file name pattern and be fully written.">New
                                MapR FS Standalone origin</a> - Similar to the Directory origin,
                            the MapR FS Standalone origin can use multiple threads to read
                            fully-written files. Use this origin in standalone execution mode
                            pipelines to read files in MapR FS. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">New
                                Dev Snapshot Replaying origin</a> - The Dev Snapshot Replaying
                            origin is a development stage that reads records from a downloaded
                            snapshot file.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_mnv_s5r_35" title="The HTTP Client origin processes data differently based on the data format. The origin processes the following types of data:">HTTP Client origin enhancement</a> - You can now configure the
                            origin to process JSON files that include multiple JSON objects or a
                            single JSON array</li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_h4j_4zs_kz">JDBC Multitable Consumer origin enhancements</a> - The origin
                            can now generate table-finished and schema-finished events when it
                            completes processing all rows in a table or schema. You can also
                            configure the number of seconds that the origin delays generating the
                            no-more-data event. You might want to configure a delay if you want the
                            table-finished or schema-finished events to appear in the event stream
                            before the no-more-data event. </li>

                        <li class="li">Oracle CDC Client origin enhancements - The origin includes the
                            following enhancements:<ul class="ul" id="concept_yv1_cm2_pdb__ul_ksy_ym2_pdb">
                                <li class="li">You can set a new Parse SQL Query property to false to skip
                                    parsing the SQL queries. Instead, the origin writes the SQL
                                    query to a “sql” field that can be parsed later. Default is
                                    true, which retains the previous behavior of parsing the SQL
                                    queries.</li>

                                <li class="li">The Send Redo Query property has been renamed. The new name is
                                    Send Redo Query in Headers.</li>

                            </ul>
</li>

                    </ul>

                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_s5l_bn2_pdb">
                        <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_bqt_tl4_sz">TCP
                                Server origin enhancement</a> - You can now use the origin to
                            read the supported <span class="ph">Data Collector</span> data formats when passed in Flume events as Avro messages.</li>

                    </ul>

                </dd>

            
        </dl>

        <dl class="dl">
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_tvz_2n2_pdb">
                        <li class="li"><a class="xref" href="../Processors/HTTPClient.html#concept_t4k_2hh_jw">HTTP Client processor enhancement</a> - You can now use the
                            PATCH method with the processor. </li>

                        <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup processor enhancement</a> - The Retry on Cache Miss
                            property has been renamed to Retry on Missing Value.</li>

                        <li class="li">Kudu Lookup processor enhancement - You can now configure the processor
                            behavior when a lookup returns no value. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_tkd_2p2_pdb">
                        <li class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_zvc_bv5_1r">Local
                                FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_spv_xlc_fv" title="The MapR FS destination writes files to MapR FS. You can write the data to MapR as flat files or Hadoop sequence files.">MapR FS</a>
                            destination enhancements - These destinations now support writing
                            records using the SDC Record format. </li>

                        <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#concept_ry4_ct5_lz">HTTP Client destination enhancement</a> - You can now use the
                            PATCH method with the destination. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_lr5_gp2_pdb">
                        <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_jqk_g4y_mx">MapReduce executor enhancement</a> - You can now use the new
                            Avro to ORC job to convert Avro files to ORC files.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm"><span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>)</dt>

                <dd class="dd">
                    <div class="p"><span class="ph">SDC Edge</span> includes the following enhancements:<ul class="ul" id="concept_yv1_cm2_pdb__ul_qpd_4p2_pdb">
                            <li class="li">JavaScript Evaluator processor supported - Both <a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_kly_qtq_4bb" title="An edge sending pipeline uses an origin specific to the edge device to read local data residing on the device. The pipeline can perform minimal processing on the data before sending the data to a Data Collector receiving pipeline.">edge sending pipelines</a> and <a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_q3j_ytq_4bb" title="An edge receiving pipeline listens for data sent by another pipeline running on Data Collector or on SDC Edge and then acts on that data to control the edge device.">edge receiving pipelines</a> now support the JavaScript
                                Evaluator processor.</li>

                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Deploy.html#task_isp_m2f_3db">Publish edge pipelines to SDC Edge</a> - You can now use the
                                    <span class="ph">Data Collector</span> UI to directly publish edge pipelines to an <span class="ph">SDC Edge</span> that is running. Previously, you had to first export edge
                                pipelines from <span class="ph">Data Collector</span>, and then move them to the <span class="ph">SDC Edge</span> installed on the edge device.</li>

                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelines_Manage.html#concept_tqk_dbb_4db" title="After designing edge pipelines in Data Collector and then deploying the edge pipelines to SDC Edge, you can manage the pipelines on SDC Edge. Managing edge pipelines includes previewing, validating, starting, stopping, resetting the origin, and monitoring the pipelines.">Manage edge pipelines from the Data Collector UI</a> - You
                                can now use the <span class="ph">Data Collector</span> UI to start, monitor, stop, and reset the origin for edge
                                pipelines running on a remote <span class="ph">SDC Edge</span>. Previously, you had to use the command line and REST API to
                                manage edge pipelines on <span class="ph">SDC Edge</span>.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_yv1_cm2_pdb__ul_dvy_2r2_pdb">
                        <li class="li"><a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_pm4_txm_vq">Pipeline error handling enhancement</a> - You can now configure
                            pipelines to write error records to Azure Event Hub.</li>

                        <li class="li">Pipeline runner idle time enhancement - You can configure the number of
                            seconds that a pipeline runner waits before sending an empty batch.</li>

                        <li class="li">Runtime statistics enhancement - Runtime statistics now include the
                            number of empty or idle batches that are generated by the pipeline. </li>

                        <li class="li">Snapshot enhancement - Snapshots now include record header attributes
                            for error records. Previously, snapshots included only the record fields
                            in an error record.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stages</dt>

                <dd class="dd">
                    <div class="p">This version of <span class="ph">Data Collector</span> includes the following new stage library:<ul class="ul" id="concept_yv1_cm2_pdb__ul_u5c_pv2_pdb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Apache Kudu version 1.7</a></li>

                        </ul>
</div>

                </dd>

            
        </dl>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title6" id="concept_ozm_kxk_fdb">
    <h2 class="title topictitle2" id="ariaid-title6">What's New in 3.1.2.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.1.2.0 includes the following new features and enhancements:<ul class="ul" id="concept_ozm_kxk_fdb__ul_bvm_d3l_fdb">
                <li class="li"><a class="xref" href="../Origins/Directory.html#concept_b4d_fym_xv">Directory
                        origin enhancement</a> - When processing files using the Last Modified
                    Timestamp read order, the Directory origin now assesses the change timestamp in
                    addition to the last modified timestamp to establish file processing order.</li>

                <li class="li">Impersonation enhancement for Control Hub - You can now configure Data Collector
                    to use a partial Control Hub user ID for <a class="xref" href="../Configuration/DCConfig.html#concept_rcn_gqk_fdb" title="When Data Collector is registered with Control Hub, you can configure Data Collector to use an abbreviated version of the Control Hub user ID to impersonate a Hadoop user.">Hadoop impersonation mode</a> and <a class="xref" href="../Executors/Shell.html#concept_n2w_txv_vz">shell
                        impersonation mode</a>. Use this feature when Data Collector is
                    registered with Control Hub, and when the Hadoop or target operating system has
                    user name requirements that do not allow using the full Control Hub user
                    ID.</li>

                <li class="li">NetFlow 9 processing enhancement - When processing NetFlow 9 data, Data
                    Collector now includes FIELD_SENDER and FIELD_RECIPIENT fields to include sender
                    and receiver information.</li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title7" id="concept_g3c_fn2_ycb">
    <h2 class="title topictitle2" id="ariaid-title7">What's New in 3.1.1.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.1.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_g3c_fn2_ycb__ul_v4n_r5q_1db">
                            <li class="li"><a class="xref" href="../Origins/Directory.html#task_gfj_ssv_yq" title="Configure a Directory origin to read data from files in a directory.">Directory origin enhancements</a> - The origin includes the
                                following enhancements:<ul class="ul" id="concept_g3c_fn2_ycb__ul_bff_s5q_1db">
                                    <li class="li">The Max Files in Directory property has been renamed to Max
                                        Files Soft Limit. As the name indicates, the property is now
                                        a soft limit rather than a hard limit. As such, if the
                                        directory contains more files than the configured Max Files
                                        Soft Limit, the origin can temporarily exceed the soft limit
                                        and the pipeline can continue running.<p class="p">Previously, this
                                            property was a hard limit. When the directory contained
                                            more files, the pipeline failed.</p>
</li>

                                    <li class="li">The origin includes a new Spooling Period property that
                                        determines the number of seconds to continue adding files to
                                        the processing queue after the maximum files soft limit has
                                        been exceeded.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_g3c_fn2_ycb__ul_c1b_f5q_1db">
                            <li class="li">
                                <a class="xref" href="../Destinations/WaveAnalytics.html#task_mdt_dv3_rx">Einstein Analytics destination enhancement</a> - The Append
                                Timestamp to Alias property is now disabled by default for new
                                pipelines. When disabled, the destination can append, delete,
                                overwrite, or upsert data to an existing dataset. When enabled, the
                                destination creates a new dataset for each upload of data.<p class="p">The
                                    property was added in version 3.1.0.0 and was enabled by
                                    default. Pipelines upgraded from versions earlier than 3.1.0.0
                                    have the property enabled by default.</p>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr destination enhancements</a> - The destination includes
                                the following enhancements:<ul class="ul" id="concept_g3c_fn2_ycb__ul_j1k_dt2_ycb">
                                    <li class="li">The destination now includes an Ignore Optional Fields
                                        property that allows ignoring null values in optional fields
                                        when writing records.</li>

                                    <li class="li">The destination allows you to configure Wait Flush, Wait
                                        Searcher, and Soft Commit properties to tune write
                                        performance.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title8" id="concept_agl_4tw_scb">
    <h2 class="title topictitle2" id="ariaid-title8">What's New in 3.1.0.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.1.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Data Synchronization Solution for Postgres</dt>

                    <dd class="dd">This release includes a beta version of the <a class="xref" href="../JDBC_DriftSolution/JDBC_DriftSyncSolution_title.html#concept_ljq_knr_4cb">Data Synchronization Solution for Postgres</a>. The solution uses
                        the new <a class="xref" href="../Processors/JDBCMetadata.html#concept_lcp_ssh_qcb">Postgres Metadata processor</a> to detect drift in incoming data and
                        automatically create or alter corresponding PostgreSQL tables as needed
                        before the data is written. The solution also leverages the JDBC Producer
                        destination to perform the writes. </dd>

                    <dd class="dd ddexpand">As a beta feature, use the Data Synchronization Solution for Postgres for
                        development or testing only. Do not use the solution in production
                        environments.</dd>

                    <dd class="dd ddexpand">Support for additional databases is planned for future releases. To state a
                        preference, leave a comment <a class="xref" href="https://issues.streamsets.com/browse/SDC-8051" target="_blank"><u class="ph u">on this issue</u></a>.</dd>

                
                
                    <dt class="dt dlterm">Data Collector Edge (SDC Edge)</dt>

                    <dd class="dd">SDC Edge includes the following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_x34_y5w_scb">
                            <li class="li"><a class="xref" href="../Edge_Mode/EdgePipelineTypes.html#concept_kly_qtq_4bb" title="An edge sending pipeline uses an origin specific to the edge device to read local data residing on the device. The pipeline can perform minimal processing on the data before sending the data to a Data Collector receiving pipeline.">Edge sending pipelines</a> now support the following
                                    stages:<ul class="ul" id="concept_agl_4tw_scb__ul_p11_dvw_scb">
                                    <li class="li">Dev Raw Data Source origin</li>

                                    <li class="li">Kafka Producer destination</li>

                                </ul>
</li>

                            <li class="li">Edge pipelines now support the following functions:<ul class="ul" id="concept_agl_4tw_scb__ul_b1t_2vw_scb">
                                    <li class="li">emptyList()</li>

                                    <li class="li">emptyMap()</li>

                                    <li class="li">isEmptyMap()</li>

                                    <li class="li">isEmptyList()</li>

                                    <li class="li">length()</li>

                                    <li class="li">record:attribute()</li>

                                    <li class="li">record:attributeOrDefault()</li>

                                    <li class="li">size()</li>

                                </ul>
</li>

                            <li class="li">When you start SDC Edge, you can now <a class="xref" href="../Edge_Mode/SDCeAdminister.html#concept_ecy_hjx_scb" title="By default, SDC Edge writes log messages at the info severity level to the &lt;SDCEdge_home&gt;/log/edge.log file. To view the logs, simply open the edge.log file in a text editor.">change the
                                    default log directory</a>.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_jxt_jvw_scb">
                            <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_edk_j5t_zw" title="You can use pagination to retrieve a large volume of data from a paginated API.">HTTP Client origin enhancement</a> - You can now configure
                                the origin to use the Link in Response Field pagination type. After
                                processing the current page, this pagination type uses a field in
                                the response body to access the next page. </li>

                            <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y" title="The HTTP Server origin is a multithreaded origin that listens on an HTTP endpoint and processes the contents of all authorized HTTP POST and PUT requests. Use the HTTP Server origin to read high volumes of HTTP POST and PUT requests using multiple threads.">HTTP
                                    Server origin enhancement</a> - You can now use the origin to
                                process the contents of authorized HTTP PUT requests.</li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#concept_qy4_rky_vcb" title="The Kinesis Consumer origin stores offsets - the location where the origin stops reading - in DynamoDB lease tables. You can optionally define tags to apply to a lease table created by the origin. The origin cannot add tags to existing lease tables.">Kinesis Consumer origin enhancement</a> - You can now define
                                tags to apply to the DynamoDB lease table that the origin creates to
                                store offsets.</li>

                            <li class="li"><a class="xref" href="../Origins/MQTTSubscriber.html#concept_fwm_2kn_scb">MQTT Subscriber origin enhancement</a> - The origin now
                                includes a TOPIC_HEADER_NAME record header attribute that includes
                                the topic information for each record.</li>

                            <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_vx3_1gh_scb">MongoDB origin enhancement</a> - The origin now generates a
                                no-more-data event when it has processed all available documents and
                                the configured batch wait time has elapsed.</li>

                            <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_gj4_sjq_qcb">Oracle CDC Client origin enhancement</a> - You can now
                                specify the tables to process by using SQL-like syntax in table
                                inclusion patterns and exclusion patterns.</li>

                            <li class="li">Salesforce origin enhancements - The origin includes the following
                                    enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_mfg_wvw_scb">
                                    <li class="li">The origin can now subscribe to <a class="xref" href="../Origins/Salesforce.html#concept_cwb_mkg_5cb">Salesforce platform events</a>. </li>

                                    <li class="li">You can now configure the origin to use <a class="xref" href="../Origins/Salesforce.html#concept_aq1_kd1_5cb">Salesforce PK Chunking</a>.</li>

                                    <li class="li">When necessary, you can <a class="xref" href="../Origins/Salesforce.html#task_h1n_bs3_rx">disable query validation</a>.</li>

                                    <li class="li">You can now use <a class="xref" href="../Origins/Salesforce.html#task_h1n_bs3_rx">Mutual Authentication to connect to
                                        Salesforce</a>.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_vyk_1ww_scb">
                            <li class="li"><a class="xref" href="../Processors/FieldReplacer.html#concept_rw4_2d3_4cb" title="The Field Replacer replaces values in fields with nulls or with new values. Use the Field Replacer to update values or to replace invalid values.">New Field Replacer processor</a> - A new processor that
                                replaces values in fields with nulls or with new values. <p class="p">The
                                    Field Replacer processor replaces the Value Replacer processor
                                    which has been deprecated. The Field Replacer processor lets you
                                    define more complex conditions to replace values. For example,
                                    the Field Replacer can replace values which fall within a
                                    specified range. The Value Replacer cannot replace values that
                                    fall within a specified range.</p>
<p class="p">StreamSets recommends that
                                    you <a class="xref" href="../Upgrade/PostUpgrade.html#concept_hxf_3yd_qcb" title="With version 3.1.0.0, Data Collector introduces a new Field Replacer processor and has deprecated the Value Replacer processor.">update Value Replacer pipelines</a> as soon as possible.
                                </p>
</li>

                            <li class="li"><a class="xref" href="../Processors/JDBCMetadata.html#concept_lcp_ssh_qcb">New
                                    Postgres Metadata processor</a> - A new processor that
                                determines when changes in data structure occur and creates and
                                alters PostgreSQL tables accordingly. Use as part of the <a class="xref" href="../JDBC_DriftSolution/JDBC_DriftSyncSolution_title.html#concept_ljq_knr_4cb">Drift Synchronization Solution for Postgres</a> in
                                development or testing environments only.</li>

                            <li class="li">Aggregator processor enhancements - The processor includes the
                                following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_e2h_nww_scb">
                                    <li class="li"><a class="xref" href="../Processors/Aggregator.html#concept_bc4_c42_wbb">Event records</a> now include the results of the
                                        aggregation. </li>

                                    <li class="li">You can now specify the <a class="xref" href="../Processors/Aggregator.html#concept_dld_z2q_vcb">root field for event records</a>. You can use a
                                        String or Map root field. Upgraded pipelines retain the
                                        previous behavior, writing aggregation data to a String root
                                        field. </li>

                                </ul>
</li>

                            <li class="li">JDBC Lookup processor enhancement - The processor includes the
                                following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_k14_pww_scb">
                                    <li class="li">You can now configure a <a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">Missing Values Behavior property</a> that defines
                                        processor behavior when a lookup returns no value. Upgraded
                                        pipelines continue to send records with no return value to
                                        error.</li>

                                    <li class="li">You can now enable the <a class="xref" href="../Processors/JDBCLookup.html#concept_jt5_kx2_px">Retry on Cache Miss</a> property so that the
                                        processor retries lookups for known missing values. By
                                        default, the processor always returns the default value for
                                        known missing values to avoid unnecessary lookups.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_cs4_vj1_s1b">Kudu Lookup processor enhancement</a> - The processor no
                                longer requires that you add a primary key column to the Key Columns
                                Mapping. However, adding only non-primary keys can slow the
                                performance of the lookup.</li>

                            <li class="li"><a class="xref" href="../Processors/SalesforceLookup.html#task_fhn_yrk_yx">Salesforce Lookup processor enhancement</a> - You can now
                                use Mutual Authentication to connect to Salesforce.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_ynj_2xw_scb">
                            <li class="li"><a class="xref" href="../Destinations/Aerospike.html#concept_gyq_rpr_4cb" title="The Aerospike destination writes data to Aerospike.">New
                                    Aerospike destination</a> - A new destination that writes
                                data to Aerospike.</li>

                            <li class="li"><a class="xref" href="../Destinations/NamedPipe.html#concept_pl5_tdg_gcb" title="The Named Pipe destination writes data to a UNIX named pipe.">New
                                    Named Pipe destination</a>- A new destination that writes
                                data to a UNIX named pipe. </li>

                            <li class="li">Einstein Analytics destination enhancements - The destination
                                includes the following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_r3y_xxw_scb">
                                    <li class="li">You can specify the name of the <a class="xref" href="../Destinations/WaveAnalytics.html#task_mdt_dv3_rx">edgemart container</a> that contains the
                                        dataset.</li>

                                    <li class="li">You can define the <a class="xref" href="../Destinations/WaveAnalytics.html#concept_ryp_g4r_vcb">operation to perform</a>: Append, Delete, Overwrite,
                                        or Upsert.</li>

                                    <li class="li">You can now use <a class="xref" href="../Destinations/WaveAnalytics.html#task_mdt_dv3_rx">Mutual Authentication to connect to
                                        Salesforce</a>.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Elasticsearch.html#concept_w2r_ktb_ry">Elasticsearch destination enhancement</a> - You can now
                                configure the destination to merge data which performs an update
                                with <code class="ph codeph">doc_as_upsert</code>. </li>

                            <li class="li">Salesforce destination enhancement - The destination includes the
                                following enhancements:<ul class="ul" id="concept_agl_4tw_scb__ul_rpj_2yw_scb">
                                    <li class="li">The destination can now <a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">publish Salesforce platform events</a>.</li>

                                    <li class="li">You can now use <a class="xref" href="../Destinations/Salesforce.html#task_ncv_153_rx">Mutual Authentication to connect to
                                        Salesforce</a>.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_sl2_hyw_scb">
                            <li class="li"><a class="xref" href="../Data_Formats/LogFormats.html#concept_tr1_spd_sr" title="When you use an origin to read log data, you define the format of the log files to be read.">Log
                                    data format enhancement</a> - Data Collector can now process
                                data using the following log format types:<ul class="ul" id="concept_agl_4tw_scb__ul_dmj_3yw_scb">
                                    <li class="li">Common Event Format (CEF)</li>

                                    <li class="li">Log Event Extended Format (LEEF)</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_vjw_nyw_scb">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ndj_43v_1r" title="Error record functions provide information about error records. Use error functions to process error records.">Error record functions</a> - This release includes the
                                following new function:<ul class="ul" id="concept_agl_4tw_scb__ul_yvn_vzw_scb">
                                    <li class="li">record:errorStackTrace() - Returns the error stack trace for
                                        the record.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">Time functions</a> - This release includes the following new
                                    functions:<ul class="ul" id="concept_agl_4tw_scb__ul_own_yzw_scb">
                                    <li class="li">time:dateTimeZoneOffset() - Returns the time zone offset in
                                        milliseconds for the specified date and time zone.</li>

                                    <li class="li">time:timeZoneOffset() - Returns the time zone offset in
                                        milliseconds for the specified time zone.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">Miscellaneous functions</a> - This release includes the
                                following changed and new functions:<ul class="ul" id="concept_agl_4tw_scb__ul_uqv_b1x_scb">
                                    <li class="li">runtime:loadResource() - This function has been changed to
                                        trim any leading or trailing whitespace characters from the
                                        file before returning the value in the file. Previously, the
                                        function did not trim white space characters - you had to
                                        avoid including unnecessary characters in the file.</li>

                                    <li class="li">runtime:loadResourceRaw() - New function that returns the
                                        value in the specified file, including any leading or
                                        trailing whitespace characters in the file.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Additional Stage Libraries</dt>

                    <dd class="dd">This release includes the following <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fb2_qmn_bz">additional stage libraries</a>:<ul class="ul" id="concept_agl_4tw_scb__ul_ksd_21x_scb">
                            <li class="li"> Apache Kudu 1.6 </li>

                            <li class="li"> Cloudera 5.13 distribution of Apache Kafka 2.1 </li>

                            <li class="li"> Cloudera 5.14 distribution of Apache Kafka 2.1 </li>

                            <li class="li"> Cloudera CDH 5.14 distribution of Hadoop </li>

                            <li class="li"> Kinetica 6.1 </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Miscellaneous</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_agl_4tw_scb__ul_idl_31x_scb">
                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Data Collector classpath validation</a> - Data Collector now
                                performs a classpath health check upon starting up. The results of
                                the health check are written to the Data Collector log. When
                                necessary, you can configure Data Collector to skip the health check
                                or to stop upon errors.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Support bundle Data Collector property</a> - You can
                                configure a property in the Data Collector configuration file to
                                have Data Collector automatically upload support bundles when
                                problems occur. The property is disabled by default.</li>

                            <li class="li"><a class="xref" href="../DPM/DPMConfiguration.html#concept_hrn_zz3_fx" title="You can customize how a registered Data Collector works with StreamSets Control Hub by editing the Control Hub configuration file, $SDC_CONF/dpm.properties, located in the Data Collector installation.">Redirect registered Data Collector user logins property</a>
                                - You can enable a property in the Control Hub configuration file,
                                    <code class="ph codeph">dpm.properties</code>, to redirect Data Collector user
                                logins to Control Hub using the HTML meta refresh method.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_fjx_g31_1s" title="Runtime properties are properties that you define in a file local to the Data Collector and call from within a pipeline. With runtime properties, you can define different sets of values for different Data Collectors.">Runtime properties enhancement</a> - You can now use
                                environment variables in runtime properties.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title9" id="concept_z4b_qrb_4cb">
    <h2 class="title topictitle2" id="ariaid-title9">What's New in 3.0.3.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.3.0 includes the following enhancements:<ul class="ul" id="concept_z4b_qrb_4cb__ul_q44_5rb_4cb">
                <li class="li"><a class="xref" href="../Origins/MySQLBinaryLog.html#task_qbt_kyh_xx">MySQL
                        Binary Log origin enhancement</a> - You can now use a Keep Alive thread
                    to connect to the MySQL server by configuring the Enable KeepAlive Thread and
                    the KeepAlive Interval advanced properties. <p class="p">By default, the origin uses Keep
                        Alive threads with an interval of one minute. Upgraded pipelines also use
                        the new defaults.</p>
</li>

                <li class="li">HTTP Client processor enhancement - The processor can now process compressed
                    data. </li>

                <li class="li">Scripting processors enhancement - The Groovy Evaluator, JavaScript Evaluator,
                    and Jython Evaluator processors can use a new boolean sdcFunction.isPreview()
                    method to determine if the pipeline is in preview mode.</li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title10" id="concept_lhn_cvv_lcb">
    <h2 class="title topictitle2" id="ariaid-title10">What's New in 3.0.2.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.2.0 includes the following enhancement:<ul class="ul" id="concept_lhn_cvv_lcb__ul_y3g_kvv_lcb">
                <li class="li">SFTP/FTP Client origin enhancement - The origin can now generate events when
                    starting and completing processing for a file and when all available files have
                    been processed. </li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title11" id="concept_lgz_yp4_gcb">
    <h2 class="title topictitle2" id="ariaid-title11">What's New in 3.0.1.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.1.0 includes the following enhancements:<ul class="ul" id="concept_lgz_yp4_gcb__ul_fjz_1q4_gcb">
                <li class="li">Azure IoT/Event Hub Consumer origin enhancement - The Azure Event Hub Consumer
                    origin has been renamed to the Azure IoT/Event Hub Consumer origin.</li>

                <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_rvj_5qy_4cb">HTTP Server origin enhancement</a> - The HTTP Server origin now includes
                    path and queryString record header attributes, as well as any other HTTP header
                    attributes included in the request. </li>

                <li class="li">MongoDB origins enhancement - Both the <a class="xref" href="../Origins/MongoDB.html#task_mdf_2rs_ns">MongoDB
                        origin</a> and the <a class="xref" href="../Origins/MongoDBOplog.html#task_qj5_drw_4y">MongoDB
                        Oplog</a> origin now support delegated authentication and the BSON data
                    type for binary data. </li>

                <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_pc4_xts_r1b">SQL Server CDC origin enhancement</a> - The SQL Server CDC origin now
                    includes information from the SQL Server CDC __$command_id column in a record
                    header attribute named jdbc. __$command_id.</li>

                <li class="li">Mongo DB destination enhancement - The MongoDB destination now supports <a class="xref" href="../Destinations/MongoDB.html#task_mrc_k5n_4v">delegated
                        authentication</a>.</li>

            </ul>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title12" id="concept_cjx_y4k_wbb">
    <h2 class="title topictitle2" id="ariaid-title12">What's New in 3.0.0.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 3.0.0.0 includes the following new features:<dl class="dl">
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_dxx_z5k_wbb">
                            <li class="li"><a class="xref" href="../Installation/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Java requirement</a> - Data Collector now supports both
                                Oracle Java 8 and OpenJDK 8.</li>

                            <li class="li"><a class="xref" href="../Installation/FullInstall_ServiceStart.html#task_th5_1yj_dx" title="You can install the Data Collector RPM package and start it as a service on CentOS or Red Hat Enterprise Linux.">RPM packages</a> - StreamSets now provides the following <span class="ph">Data Collector</span> RPM packages:<ul class="ul" id="concept_cjx_y4k_wbb__ul_k13_cvk_wbb">
                                    <li class="li">EL6 - Use to install <span class="ph">Data Collector</span> on CentOS 6 or Red Hat Enterprise Linux 6</li>

                                    <li class="li">EL7 - Use to install <span class="ph">Data Collector</span> on CentOS 7 or Red Hat Enterprise Linux 7.</li>

                                </ul>
<p class="p">Previously, <span class="ph">StreamSets</span> provided a single RPM package used to install <span class="ph">Data Collector</span> on any of these operating systems.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Edge Pipelines</dt>

                    <dd class="dd">You can now design and run <a class="xref" href="../Edge_Mode/EdgePipelines_Overview.html#concept_d4h_kkq_4bb" title="An edge pipeline is a pipeline that runs on an edge device with limited resources. Use edge pipelines to read data from the edge device or to receive data from another pipeline and then act on that data to control the edge device.">edge
                            pipelines</a> to read data from or send data to an edge device. Edge
                        pipelines are bidirectional. They can send edge data to other <span class="ph">Data Collector</span> pipelines for further processing. Or, they can receive data from other
                        pipelines and then act on that data to control the edge device. </dd>

                    <dd class="dd ddexpand">Edge pipelines run in edge execution mode on <span class="ph">StreamSets</span>
                        <span class="ph">Data Collector Edge</span> (<span class="ph">SDC Edge</span>). <span class="ph">SDC Edge</span> is a lightweight agent without a UI that runs pipelines on edge devices.
                        Install <span class="ph">SDC Edge</span> on each edge device where you want to run edge pipelines. </dd>

                    <dd class="dd ddexpand">You design edge pipelines in <span class="ph">Data Collector</span>, export the edge pipelines, and then use commands to run the edge
                        pipelines on an <span class="ph">SDC Edge</span> installed on an edge device. </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_xdj_qvk_wbb">
                            <li class="li"><a class="xref" href="../Origins/AmazonSQS.html#concept_xsh_knm_5bb">New
                                    Amazon SQS Consumer origin</a> - An origin that reads
                                messages from Amazon Simple Queue Service (SQS). Can create multiple
                                threads to enable parallel processing in a multithreaded
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/GCS.html#concept_iyd_wql_nbb">New Google
                                    Cloud Storage origin</a> - An origin that reads fully written
                                objects in Google Cloud Storage. </li>

                            <li class="li"><a class="xref" href="../Origins/MapRdbCDC.html#concept_qwj_5vm_pbb">New MapR
                                    DB CDC origin</a> - An origin that reads changed MapR DB data
                                that has been written to MapR Streams. Can create multiple threads
                                to enable parallel processing in a multithreaded pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/MapRStreamsMultiConsumer.html#concept_hvd_hww_lbb">New MapR Multitopic Streams Consumer origin</a> - An origin
                                that reads messages from multiple MapR Streams topics. It can create
                                multiple threads to enable parallel processing in a multithreaded
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/UDPMulti.html#concept_wng_g5f_5bb">New UDP
                                    Multithreaded Source origin</a> - The origin listens for UDP
                                messages on one or more ports and queues incoming packets on an
                                intermediate queue for processing. It can create multiple threads to
                                enable parallel processing in a multithreaded pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#concept_unk_nzk_fbb" title="The WebSocket Client origin reads data from a WebSocket server endpoint. Use the origin to read data from a WebSocket resource URL.">New
                                    WebSocket Client origin</a> - An origin that reads data from
                                a WebSocket server endpoint.</li>

                            <li class="li"><a class="xref" href="../Origins/WindowsLog.html#concept_agf_5jv_sbb" title="The Windows Event Log origin reads data from a Microsoft Windows event log located on a Windows machine. The origin generates a record for each event in the log.">New
                                    Windows Event Log origin</a> - An origin that reads data from
                                Microsoft Windows event logs. You can use this origin only in
                                pipelines configured for edge execution mode.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">New Sensor Reader development origin</a> - A development
                                origin that generates sample atmospheric data for <a class="xref" href="../Edge_Mode/EdgePipelines_Overview.html#concept_d4h_kkq_4bb" title="An edge pipeline is a pipeline that runs on an edge device with limited resources. Use edge pipelines to read data from the edge device or to receive data from another pipeline and then act on that data to control the edge device.">edge pipelines</a>.</li>

                            <li class="li">Amazon S3 origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_gxg_dbl_wbb">
                                    <li class="li">The origin now produces <a class="xref" href="../Origins/AmazonS3.html#concept_vtn_ty4_jbb">no-more-data events</a> and includes a new socket
                                        timeout property.</li>

                                    <li class="li">You can now specify the number of times the origin <a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">retries a query</a>. The default is three.</li>

                                </ul>
</li>

                            <li class="li">
                                <a class="xref" href="../Origins/Directory.html#concept_pcl_nwn_qbb" title="The Directory origin uses multiple concurrent threads to process data based on the Number of Threads property.">Directory origin enhancement</a> - The origin can now use
                                multiple threads to perform parallel processing of files.</li>

                            <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_uyw_rct_fdb" title="The HTTP Client origin can log request and response data to the Data Collector log.">HTTP Client origin enhancement</a> - The origin can now log
                                request and response data to the <span class="ph">Data Collector</span> log. </li>

                            <li class="li">JDBC Multitable Consumer origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_ij2_lbl_wbb">
                                    <li class="li">The origin can now use <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_xwr_bhm_nbb">non-incremental processing</a> for tables with no
                                        primary key or offset column. </li>

                                    <li class="li">You can now specify an <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#task_kst_m4w_4y">Init Query</a> to be executed after establishing a
                                        connection to the database, before performing other tasks.
                                        This can be used, for example, to modify session attributes. </li>

                                    <li class="li">A new <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#task_kst_m4w_4y">Queries Per Second property</a> determines how many
                                        queries can be run every second. <p class="p">This property replaces
                                            the Query Interval property. For information about
                                            possible upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_hky_ljl_wbb">JDBC Multitable Consumer Query Interval
                                                Change</a>.</p>
</li>

                                </ul>
</li>

                            <li class="li">JDBC Query Consumer origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_hzy_tbl_wbb">
                                    <li class="li">You can now specify an <a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">Init Query</a> to be executed after establishing a
                                        connection to the database, before performing other tasks.
                                        This can be used, for example, to modify session
                                        attributes.</li>

                                    <li class="li">The Microsoft SQL Server CDC functionality in the JDBC Query
                                        Consumer origin is now deprecated and will be removed from
                                        the origin in a future release. For upgrade information, see
                                            <a class="xref" href="../Upgrade/PostUpgrade.html#concept_ys3_bjl_wbb">Update JDBC Query Consumer Pipelines used for SQL
                                            Server CDC Data</a>.</li>

                                </ul>
</li>

                            <li class="li">Kafka Multitopic Consumer origin enhancement - The origin is now
                                available in the following stage libraries, in addition to the
                                Apache Kafka 0.10 stage library:<ul class="ul" id="concept_cjx_y4k_wbb__ul_emj_ccl_wbb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Apache Kafka 0.9</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">CDH Kafka 2.0 (0.9.0) and 2.1 (0.9.0)</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p class="p">HDP 2.5 and 2.6</p>

                                    </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#task_p4b_vv4_yr">Kinesis Consumer origin enhancement</a> - You can now
                                specify the number of times the origin retries a query. The default
                                is three.</li>

                            <li class="li">Oracle CDC Client origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_wbj_fcl_wbb">
                                    <li class="li">When using <a class="xref" href="../Origins/OracleCDC.html#concept_zrc_pyj_dx">SCNs for the initial change</a>, the origin now
                                        treats the specified SCN as a starting point rather than
                                        looking for an exact match.</li>

                                    <li class="li">The origin now passes <a class="xref" href="../Origins/OracleCDC.html#concept_gwp_d4n_n1b">raw data</a> to the pipeline as a byte array.</li>

                                    <li class="li">The origin can now include unparsed strings from the parsed
                                        SQL query for <a class="xref" href="../Origins/OracleCDC.html#concept_gwp_d4n_n1b">unsupported data types</a> in records.</li>

                                    <li class="li">The origin now uses <a class="xref" href="../Origins/OracleCDC.html#concept_yqk_3hn_n1b">local buffering</a> instead of Oracle LogMiner
                                        buffering by default. Upgraded pipelines require no changes. </li>

                                    <li class="li">The origin now supports reading the <a class="xref" href="../Origins/OracleCDC.html#concept_w1n_p5l_kcb">Timestamp with Timezone data type</a>. When reading
                                        Timestamp with Timezone data, the origin includes the offset
                                        with the datetime data in the <span class="ph">Data Collector</span> Zoned Datetime data type. It does not include the time
                                        zone ID.</li>

                                </ul>
</li>

                            <li class="li">SQL Server CDC Client origin enhancements - You can now perform the
                                following tasks with the SQL Server CDC Client origin:<ul class="ul" id="concept_cjx_y4k_wbb__ul_et1_pcl_wbb">
                                    <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_nxm_1lp_qbb">Process CDC tables</a> that appear after the
                                        pipeline starts.</li>

                                    <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_avq_s2q_qbb">Check for schema changes and generate events</a>
                                        when they are found.</li>

                                    <li class="li">
                                        <p dir="ltr" class="p">In addition, a new Capture Instance Name
                                            property replaces the Schema and Table Name Pattern
                                            properties from earlier releases.</p>

                                        <p dir="ltr" class="p">You can simply use the schema name and table
                                            name pattern for the capture instance name. Or, you can
                                            specify the schema name and a capture instance name
                                            pattern, which allows you to specify specific CDC tables
                                            to process when you have multiple CDC tables for a
                                            single data table.</p>

                                        <p dir="ltr" class="p">Upgraded pipelines require no changes.</p>

                                    </li>

                                </ul>
</li>

                            <li class="li">UDP Source origin enhancement - The Enable Multithreading property
                                that enabled using multiple epoll receiver threads is now named Use
                                Native Transports (epoll).</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_qfc_3fl_wbb">
                            <li class="li"><a class="xref" href="../Processors/Aggregator.html#concept_ofb_svm_5bb">New
                                    Aggregator processor</a> - A processor that aggregates data
                                within a window of time. Displays the results in Monitor mode and
                                can write the results to events. </li>

                            <li class="li"><a class="xref" href="../Processors/Delay.html#concept_ez5_pvf_wbb">New Delay
                                    processor</a> - A processor that can delay processing a batch
                                of records for a specified amount of time.</li>

                            <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#task_g23_2tq_wq">Field Type Converter processor enhancement</a> - You can now
                                convert strings to the Zoned Datetime data type, and vice versa. You
                                can also specify the format to use. </li>

                            <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#task_hpg_pft_zv">Hive Metadata processor enhancement</a> - You can now
                                configure additional JDBC configuration properties to pass to the
                                JDBC driver.</li>

                            <li class="li">HTTP Client processor enhancements: <ul class="ul" id="concept_cjx_y4k_wbb__ul_usn_wjr_hdb">
                                    <li class="li">The processor can now <a class="xref" href="../Processors/HTTPClient.html#concept_dhg_gmk_hdb" title="The HTTP Client processor can log request and response data to the Data Collector log.">log request and response data</a> to the <span class="ph">Data Collector</span> log. </li>

                                    <li class="li">The <a class="xref" href="../Processors/HTTPClient.html#task_z54_1qr_fw" title="Configure an HTTP Client processor to perform requests against a resource URL.">Rate Limit</a> now defines the minimum amount of
                                        time between requests in milliseconds. Previously, it
                                        defined the time between requests in seconds. Upgraded
                                        pipelines require no changes.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> and <a class="xref" href="../Processors/JDBCTee.html#task_qpj_ncy_hw">JDBC Tee</a> processor enhancements - You can now specify an
                                Init Query to be executed after establishing a connection to the
                                database, before performing other tasks. This can be used, for
                                example, to modify session attributes. </li>

                            <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_eqf_dh3_x1b">Kudu Lookup processor enhancement</a> - The Cache Kudu Table
                                property is now named Enable Table Caching. The Maximum Entries to
                                Cache Table Objects property is now named Maximum Table Entries to
                                Cache. </li>

                            <li class="li">Salesforce Lookup processor enhancement - You can use a new <a class="xref" href="../Processors/SalesforceLookup.html#concept_ow1_lj3_xbb">Retrieve lookup mode</a> to look up data for a set of
                                records instead of record-by-record. The mode provided in previous
                                releases is now named SOQL Query. Upgraded pipelines require no
                                changes. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_rv3_nfl_wbb">
                            <li class="li"><a class="xref" href="../Destinations/GCS.html#concept_p4n_jrl_nbb">New Google
                                    Cloud Storage destination</a> - A new destination that writes
                                data to objects in Google Cloud Storage. The destination can
                                generate events for use as dataflow triggers.</li>

                            <li class="li"><a class="xref" href="../Destinations/KineticaDB.html#concept_hxh_5xg_qbb">New
                                    KineticaDB destination</a> - A new destination that writes
                                data to a Kinetica table. </li>

                            <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">Amazon S3 destination enhancement</a> - You can now specify
                                the number of times the destination retries a query. The default is
                                three.</li>

                            <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#task_a4n_1ft_zv">Hive Metastore destination enhancement</a> - You can now
                                configure additional JDBC configuration properties to pass to the
                                JDBC driver.</li>

                            <li class="li">HTTP Client destination enhancements: <ul class="ul" id="concept_cjx_y4k_wbb__ul_tbw_3kr_hdb">
                                    <li class="li">The destination can now <a class="xref" href="../Destinations/HTTPClient.html#concept_e1s_mgr_hdb" title="The HTTP Client destination can log request and response data to the Data Collector log.">log
                                            request and response data</a> to the <span class="ph">Data Collector</span> log. </li>

                                    <li class="li">You can now use the HTTP Client destination to write <a class="xref" href="../Destinations/HTTPClient.html#concept_l2r_gy5_lz">Avro, Delimited, and Protobuf data</a> in addition
                                        to the previous data formats. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Destinations/JMSProducer.html#task_udk_yw5_n1b">JDBC Producer destination enhancement</a> - You can now
                                specify an Init Query to be executed after establishing a connection
                                to the database, before performing other tasks. This can be used,
                                for example, to modify session attributes. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#task_c4x_tmh_4v">Kudu destination enhancement</a> - If the destination
                                receives a change data capture log from the following source
                                systems, you now must specify the source system in the Change Log
                                Format property so that the destination can determine the format of
                                the log: Microsoft SQL Server, Oracle CDC Client, MySQL Binary Log,
                                or MongoDB Oplog. </li>

                            <li class="li">MapR DB JSON destination enhancement - The destination now supports
                                writing to MapR DB based on the <a class="xref" href="../Destinations/MapRDBJSON.html#concept_hy5_3nb_xbb">CRUD operation in record header attributes and the Insert API
                                    and Set API properties</a>.</li>

                            <li class="li">MongoDB destination enhancements - With this release, the Upsert
                                operation is no longer supported by the destination. Instead, the
                                destination includes the following enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_xfc_xfl_wbb">
                                    <li class="li">Support for the <a class="xref" href="../Destinations/MongoDB.html#concept_bkc_m24_4v">Replace and Update operations</a>. </li>

                                    <li class="li">Support for an <a class="xref" href="../Destinations/MongoDB.html#concept_syh_s1l_tbb">Upsert flag</a> that, when enabled, is used with
                                        both the Replace and Update operations. </li>

                                </ul>
<p class="p">For information about upgrading existing upsert pipelines,
                                    see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_ncs_5jl_wbb">Update MongoDB Destination Upsert Pipelines</a>.
                                </p>
</li>

                            <li class="li">Redis destination enhancement - The destination now supports
                                processing data using <a class="xref" href="../Destinations/Redis.html#concept_dz2_4xh_xbb">CRUD operations stored in record header attributes</a>. </li>

                            <li class="li"><a class="xref" href="../Destinations/Salesforce.html#concept_xql_wbj_mcb" title="When you configure the Salesforce destination, you map fields in the record to existing fields in the Salesforce object.">Salesforce destination enhancement</a> - When using the
                                Salesforce Bulk API to update, insert, or upsert data, you can now
                                use a colon (:) or period (.) as a field separator when defining the
                                Salesforce field to map the Data Collector field to. For example,
                                    <code class="ph codeph">Parent__r:External_Id__c</code> or
                                    <code class="ph codeph">Parent__r.External_Id__c</code> are both valid
                                Salesforce fields. </li>

                            <li class="li">Wave Analytics destination rename - With this release, the Wave
                                Analytics destination is now named the <a class="xref" href="../Destinations/WaveAnalytics.html#concept_hlx_r53_rx" title="The Einstein Analytics destination writes data to Salesforce Einstein Analytics. The destination connects to Einstein Analytics to upload external data to a dataset.">Einstein Analytics destination</a>, following the recent
                                Salesforce rebranding. All of the properties and functionality of
                                the destination remain the same. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_if5_jgl_wbb">
                            <li class="li"><a class="xref" href="../Executors/HiveQuery.html#task_mgm_4lk_fx">Hive Query executor enhancement</a> - You can now configure
                                additional JDBC configuration properties to pass to the JDBC driver. </li>

                            <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#task_ym2_3cv_sx">JDBC Query executor enhancement</a> - You can now specify an
                                Init Query to be executed after establishing a connection to the
                                database, before performing other tasks. This can be used, for
                                example, to modify session attributes. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Cloudera Navigator </dt>

                    <dd class="dd">Cloudera Navigator integration is now released as part of the StreamSets
                        Commercial Subscription. The beta version included in earlier releases is no
                        longer available with <span class="ph">Data Collector</span>. For information about the StreamSets Commercial Subscription, <a class="xref" href="https://streamsets.com/contact-us/" target="_blank">contact us</a>.<p dir="ltr" class="p">For information about upgrading a
                            version of <span class="ph">Data Collector</span> with Cloudera Navigator integration enabled, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnp_scs_wbb">Disable Cloudera Navigator Integration</a>.</p>
</dd>

                
                
                    <dt class="dt dlterm">Credential Stores</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_o1b_mgl_wbb">
                            <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_v21_nvd_fbb">CyberArk</a> - Data Collector now provides a credential
                                store implementation for CyberArk Application Identity Manager. You
                                can define the credentials required by external systems - user names
                                or passwords - in CyberArk. Then you use credential expression
                                language functions in JDBC stage properties to retrieve those
                                values, instead of directly entering credential values in stage
                                properties. </li>

                            <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">Supported stages</a> - You can now use the credential
                                functions in all stages that require you to enter sensitive
                                information. Previously, you could only use the credential functions
                                in JDBC stages. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm"><span class="ph">Data Collector</span> Configuration</dt>

                    <dd class="dd">By default when <span class="ph">Data Collector</span> restarts, it automatically restarts all pipelines that were running
                        before Data Collector shut down. You can now disable the automatic restart
                        of pipelines by <a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r__table_k1y_drr_bx">configuring the <code class="ph codeph">runner.boot.pipeline.restart</code>
                            property</a> in the <code class="ph codeph">$SDC_CONF/sdc.properties</code> file.
                    </dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager / <span class="ph">StreamSets Control Hub</span></dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_p2b_4hl_wbb">
                            <li class="li"><span class="ph">StreamSets Control Hub</span> - With this release, we have created a new product called <a class="xref" href="https://streamsets.com/products/sch" target="_blank"><span class="ph">StreamSets Control Hub</span><sup class="ph sup">TM</sup></a> that includes a number of new
                                cloud-based dataflow design, deployment, and scale-up features.
                                Since this release is now our core service for controlling
                                dataflows, we have renamed the StreamSets cloud experience from
                                "Dataflow Performance Manager (DPM)" to "<span class="ph">StreamSets Control Hub</span>".<p class="p">DPM now refers to the performance management functions
                                    that reside in the cloud such as live metrics and data SLAs.
                                    Customers who have purchased the StreamSets Enterprise Edition
                                    will gain access to all <span class="ph">Control Hub</span> functionality and continue to have access to all DPM
                                    functionality as before.</p>
<p class="p">To understand the end-to-end
                                    StreamSets Data Operations Platform and how the products fit
                                    together, visit <a class="xref" href="https://streamsets.com/products/" target="_blank">https://streamsets.com/products/</a>. </p>
</li>

                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_qh5_v5t_mbb" title="When you write statistics to MapR Streams, Data Collector effectively adds a MapR Streams Producer destination to the pipeline that you are configuring. Control Hub automatically generates and runs a system pipeline for the job. The system pipeline reads the statistics from MapR Streams, and then aggregates and sends the statistics to Control Hub.">Aggregated statistics</a> - When working with <span class="ph">Control Hub</span>, you can now configure a pipeline to write aggregated statistics
                                to MapR Streams. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_sp4_rhl_wbb">
                            <li class="li"><a class="xref" href="../Data_Formats/NetFlow_Overview.html#concept_thl_nnr_hbb">New NetFlow 9 support</a> - <span class="ph">Data Collector</span> now supports processing NetFlow 9 template-based messages. Stages
                                that previously processed NetFlow 5 data can now process NetFlow 9
                                data as well. </li>

                            <li class="li">Datagram data format enhancement - The Datagram Data Format property
                                is now named the Datagram Packet Format. </li>

                            <li class="li">Delimited data format enhancement - <span class="ph">Data Collector</span> can now process data using the Postgres CSV and Postgres Text
                                delimited format types. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_ppq_whl_wbb">
                            <li class="li"><a class="xref" href="../Pipeline_Configuration/Expressions.html#concept_ir4_rxt_3cb" title="You can use field path expressions in certain processors to determine the set of fields that you want the processor to use.">New field path expressions</a> - You can use field path
                                expressions in certain stages to specify the fields to use in an
                                expression. </li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_gfs_w55_3cb" title="You can use field functions in field path expressions that determine the set of fields that a processor uses. Each function is evaluated against a set of matching fields individually.">New field functions</a> - You can use the following new
                                field functions in field path expressions:<ul class="ul" id="concept_cjx_y4k_wbb__ul_avp_bws_kcb">
                                    <li class="li">f:attribute() - Returns the value of the specified
                                        attribute.</li>

                                    <li class="li">f:path() - Returns the path of a field. </li>

                                    <li class="li">f:type() - Returns the data type of a field.</li>

                                    <li class="li">f:value() - Returns the value of a field.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New string functions</a> - The release includes the
                                following new functions:<ul class="ul" id="concept_cjx_y4k_wbb__ul_uss_whl_wbb">
                                    <li class="li">str:isNullOrEmpty() - Returns true or false based on whether
                                        a string is null or is the empty string.</li>

                                    <li class="li">str:splitKV() - Splits key-value pairs in a string into a
                                        map of string values.</li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_qhq_c3l_wbb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release includes the
                                following new stage libraries:<ul class="ul" id="concept_cjx_y4k_wbb__ul_rcr_23l_wbb">
                                    <li class="li">Apache Kafka 1.0 </li>

                                    <li class="li">Apache Kafka 0.11 </li>

                                    <li class="li">Apache Kudu 1.5 </li>

                                    <li class="li">Cloudera CDH 5.13 </li>

                                    <li class="li">Cloudera Kafka 3.0.0 (0.11.0) </li>

                                    <li class="li">Hortonworks 2.6.1, including Hive 1.2 </li>

                                    <li class="li">Hortonworks 2.6.2, including Hive 1.2 and 2.0 </li>

                                    <li class="li">MapR version 6.0 (MEP 4)</li>

                                    <li class="li">MapR Spark 2.1 (MEP 3) </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy stage libraries</a> - Stage libraries that are more
                                than two years old are no longer included with <span class="ph">Data Collector</span>. Though not recommended, you can still download and install the
                                older stage libraries as custom stage libraries. <p class="p">If you have
                                    pipelines that use these legacy stage libraries, you will need
                                    to update the pipelines to use a more current stage library or
                                    install the legacy stage library manually, For more information
                                    see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnl_zk4_5bb">Update Pipelines using Legacy Stage
                                Libraries</a>.</p>
</li>

                            <li class="li"><a class="xref" href="../Installation/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space.">Statistics stage library enhancement</a> - The statistics
                                stage library is now included in the core <span class="ph">Data Collector</span> installation. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Miscellaneous</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_fw1_s3l_wbb">
                            <li class="li"><a class="xref" href="../Pipeline_Design/DatainMotion.html#concept_hjl_vyd_kcb">New data type</a> - <span class="ph">Data Collector</span> now supports the Zoned Datetime data type.</li>

                            <li class="li"><a class="xref" href="../Administration/Administration_title.html#task_wvx_4hk_br" title="You can view metrics about Data Collector, such as the CPU usage or the number of pipeline runners in the thread pool.">New <span class="ph">Data Collector</span> metrics</a> - JVM metrics have been renamed <span class="ph">Data Collector</span> Metrics and now include general <span class="ph">Data Collector</span> metrics in addition to JVM metrics. The JVM Metrics menu item has
                                also been renamed SDC Metrics.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_kgc_l4y_5r">Pipeline error records </a>- You can now write error records
                                to Google Pub/Sub, Google Cloud Storage, or an MQTT broker.</li>

                            <li class="li">Snapshot enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_cv5_53l_wbb">
                                    <li class="li">Standalone pipelines can now automatically take a <a class="xref" href="../Pipeline_Monitoring/PipelineMonitoring_title.html#concept_vkf_xjf_ybb" title="A failure snapshot is a partial snapshot that occurs automatically when the pipeline stops due to unexpected data. You can view the failure snapshot to troubleshoot the problem.">failure snapshot</a> when the pipeline fails due to
                                        a data-related exception. </li>

                                    <li class="li">You can now <a class="xref" href="../Pipeline_Monitoring/PipelineMonitoring_title.html#task_ut4_gvw_2cb" title="When needed, you can download a snapshot. You might download a snapshot from a production Data Collector so you can review it on a development Data Collector. Or you might download a snapshot to use the Dev Snapshot Replaying origin to read records from the downloaded file.">download snapshots through the UI</a> and the REST
                                        API.</li>

                                </ul>
</li>

                            <li class="li">Time zone enhancement - Time zones have been organized and updated
                                to use JDK 8 names. This should make it easier to select time zones
                                in stage properties. In the rare case that your pipeline uses a
                                format not supported by JDK 8, edit the pipeline to select a
                                compatible time zone.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title13" id="concept_rrq_v3k_kbb">
 <h2 class="title topictitle2" id="ariaid-title13">What's New in 2.7.2.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.7.2.0 includes the following new features:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rrq_v3k_kbb__ul_ibg_djk_kbb">
                            <li class="li"><a class="xref" href="../Origins/KafkaMultiConsumer.html#concept_ccs_fn4_x1b">New Kafka Multitopic Consumer origin</a> - A new origin that
                                reads messages from multiple Kafka topics. Creates multiple threads
                                to enable parallel processing in a multithreaded pipeline. </li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#task_p4b_vv4_yr">Kinesis Consumer origin enhancement</a> - You can now
                                configure the origin to start reading messages from a specified
                                timestamp.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rrq_v3k_kbb__ul_crw_frk_kbb">
                            <li class="li"><a class="xref" href="../Destinations/BigQuery.html#concept_hj4_brk_dbb">New
                                    Google BigQuery destination</a> - A new destination that
                                streams data into Google BigQuery.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title14" id="concept_rr2_mbz_w1b">
    <h2 class="title topictitle2" id="ariaid-title14">What's New in 2.7.1.1</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 2.7.1.1 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rr2_mbz_w1b__ul_gr5_hvr_1bb">
                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin enhancement</a> - You can now specify a
                                Connection Timeout advanced property. </li>

                            <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_m43_zzm_dbb" title="The JDBC Multitable Consumer origin can read from views in addition to tables.">JDBC Multitable Consumer origin enhancement</a> - You can
                                now use the origin to read from views in addition to tables.</li>

                            <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#task_bqt_mx3_h1b">OPC UA Client origin enhancement</a> - You can now configure
                                channel properties, such as the maximum chunk or message size.</li>

                            <li class="li"><a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client origin enhancement </a>- You can now
                                configure a JDBC Fetch Size property to determine the minimum number
                                of records that the origin waits for before passing data to the
                                pipeline. When writing to the destination is slow, use the default
                                of 1 record to improve performance. Previously, the origin used the
                                Oracle JDBC driver default of 10 records.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rr2_mbz_w1b__ul_zzr_ztn_2bb">
                            <li class="li"><a class="xref" href="../Executors/MapRFSFileMeta.html#concept_ohx_r5h_z1b">New MapR FS File Metadata executor</a> - The new executor
                                can change file metadata, create an empty file, or remove a file or
                                directory in MapR each time it receives an event. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title15" id="unique_887908705">
    <h2 class="title topictitle2" id="ariaid-title15">What's New in 2.7.1.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 2.7.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">Data Collector includes the following upgraded stage library:<ul class="ul" id="unique_887908705__ul_s1w_cvr_1bb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Jython 2.7.1</a></li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_887908705__ul_gr5_hvr_1bb">
                            <li class="li"><a class="xref" href="../Origins/AzureEventHub.html#concept_c1z_15q_1bb">New
                                    Azure Event Hub Consumer origin</a> - A multithreaded origin
                                that reads data from Microsoft Azure Event Hub.</li>

                            <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#concept_p25_wm2_dbb">OPC UA Client origin enhancement</a> - You can now specify
                                node information in a file. Or have the origin browse for nodes to
                                use based on a specified root node.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_887908705__ul_rh5_3vr_1bb">
                            <li class="li"><a class="xref" href="../Processors/SchemaGenerator.html#concept_rfz_ks3_x1b">New Schema Generator processor</a> - A processor that
                                generates a schema for each record and writes the schema to a record
                                header attribute. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_887908705__ul_s22_lvr_1bb">
                            <li class="li"><a class="xref" href="../Destinations/AzureEventHubProducer.html#concept_xq5_d5q_1bb">New Azure Event Hub Producer destination</a> - A destination
                                that writes data to Microsoft Azure Event Hub.</li>

                            <li class="li"><a class="xref" href="../Destinations/AzureIoTHub.html#concept_pnd_jkq_1bb">New Azure IoT Hub Producer destination</a> - A destination
                                that writes data to Microsoft Azure IoT Hub.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title16" id="unique_468165943">
 <h2 class="title topictitle2" id="ariaid-title16">What's New in 2.7.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data Collector</span>
            version 2.7.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Credential Stores</dt>

                <dd class="dd"><span class="ph">Data Collector</span> now has a <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential
                        store</a> API that integrates with the following credential store
                        systems:<ul class="ul" id="unique_468165943__ul_vgr_3cz_w1b">
                        <li class="li">Java keystore</li>

                        <li class="li">Hashicorp Vault</li>

                    </ul>
</dd>

                <dd class="dd ddexpand">
                    <p class="p">You define the credentials required by external systems - user names,
                        passwords, or access keys - in a Java keystore file or in Vault. Then you
                        use <a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential expression language functions</a> in JDBC stage
                        properties to retrieve those values, instead of directly entering credential
                        values in stage properties. </p>

                </dd>

                <dd class="dd ddexpand">
                    <div class="p">The following JDBC stages can use the new credential functions:<ul class="ul" id="unique_468165943__ul_ezm_4cz_w1b">
                            <li class="li">JDBC Multitable Consumer origin</li>

                            <li class="li">JDBC Query Consumer origin</li>

                            <li class="li">Oracle CDC Client origin</li>

                            <li class="li">SQL Server CDC Client origin</li>

                            <li class="li">SQL Server Change Tracking origin</li>

                            <li class="li">JDBC Lookup processor</li>

                            <li class="li">JDBC Tee processor</li>

                            <li class="li">JDBC Producer destination</li>

                            <li class="li">JDBC Query executor</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Publish Pipeline Metadata to Cloudera Navigator (Beta)</dt>

                <dd class="dd">
                    <p class="p"><span class="ph">Data Collector</span> now provides beta support for publishing metadata about running pipelines
                        to Cloudera Navigator. You can then use Cloudera Navigator to explore the
                        pipeline metadata, including viewing lineage diagrams of the metadata.</p>

                </dd>

                <dd class="dd ddexpand">Feel free to try out this feature in a development or test <span class="ph">Data Collector</span>, and send us your feedback. We are continuing to refine metadata publishing
                    as we gather input from the community and work with Cloudera.</dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <div class="p">Data Collector includes the following new stage libraries:<ul class="ul" id="unique_468165943__ul_cxp_d2z_w1b">
                            <li class="li">Apache Kudu version 1.4.0</li>

                            <li class="li">Cloudera CDH version 5.12 distribution of Hadoop</li>

                            <li class="li">Cloudera version 5.12 distribution of Apache Kafka 2.1</li>

                            <li class="li">Google Cloud - Includes the Google BigQuery origin, Google Pub/Sub
                                Subscriber origin, and Google Pub/Sub Publisher destination.</li>

                            <li class="li">Java keystore credential store - For use with <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential stores</a>.</li>

                            <li class="li">Vault credential store - For use with <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential stores</a>.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Data Collector Configuration</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_hc5_q2z_w1b">
                        <li class="li"><a class="xref" href="../Configuration/Vault-Overview.html#concept_bmq_gl1_mw">Access  Hashicorp Vault secrets</a> - The Data Collector Vault
                            integration now relies on Vault's App Role authentication backend.
                            Previously, Data Collector relied on Vault's App ID authentication
                            backend. Hashicorp has deprecated the App ID authentication
                            backend.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#concept_dcq_mpk_f1b">New Hadoop user impersonation property</a> - When you enable
                            Data Collector to impersonate the current Data Collector user when
                            writing to Hadoop, you can now also configure Data Collector to make the
                            username lowercase. This can be helpful with case-sensitive
                            implementations of LDAP.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New Java security properties</a> - The Data Collector
                            configuration file now includes properties with a "java.security."
                            prefix, which you can use to configure Java security properties.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New property to define the amount of time to cache DNS
                                lookups</a> - By default, the
                            java.security.networkaddress.cache.ttl property is set to 0 so that the
                            JVM uses the Domain Name Service (DNS) time to live value, instead of
                            caching the lookups for the lifetime of the JVM.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_cy3_r44_z1b">SDC_HEAPDUMP_PATH enhancement</a> - The new default file name,
                                <code class="ph codeph">$SDC_LOG/sdc_heapdump_${timestamp}.hprof</code>, includes
                            a timestamp so you can write multiple heap dump files to the specified
                            directory.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Dataflow Triggers</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_fg2_gfz_w1b">
                        <li class="li"><a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_amg_2qr_t1b">Pipeline events</a> - The event framework now generates pipeline
                            lifecycle events when the pipeline stops and starts. You can pass each
                            pipeline event to an executor or to another pipeline for more complex
                            processing. Use pipeline events to trigger tasks before pipeline
                            processing begins or after it stops.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_tpp_sfz_w1b">
                        <li class="li"><a class="xref" href="../Origins/BigQuery.html#concept_cg3_y3v_q1b" title="The Google BigQuery origin executes a query job and reads the result from Google BigQuery.">New Google
                                BigQuery origin</a> - An origin that executes a query job and
                            reads the result from Google BigQuery.</li>

                        <li class="li"><a class="xref" href="../Origins/PubSub.html#concept_pjw_qtl_r1b" title="The Google Pub/Sub Subscriber origin consumes messages from a Google Pub/Sub subscription.">New Google
                                Pub/Sub Subscriber origin</a> - A multithreaded origin that
                            consumes messages from a Google Pub/Sub subscription.</li>

                        <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#concept_nmf_1ly_f1b">New OPC UA
                                Client origin</a> - An origin that processes data from an OPC UA
                            server.</li>

                        <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_ut3_ywc_v1b">New SQL
                                Server CDC Client origin</a> - A multithreaded origin that reads
                            data from Microsoft SQL Server CDC tables. </li>

                        <li class="li"><a class="xref" href="../Origins/SQLServerChange.html#concept_ewq_b2s_r1b">New SQL
                                Server Change Tracking origin</a> - A multithreaded origin that
                            reads data from Microsoft SQL Server change tracking tables and
                            generates the latest version of each record.</li>

                        <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory origin event enhancements</a> - The Directory origin
                            can now generate no-more-data events when it completes processing all
                            available files and the batch wait time has elapsed without the arrival
                            of new files. Also, the File Finished event now includes the number of
                            records and files processed. </li>

                        <li class="li"><a class="xref" href="../Origins/HadoopFS-origin.html#concept_ogc_xzd_f1b" title="The Hadoop FS origin included in a cluster batch pipeline allows you to read from file systems other than HDFS using the Hadoop FileSystem interface.">Hadoop
                                FS origin enhancement</a> - The Hadoop FS origin now allows you
                            to read data from other file systems using the Hadoop FileSystem
                            interface. Use the Hadoop FS origin in cluster batch pipelines.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#task_akl_rkz_5r">HTTP
                                Client origin enhancement</a> - The HTTP Client origin now allows
                            time functions and datetime variables in the request body. It also
                            allows you to specify the time zone to use when evaluating the request
                            body. </li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_anf_ss4_qy">HTTP Server origin enhancement</a> - The HTTP Server origin can
                            now process Avro files.</li>

                        <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Query Consumer origin enhancement</a> - You can now
                            configure the behavior for the origin when it encounters data of an
                            unknown data type.</li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y">JDBC Multitable Consumer origin enhancements</a>:<ul class="ul" id="unique_468165943__ul_gnl_rhz_w1b">
                                <li class="li">You can now use the origin to perform multithreaded processing
                                    of partitions within a table. Use partition processing to handle
                                    even larger volumes of data. This enhancement also includes new
                                    JDBC header attributes.<p class="p">By default, all new pipelines use
                                        partition processing when possible. Upgraded pipelines use
                                        multithreaded table processing to preserve previous
                                        behavior.</p>
</li>

                                <li class="li">You  can now configure the behavior for the origin when it
                                    encounters data of an unknown data type.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle CDC
                                Client origin enhancements</a>:<ul class="ul" id="unique_468165943__ul_ych_yhz_w1b">
                                <li class="li">The origin can now buffer data locally rather than utilizing
                                    Oracle LogMiner buffers.</li>

                                <li class="li">You can now specify the behavior when the origin encounters an
                                    unsupported field type - send to the pipeline, send to error, or
                                    discard.</li>

                                <li class="li">You can configure the origin to include null values passed from
                                    the LogMiner full supplemental logging. By default, the origin
                                    ignores null values.</li>

                                <li class="li">You now must select the target server time zone for the origin. </li>

                                <li class="li">You can now configure a query timeout for the origin.</li>

                                <li class="li">The origin now includes the row ID in the oracle.cdc.rowId
                                    record header attribute and can include the LogMiner redo query
                                    in the oracle.cdc.query record header attribute.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/RabbitMQ.html#concept_rg5_yts_y1b">RabbitMQ Consumer origin enhancement</a> - When available, the
                            origin now provides attributes generated by RabbitMQ, such as
                            contentType, contentEncoding, and deliveryMode, as record header
                            attributes.</li>

                        <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_bqt_tl4_sz">TCP
                                Server origin enhancement</a> - The origin can now process
                            character-based data that includes a length prefix. </li>

                        <li class="li"><a class="xref" href="../Origins/UDP.html#concept_jhh_ryx_r1b">UDP Source
                                origin enhancement</a> - The origin can now process binary and
                            character-based raw data.</li>

                        <li class="li">New last-modified time record header attribute - <a class="xref" href="../Origins/Directory.html#concept_tlj_3g1_2z" title="The Directory origin creates record header attributes that include information about the originating file for the record.">Directory</a>, <a class="xref" href="../Origins/FileTail.html#concept_tlj_3g1_2z">File Tail</a>, and <a class="xref" href="../Origins/SFTP.html#concept_tlj_3g1_2z">SFTP/FTP Client</a> origins now include the last modified time
                            for the originating file for a record in an mtime record header
                            attribute.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_py4_vmz_w1b">
                        <li class="li"><a class="xref" href="../Processors/DataParser.html#concept_xw3_4xk_r1b" title="The Data Parser processor allows you to parse supported data formats embedded in a field. You can parse NetFlow embedded in a byte array field or syslog messages embedded in a string field.">New Data
                                Parser processor</a> - Use the new processor to extract NetFlow
                            or syslog messages as well as other supported data formats that are
                            embedded in a field. </li>

                        <li class="li"><a class="xref" href="../Processors/JSONParser.html#concept_bs1_4t3_yq">New JSON
                                Generator processor</a> - Use the new processor to serialize data
                            from a record field to a JSON-encoded string.</li>

                        <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_a1x_3wl_p1b" title="The Kudu Lookup processor performs lookups in a Kudu table and passes the lookup values to fields. Use the Kudu Lookup to enrich records with additional data.">New Kudu
                                Lookup processor</a> - Use the new processor to perform lookups
                            in Kudu to enrich records with additional data.</li>

                        <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_jv2_jjn_l1b">Hive Metadata processor enhancement</a> - You can now configure
                            custom record header attributes for metadata records.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_wdl_lnz_w1b">
                        <li class="li"><a class="xref" href="../Destinations/PubSubPublisher.html#concept_qsj_hk1_v1b">New Google Pub/Sub Publisher destination</a> - A destination
                            that publishes messages to Google Pub/Sub.</li>

                        <li class="li"><a class="xref" href="../Destinations/JMSProducer.html#concept_sfz_ww5_n1b">New
                                JMS Producer destination</a> - A destination that writes data to
                            JMS.</li>

                        <li class="li">Amazon S3 destination enhancements:<ul class="ul" id="unique_468165943__ul_ls3_d4z_w1b">
                                <li class="li">You can now use expressions in the <a class="xref" href="../Destinations/AmazonS3.html#concept_bnp_gwp_f1b">Bucket property</a> for the Amazon S3 destination. This
                                    enables you to write records dynamically based expression
                                    evaluation.</li>

                                <li class="li">The Amazon S3 object written <a class="xref" href="../Destinations/AmazonS3.html#concept_nly_sw2_px">event record</a> now includes the number of records
                                    written to the object.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#task_jfl_nf4_zx">Azure Data Lake Store destination enhancement</a> - The Client
                            ID and Client Key properties have been renamed Application ID and
                            Application Key to align with the updated property names in the new
                            Azure portal.</li>

                        <li class="li"><a class="xref" href="../Destinations/Cassandra.html#concept_ajh_vhp_x1b">Cassandra destination enhancement</a> - The destination now
                            supports Kerberos authentication if you have installed the DataStax
                            Enterprise Java driver. </li>

                        <li class="li"><a class="xref" href="../Destinations/Elasticsearch.html#task_uns_gtv_4r">Elasticsearch destination enhancement</a> - The destination can
                            now create parent-child relationships between documents in the same
                            index.</li>

                        <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive
                                Metastore destination</a> - You can now configure the destination
                            to create custom record header attributes.</li>

                        <li class="li"><a class="xref" href="../Destinations/KProducer.html#concept_lww_3b3_kr">Kafka Producer destination enhancement</a> - The destination can
                            now write XML documents.</li>

                        <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr
                                destination enhancement</a> - You can now configure the
                            destination to skip connection validation when the Solr configuration
                            file, <code class="ph codeph">solrconfig.xml</code>, does not define the default
                            search field (“df”) parameter.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_gqx_fpz_w1b">
                        <li class="li"><a class="xref" href="../Executors/AmazonS3.html#concept_mvh_bnm_f1b">New Amazon
                                S3 executor</a> - Use the Amazon S3 executor to create new Amazon
                            S3 objects for the specified content or add tags to existing objects
                            each time it receives an event.</li>

                        <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_yf2_hc4_x1b">HDFS File Metadata executor enhancement</a> - The executor can
                            now remove a file or directory when it receives an event.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Dataflow Performance Manager</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_dwx_lpz_w1b">
                        <li class="li"><a class="xref" href="../DPM/PipelineManagement.html#task_c4x_vff_p1b">Revert changes to published pipelines</a> - If you update a
                            published pipeline but decide not to publish the updates to DPM as a new
                            version, you can revert the changes made to the pipeline
                            configuration.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_ngb_qpz_w1b">
                        <li class="li"><a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_pm4_txm_vq">Pipeline error handling enhancements</a>: <ul class="ul" id="unique_468165943__ul_rq1_tpz_w1b">
                                <li class="li">Use the new Error Record Policy to specify the version of the
                                    record to include in error records. </li>

                                <li class="li">You can now write error records to Amazon Kinesis Streams.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_itf_55z_dz">Error records enhancement</a> - Error records now include the
                            user-defined stage label in the errorStageLabel header attribute.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#concept_s4p_ns5_nz">Pipeline state enhancements</a> - Pipelines can now display the
                            following new states: STARTING_ERROR, STOPPING_ERROR, and
                            STOP_ERROR.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Data Formats</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_btq_yqz_w1b">
                        <li class="li"><a class="xref" href="../Data_Formats/WritingXML.html#concept_t2m_hhx_41b">Writing
                                XML</a> - You can now use the Google Pub/Sub Publisher, JMS
                            Producer, and Kafka Producer destinations to write XML documents to
                            destination systems. Note the record structure requirement before you
                            use this data format. </li>

                        <li class="li">Avro:<ul class="ul" id="unique_468165943__ul_k4b_yrz_w1b">
                                <li class="li">Origins now write the Avro schema to an avroSchema record header
                                    attribute.</li>

                                <li class="li">Origins now include precision and scale <a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">field attributes</a> for every Decimal field.</li>

                                <li class="li">Data Collector now supports the time-based logical types added
                                    to Avro in version 1.8.</li>

                            </ul>
</li>

                        <li class="li">Delimited - Data Collector can now continue processing records with
                            delimited data when a row has more fields than the header. Previously,
                            rows with more fields than the header were sent to error. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Cluster Pipelines</dt>

                <dd class="dd">
                    <div class="p">This release includes the following <a class="xref" href="../Cluster_Mode/ClusterPipelines.html#concept_rjc_4m5_lx" title="Data Collector can run a cluster pipeline using cluster batch or cluster streaming execution mode.">Cluster Yarn Streaming enhancements</a>:<ul class="ul" id="unique_468165943__ul_b14_nsz_w1b">
                            <li class="li">Use a new Worker Count property to limit the number of worker nodes
                                used in Cluster Yarn Streaming pipelines. By default, a Data
                                Collector worker is spawned for each partition in the topic. </li>

                            <li class="li">You can now define Spark configuration properties to pass to the
                                spark-submit script.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Expression Language</dt>

                <dd class="dd">
                    <div class="p">This release includes the following new functions:<ul class="ul" id="unique_468165943__ul_abf_qsz_w1b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential:get()</a> - Returns credential values from a
                                credential store.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential:getWithOptions()</a> - Returns credential values
                                from a credential store using additional options to communicate with
                                the credential store.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ndj_43v_1r" title="Error record functions provide information about error records. Use error functions to process error records.">record:errorStageLabel()</a> - Returns the user-defined name
                                of the stage that generated the error record.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">list:join()</a>  - Merges elements in a List field into a
                                String field, using the specified separator between elements. </li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">list:joinSkipNulls()</a> - Merges elements in a List field
                                into a String field, using the specified separator between elements
                                and skipping null values.</li>

                        </ul>
</div>

                </dd>

                <dd class="dd ddexpand">
                    <div class="p">
                        <ul class="ul" id="unique_468165943__ul_k5c_rg4_z1b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">str:indexOf()</a> - Returns the index within a string of the
                                first occurrence of the specified subset of characters.</li>

                        </ul>

                    </div>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_468165943__ul_y2k_htz_w1b">
                        <li class="li"><a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">Global bulk edit mode</a> - In any property where you would
                            previously click an Add icon to add additional configurations, you can
                            now switch to bulk edit mode to enter a list of configurations in JSON
                            format.</li>

                        <li class="li">Snapshot enhancement - Snapshots no longer produce empty batches when
                            waiting for data.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/Webhooks.html#concept_rby_1rl_rz">Webhooks enhancement</a> - You can use several new pipeline
                            state notification parameters in webhooks.</li>

                    </ul>

                </dd>

            
        </dl>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title17" id="concept_avm_x1y_h1b">
 <h2 class="title topictitle2" id="ariaid-title17">What's New in 2.6.0.1</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.6.0.1 includes the following enhancement:<ul class="ul" id="concept_avm_x1y_h1b__ul_x5t_gby_h1b">
                <li class="li"><a class="xref" href="../Origins/KinConsumer.html#concept_dt1_4mq_h1b">Kinesis
                        Consumer origin</a> - You can now reset the origin for Kinesis Consumer
                    pipelines. Resetting the origin for Kinesis Consumer differs from other origins,
                    so please note the requirement and guidelines.</li>

            </ul>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title18" id="concept_bsw_cky_11b">
 <h2 class="title topictitle2" id="ariaid-title18">What's New in 2.6.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.6.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_s2s_lky_11b">
                            <li class="li"><a class="xref" href="../Installation/MapR-Prerequisites.html#concept_jgs_qpg_2v" title="Due to licensing restrictions, StreamSets cannot distribute MapR libraries with Data Collector. As a result, you must perform additional steps to enable the Data Collector machine to connect to MapR. Data Collector does not display MapR origins and destinations in stage library lists nor the MapR Streams statistics aggregator in the pipeline properties until you perform these prerequisites.">MapR prerequisites</a> - You can now run the
                                    <code class="ph codeph">setup-mapr</code> command in interactive or
                                non-interactive mode. In interactive mode, the command prompts you
                                for the MapR version and home directory. In non-interactive mode,
                                you define the MapR version and home directory in environment
                                variables before running the command.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">Data Collector now supports the following <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage libraries</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_zcv_4ky_11b">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">Hortonworks version 2.6 distribution of Apache
                                    Hadoop</p>

                            </li>

                            <li class="li">Cloudera distribution of Spark 2.1</li>

                            <li class="li">MapR distribution of Spark 2.1</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Collector Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_jys_cpm_d1b">
                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New buffer size configuration</a> - You can now use a new
                                parser.limit configuration property to increase the Data Collector
                                parser buffer size. The parser buffer is used by the origin to
                                process many data formats, including Delimited, JSON, and XML. The
                                parser buffer size limits the size of the records that origins can
                                process. The Data Collector parser buffer size is 1048576 bytes by
                                default.</li>

                        </ul>

                    </dd>

                
            </dl>
<dl class="dl">
                
                    <dt class="dt dlterm">Drift Synchronization Solution for Hive</dt>

                    <dd class="dd"><ul class="ul" id="concept_bsw_cky_11b__ul_x5p_zky_11b">
                            <li class="li"><a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_ndg_3zw_vz">Parquet support</a> - You can now use the <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_phk_bdf_2w">Drift Synchronization Solution for Hive</a> to generate
                                Parquet files. Previously, the Data Synchronization Solution
                                supported only Avro data. This enhancement includes the following
                                    updates:<ul class="ul" id="concept_bsw_cky_11b__ul_dt1_sky_11b">
                                    <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv">Hive Metadata processor</a> data format property -
                                        Use the new data format property to indicate the data format
                                        to use.</li>

                                    <li class="li">Parquet support in the <a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive Metastore destination</a> - The destination can
                                        now create and update Parquet tables in Hive. The
                                        destination no longer includes a data format property since
                                        that information is now configured in the Hive Metadata
                                        processor. </li>

                                </ul>
</li>

                        </ul>
See the documentation for implementation details and a <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_vl3_v2f_zz">Parquet case study</a>. </dd>

                
                
                    <dt class="dt dlterm">Multithreaded Pipelines</dt>

                    <dd class="dd">The <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">multithreaded framework</a> includes the following enhancements:<ul class="ul" id="concept_bsw_cky_11b__ul_a2d_4ry_11b">
                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wcz_tpd_py">Origins for multithreaded pipelines</a> - You can now use
                                the following origins to create multithreaded pipelines:<ul class="ul" id="concept_bsw_cky_11b__ul_efl_4ry_11b">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/CoAPServer.html#concept_wfy_ghn_sz" title="Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Server origin is a multithreaded origin that listens on a CoAP endpoint and processes the contents of all authorized CoAP requests.">CoAP Server origin</a></p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/TCPServer.html#concept_ppm_xb1_4z">TCP Server origin</a></p>

                                    </li>

                                </ul>
</li>

                            <li class="li">Multithreaded origin icons - The icons for multithreaded origins now
                                include the following multithreaded indicator: <img class="image" id="concept_bsw_cky_11b__image_al4_tpm_d1b" src="../Graphics/icon_Multithreaded.png" height="16" width="14" /><p class="p">For example, here’s the updated Elasticsearch
                                    origin icon:</p>
<p class="p"><img class="image" id="concept_bsw_cky_11b__image_iml_ypm_d1b" src="../Graphics/Multithreaded-icon.png" height="40" width="63" /></p>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Dataflow Triggers / Event Framework</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_ot4_ssy_11b">
                            <li class="li"><a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_rxg_shn_lx">New executors</a> - You can now use the following executors
                                to perform tasks upon receiving an event:<ul class="ul" id="concept_bsw_cky_11b__ul_pd2_tsy_11b">
                                    <li class="li"><a class="xref" href="../Executors/Email.html#concept_sjs_sfp_qz">Email executor</a></li>

                                    <li class="li"><a class="xref" href="../Executors/Shell.html#concept_jsr_zpw_tz">Shell executor</a>
                                    </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager (DPM)</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_pht_hld_d1b">
                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_h2q_mb5_xw" title="A Control Hub job defines the pipeline to run and the Data Collectors that run the pipeline. When you start a job, Control Hub remotely runs the pipeline on the group of Data Collectors. To monitor the job statistics and metrics within Control Hub, you must configure the pipeline to write statistics to Control Hub or to another system.">Pipeline statistics</a> - You can now configure a pipeline
                                to <a class="xref" href="../DPM/AggregatedStatistics.html#concept_abc_1w1_c1b" title="When you write statistics directly to Control Hub, Control Hub does not generate a system pipeline for the job. Instead, the Data Collector directly sends the statistics to Control Hub.">write statistics directly to DPM</a>. Write statistics
                                directly to DPM when you run a job for the pipeline on a single Data
                                Collector. <p class="p">When you run a job on multiple Data Collectors, a
                                    remote pipeline instance runs on each of the Data Collectors. To
                                    view aggregated statistics for the job within DPM, you must
                                    configure the pipeline to write the statistics to a Kafka
                                    cluster, Amazon Kinesis Streams, or SDC RPC.</p>
</li>

                            <li class="li"><a class="xref" href="../DPM/PipelineManagement.html#task_rxy_xqc_fx">Update
                                    published pipelines</a> - When you update a published
                                pipeline, Data Collector now displays a red asterisk next to the
                                pipeline name to indicate that the pipeline has been updated since
                                it was last published.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_xb4_bty_11b">
                            <li class="li"><a class="xref" href="../Origins/CoAPServer.html#concept_wfy_ghn_sz" title="Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Server origin is a multithreaded origin that listens on a CoAP endpoint and processes the contents of all authorized CoAP requests.">New CoAP
                                    Server origin</a> - An origin that listens on a CoAP endpoint
                                and processes the contents of all authorized CoAP requests. The
                                origin performs parallel processing and can generate multithreaded
                                pipelines. </li>

                            <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_ppm_xb1_4z">New TCP
                                    Server origin</a> - An origin that listens at the specified
                                ports, establishes TCP sessions with clients that initiate TCP
                                connections, and then processes the incoming data. The origin can
                                process NetFlow, syslog, and most Data Collector data formats as
                                separated records. You can configure custom acknowledgement messages
                                and use a new batchSize variable, as well as other expressions, in
                                the messages.</li>

                            <li class="li"><a class="xref" href="../Origins/SFTP.html#concept_g5p_5ks_b1b">SFTP/FTP Client origin enhancement</a> - You can now specify
                                the first file to process. This enables you to skip processing files
                                with earlier timestamps. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_dvb_jty_11b">
                            <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_ldh_sct_gv">Groovy</a>, <a class="xref" href="../Processors/JavaScript.html#concept_n2p_jgf_lr">JavaScript</a>, and <a class="xref" href="../Processors/Jython.html#concept_a1h_lkf_lr">Jython
                                    Evaluator</a> processor enhancements:<ul class="ul" id="concept_bsw_cky_11b__ul_e4s_jty_11b">
                                    <li class="li">You can now include some methods of the sdcFunctions
                                        scripting object in the initialization and destroy scripts
                                        for the processors.</li>

                                    <li class="li">You can now use runtime parameters in the code developed for
                                        a Groovy Evaluator processor.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv">Hive
                                    Metadata processor enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_v3b_lty_11b">
                                    <li class="li">The Hive Metadata processor can now <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_ndg_3zw_vz">process Parquet data as part of the Drift
                                            Synchronization Solution for Hive</a>. </li>

                                    <li class="li">You can now specify the data format to use: Avro or
                                        Parquet.</li>

                                    <li class="li">You can now configure an expression that defines comments
                                        for generated columns. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#concept_ysc_ccy_hw" title="The JDBC Lookup processor uses a JDBC connection to perform lookups in a database table and pass the lookup values to fields. Use the JDBC Lookup to enrich records with additional data.">JDBC
                                    Lookup processor enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_vls_4ty_11b">
                                    <li class="li">The JDBC Lookup processor can now return multiple values.
                                        You can now configure the lookup to return the first value
                                        or to return all matches as separate records. </li>

                                    <li class="li">When you <a class="xref" href="../Processors/JDBCLookup.html#concept_k3l_hrd_wz" title="When you monitor a pipeline that includes the JDBC Lookup processor, the Summary tab displays statistics about the queries that the JDBC Lookup processor performs. Use the statistics to help identify any performance bottlenecks encountered by the pipeline.">monitor a pipeline that includes the JDBC Lookup
                                            processor</a>, you can now view stage statistics
                                        about the number of queries the processor makes and the
                                        average time of the queries.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/Spark.html#concept_nbj_1jb_c1b">Spark Evaluator enhancement</a> - The Spark Evaluator now
                                supports Spark 2.x.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_p5x_qty_11b">
                            <li class="li"><a class="xref" href="../Destinations/CoAPClient.html#concept_hw5_s3n_sz" title="Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Client destination writes data to a CoAP endpoint. Use the destination to send requests to a CoAP resource URL.">New
                                    CoAP Client destination</a> - A destination that writes to a
                                CoAP endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive Metastore destination enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_y5w_sty_11b">
                                    <li class="li">The destination can now <a class="xref" href="../Destinations/HiveMetastore.html#concept_wyr_5jv_hw">create and update Parquet tables in Hive</a>. </li>

                                    <li class="li">Also, the data format property has been removed. You now
                                        specify the data format in the <a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv">Hive Metadata processor.</a>
                                        <p class="p">Since the Hive Metastore previously supported only Avro
                                            data, there is no upgrade impact. </p>
</li>

                                </ul>
</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/Kudu.html#concept_chy_xxg_4v" title="The Kudu destination writes data to a Kudu cluster.">Kudu
                                        destination enhancement</a> - You can use the new
                                    Mutation Buffer Space property to set the buffer size that the
                                    Kudu client uses to write each batch.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_xl4_b5y_11b">
                            <li class="li"><a class="xref" href="../Executors/Email.html#concept_sjs_sfp_qz">New Email
                                    executor</a> - Use to send custom emails upon receiving an
                                event. For a case study, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_t2t_lp5_xz">Case Study: Sending Email</a>.<ul class="ul" id="concept_bsw_cky_11b__ul_ekw_b5y_11b">
                                    <li dir="ltr" class="li">
                                        <p class="p"><a class="xref" href="../Executors/Shell.html#concept_jsr_zpw_tz">New Shell executor</a> - Use to execute shell
                                            scripts upon receiving an event. </p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p class="p"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor enhancement</a> - A new
                                            Batch Commit property allows the executor to commit to
                                            the database after each batch. Previously, the executor
                                            did not call commits by default.</p>

                                        <p class="p">For new pipelines, the property is enabled by default.
                                            For upgraded pipelines, the property is disabled to
                                            prevent changes in pipeline behavior. </p>

                                    </li>

                                    <li dir="ltr" class="li"><a class="xref" href="../Executors/Spark.html#concept_vbm_ywb_c1b">Spark executor enhancement</a> - The executor now
                                        supports Spark 2.x. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">REST API / Command Line Interface</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_ktv_k5y_11b">
                            <li class="li">Offset management - Both the REST API and <a class="xref" href="../Administration/Administration_title.html#concept_ywx_d5x_pt" title="Data Collector provides a command line interface that includes a basic cli command. Use the command to perform some of the same actions that you can complete from the Data Collector UI. Data Collector must be running before you can use the cli command.">command line interface</a> can now retrieve the last-saved
                                offset for a pipeline and set the offset for a pipeline when it is
                                not running. Use these commands to implement pipeline failover using
                                an external storage system. Otherwise, pipeline offsets are managed
                                by Data Collector and there is no need to update the offsets.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_fm5_m5y_11b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">vault:read enhancement</a> - The vault:read function now
                                supports returning the value for a key nested in a map.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">General</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_rlz_55y_11b">
                            <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_szj_3mw_xz">Support bundles</a> - You can now use Data Collector to
                                generate a support bundle. A support bundle is a ZIP file that
                                includes Data Collector logs, environment and configuration
                                information, pipeline JSON files, resource files, and pipeline
                                snapshots. <p class="p">You upload the generated file to the StreamSets
                                    support team so that we can use the information to troubleshoot
                                    your support tickets.</p>
</li>

                            <li class="li">
                                <div class="p"><a class="xref" href="../Pipeline_Configuration/SSL-TLS.html#concept_dd1_n3f_5z">TLS property enhancements</a> - Stages that support
                                    SSL/TLS now provide the following enhanced set of properties
                                    that enable more specific configuration:<ul class="ul" id="concept_bsw_cky_11b__ul_v5g_w5y_11b">
                                        <li class="li">Keystore and truststore type - You can now choose
                                            between Java Keystore (JKS) and PKCS-12 (p-12).
                                            Previously, Data Collector only supported JKS.</li>

                                        <li class="li">Transport protocols - You can now specify the transport
                                            protocols that you want to allow. By default, Data
                                            Collector allows only TLSv1.2. </li>

                                        <li class="li">Cipher suites - You can now specify the cipher suites to
                                            allow. Data Collector provides a modern set of default
                                            cipher suites. Previously, Data Collector always allowed
                                            the default cipher suites for the JRE.</li>

                                    </ul>
To avoid upgrade impact, all SSL/TLS/HTTPS properties in
                                    existing pipelines are preserved during upgrade. </div>

                            </li>

                            <li class="li">
                                <p class="p"><a class="xref" href="../Cluster_Mode/ClusterPipelines.html#concept_hmh_kfn_1s" title="A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode.">Cluster mode enhancement</a> - Cluster streaming mode
                                    now supports Spark 2.x. For information about using Spark 2.x
                                    stages with cluster mode, see <a class="xref" href="../Cluster_Mode/StageLimitations.html#concept_pdf_r5y_fz">Cluster Pipeline Limitations</a>.</p>

                            </li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_msl_yd4_fs">Precondition enhancement</a> - Stages with user-defined
                                preconditions now process all preconditions before passing a record
                                to error handling. This allows error records to include all
                                precondition failures in the error message.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_fdf_hrr_5q">Pipeline import</a>/<a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_dtz_4tr_5q">export enhancement</a> - When you export multiple pipelines,
                                Data Collector now includes all pipelines in a single zip file. You
                                can also import multiple pipelines from a single zip file. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title19" id="concept_afr_hly_tz">
 <h2 class="title topictitle2" id="ariaid-title19">What's New in 2.5.1.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.5.1.0 includes the following enhancement:<ul class="ul" id="concept_afr_hly_tz__ul_uwf_lly_tz">
                <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New
                        stage library</a> - <span class="ph">Data Collector</span> now supports the Cloudera CDH version 5.11 distribution of Hadoop and the
                    Cloudera version 5.11 distribution of Apache Kafka 2.1. <p class="p">Upgrading to this
                        version can require updating existing pipelines. For details, see <a class="xref" href="../Upgrade/Upgrade-ExternalSystems.html#concept_spf_2gq_vz">Working with Cloudera CDH 5.11 or Later</a>.</p>

                </li>

            </ul>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title20" id="concept_ddx_bpm_pz">
 <h2 class="title topictitle2" id="ariaid-title20">What's New in 2.5.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data Collector</span>
            version 2.5.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Multithreaded Pipelines</dt>

                <dd class="dd">
                    <div class="p">The multithreaded framework includes the following enhancements:<ul class="ul" id="concept_ddx_bpm_pz__ul_gs3_3pm_pz">
                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wcz_tpd_py">Origins for multithreaded pipelines</a> - You can now use
                                the following origins to create multithreaded pipelines:<ul class="ul" id="concept_ddx_bpm_pz__ul_hx3_mpm_pz">
                                    <li class="li">Elasticsearch origin</li>

                                    <li class="li">JDBC Multitable Consumer origin</li>

                                    <li class="li">Kinesis Consumer origin</li>

                                    <li class="li">WebSocket Server origin</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_fmg_pjd_mz">Maximum pipeline runners</a> - You can now configure a
                                maximum number of pipeline runners to use in a pipeline. Previously,
                                    <span class="ph">Data Collector</span> generated pipeline runners based on the number of threads created
                                by the origin. This allows you to tune performance and resource
                                usage. By default, <span class="ph">Data Collector</span> still generates runners based on the number of threads that the
                                origin uses. </li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_np1_pkz_ry">Record Deduplicator processor enhancement</a> - The
                                processor can now deduplicate records across all pipeline runners in
                                a multithreaded pipeline.</li>

                            <li class="li">Pipeline validation enhancement - The pipeline now displays
                                duplicate errors generated by using multiple threads as one error
                                message.</li>

                            <li class="li">Log enhancement - Multithreaded pipelines now include the runner ID
                                in log information. </li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_tdn_vwy_ry">Monitoring</a> - Monitoring now displays a histogram of
                                available pipeline runners, replacing the information previously
                                included in the Runtime Statistics list.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_asz_cqm_pz">
                        <li class="li"><span class="ph">Data Collector</span>
                            <a class="xref" href="../Configuration/RolesandPermissions.html#concept_i1p_hzd_yy">pipeline permissions</a> change - With this release, pipeline
                            permissions are no longer enabled by default. To enable pipeline
                            permissions, edit the <code class="ph codeph">pipeline.access.control.enabled</code>
                            <span class="ph">Data Collector</span> configuration property.</li>

                        <li class="li"><a class="xref" href="../Executors/PipelineFinisher.html#concept_qzm_l4r_kz">Stop
                                pipeline execution</a> - You can configure pipelines to transfer
                            data and automatically stop execution based on an event such as reaching
                            the end of a table. The JDBC and Salesforce origins can generate events
                            when they reach the end of available data that the Pipeline Finisher
                            uses to stop the pipeline. Click <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_kff_ykv_lz">here</a> for a case study. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_rjh_ntz_qr" title="Runtime parameters are parameters that you define in a pipeline and then call from within that same pipeline. When you start the pipeline, you specify the parameter values to use. Use runtime parameters to specify values for pipeline properties when you start the pipeline.">Pipeline runtime parameters</a> - You can now define runtime
                            parameters when you configure a pipeline, and then call the parameters
                            from within that pipeline. When you start the pipeline from the user
                            interface, the command line, or the REST API, you specify the values to
                            use for those parameters. Use pipeline parameters to represent any stage
                            or pipeline property with a value that must change for each pipeline run
                            - such as batch sizes and timeouts, directories, or URI.<p class="p">In previous
                                versions, pipeline runtime parameters were named pipeline constants.
                                You defined the constant values in the pipeline, and could not pass
                                different values when you started the pipeline.</p>
</li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/ConfiguringAPipeline.html#task_xlv_jdw_kq" title="Configure a pipeline to define the stream of data. After you configure the pipeline, you can start the pipeline.">Pipeline ID enhancement</a> - <span class="ph">Data Collector</span> now prefixes the pipeline ID with the alphanumeric characters entered
                            for the pipeline title. For example, if you enter “Oracle To HDFS” as
                            the pipeline title, then the pipeline ID has the following value:
                            OracleToHDFStad9f592-5f02-4695-bb10-127b2e41561c.</li>

                        <li class="li">Webhooks for pipeline state changes and alerts - You can now configure
                            pipeline state changes and metric and data alerts to call webhooks in
                            addition to sending email. For example, you can configure an incoming
                            webhook in Slack so that an alert can be posted to a Slack channel. Or,
                            you can configure a webhook to start another pipeline when the pipeline
                            state is changed to Finished or Stopped.</li>

                        <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_msh_k2q_yt" title="The manager command provides subcommands to start and stop a pipeline, view the status of all pipelines, and reset the origin for a pipeline. It can also be used to get the last-saved offset and to update the last-saved offset for a pipeline.">Force
                                a pipeline to stop from the command line</a> - If a pipeline
                            remains in a Stopping state, you can now use the command line to force
                            stop the pipeline immediately.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <p class="p"><span class="ph">Data Collector</span> now supports the Apache Kudu version 1.3.x. <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage library</a>.</p>

                </dd>

            
            
                <dt class="dt dlterm">Salesforce Stages</dt>

                <dd class="dd">
                    <div class="p">The following Salesforce stages include several enhancements:<ul class="ul" id="concept_ddx_bpm_pz__ul_gsv_prm_pz">
                            <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx">Salesforce origin</a> and <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor</a><ul class="ul" id="concept_ddx_bpm_pz__ul_k1d_5rm_pz">
                                    <li class="li">The origin and processor can use a proxy to connect to
                                        Salesforce.</li>

                                    <li class="li">You can now specify <code class="ph codeph">SELECT * FROM
                                            &lt;object&gt;</code> in a SOQL query. The origin or
                                        processor expands * to all fields in the Salesforce object
                                        that are accessible to the configured user. </li>

                                    <li class="li">The origin and processor generate Salesforce field
                                        attributes that provide additional information about each
                                        field, such as the data type of the Salesforce field.</li>

                                    <li class="li">The origin and processor can now additionally retrieve
                                        deleted records from the Salesforce recycle bin. </li>

                                    <li class="li">The origin can now generate events when it completes
                                        processing all available data.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">Salesforce destination</a> - The destination can now use a
                                CRUD operation record header attribute to indicate the operation to
                                perform for each record. You can also configure the destination to
                                use a proxy to connect to Salesforce. </li>

                            <li class="li"><a class="xref" href="../Destinations/WaveAnalytics.html#concept_hlx_r53_rx" title="The Einstein Analytics destination writes data to Salesforce Einstein Analytics. The destination connects to Einstein Analytics to upload external data to a dataset.">Wave Analytics destination</a> - You can now configure the
                                authentication endpoint and the API version that the destination
                                uses to connect to Salesforce Wave Analytics. You can also configure
                                the destination to use a proxy to connect to Salesforce.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_z25_bsm_pz">
                        <li class="li"><a class="xref" href="../Origins/Elasticsearch.html#concept_f1q_vpm_2z" title="The Elasticsearch origin is a multithreaded origin that reads data from an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters). The origin generates a record for each Elasticsearch document.">New
                                Elasticsearch origin</a> - An origin that reads data from an
                            Elasticsearch cluster. The origin uses the Elasticsearch scroll API to
                            read documents using a user-defined Elasticsearch query. The origin
                            performs parallel processing and can generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Origins/MQTTSubscriber.html#concept_ukz_3vt_lz" title="The MQTT Subscriber origin subscribes to topics on an MQTT broker to read messages from the broker. The origin functions as an MQTT client that receives messages, generating a record for each message.">New MQTT
                                Subscriber origin</a> - An origin that subscribes to a topic on
                            an MQTT broker to read messages from the broker.</li>

                        <li class="li"><a class="xref" href="../Origins/WebSocketServer.html#concept_u2r_gpc_3z" title="The WebSocket Server origin is a multithreaded origin that listens on a WebSocket endpoint and processes the contents of all authorized WebSocket client requests. Use the WebSocket Server origin to read high volumes of WebSocket client requests using multiple threads.">New
                                WebSocket Server origin</a> - An origin that listens on a
                            WebSocket endpoint and processes the contents of all authorized
                            WebSocket requests. The origin performs parallel processing and can
                            generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev
                                Data Generator origin enhancement</a> - When you configure the
                            origin to generate events to test event handling functionality, you can
                            now specify the event type to use.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_wk4_bjz_5r">HTTP Client
                                origin enhancements</a> - When using pagination, the origin can
                            include all response fields in the resulting record in addition to the
                            fields in the specified result field path. The origin can now also
                            process the following new data formats: Binary, Delimited, Log, and SDC
                            Record.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_thw_wtd_kz" title="Configure the HTTP clients to include the HTTP Server application ID in each request.">HTTP Server origin enhancement</a> - The origin requires that
                            HTTP clients include the application ID in all requests. You can now
                            configure HTTP clients to send data to a URL that includes the
                            application ID in a query parameter, rather than including the
                            application ID in request headers. </li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y">JDBC Multitable Consumer origin enhancements</a> - The origin
                            now performs parallel processing and can generate multithreaded
                            pipelines. The origin can generate events when it completes processing
                            all available data. <p class="p">You can also configure the quote character to use
                                around table, schema, and column names in the query. And you can
                                configure the number of times a thread tries to read a batch of data
                                after receiving an SQL error.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_qhf_hjr_bs">JDBC Query
                                Consumer origin enhancements</a> - The origin can now generate
                            events when it completes processing all available data, and when it
                            successfully completes or fails to complete a query. <p class="p">To handle
                                transient connection or network errors, you can now specify how many
                                times the origin should retry a query before stopping the
                                pipeline.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/KConsumer.html#concept_msz_wnr_5q">Kinesis
                                Consumer origin enhancement</a> - The origin now performs
                            parallel processing and can generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_gzz_kdr_tz">MongoDB origin</a> and <a class="xref" href="../Origins/MongoDBOplog.html#concept_ovt_vpt_tz">MongoDB Oplog origin</a> enhancements - The origins can now use
                            LDAP authentication in addition to username/password authentication to
                            connect to MongoDB. You can also now include credentials in the MongoDB
                            connection string.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_apy_wsm_pz">
                        <li class="li"><a class="xref" href="../Processors/FieldOrder.html#concept_krp_5fv_vy" title="The Field Order processor orders fields in a map or list-map field and outputs the fields into a list-map or list root field.">New Field
                                Order processor</a> - A processor that orders fields in a map or
                            list-map field and outputs the fields into a list-map or list root
                            field.</li>

                        <li class="li">Field Flattener enhancement - You can now flatten a field in place to
                            raise it to the parent level.</li>

                        <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_ldh_sct_gv">Groovy</a>,
                                <a class="xref" href="../Processors/JavaScript.html#concept_n2p_jgf_lr">JavaScript</a>, and <a class="xref" href="../Processors/Jython.html#concept_a1h_lkf_lr">Jython
                                Evaluator</a> processor enhancement - You can now develop an
                            initialization script that the processor runs once when the pipeline
                            starts. Use an initialization script to set up connections or resources
                            required by the processor.<p class="p">You can also develop a destroy script that
                                the processor runs once when the pipeline stops. Use a destroy
                                script to close any connections or resources opened by the
                                processor.</p>
</li>

                        <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup enhancement</a> - Default value date formats. When
                            the default value data type is Date, use the following format:
                            yyyy/MM/dd . When the default value data type is Datetime, use the
                            following format: yyyy/MM/dd HH:mm:ss.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Record
                                Deduplicator processor enhancement</a> - The processor can now
                            deduplicate records across all pipeline runners in a multithreaded
                            pipeline.</li>

                        <li class="li"><a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx" title="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop.">Spark Evaluator
                                processor enhancements</a> - The processor is now included in the
                            MapR 5.2 stage library. <p class="p">The processor also now provides beta support
                                of cluster mode pipelines. In a development or test environment, you
                                can use the processor in pipelines that process data from a Kafka or
                                MapR cluster in cluster streaming mode. Do not use the Spark
                                Evaluator processor in cluster mode pipelines in a production
                                environment.</p>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                        <ul class="ul" id="concept_ddx_bpm_pz__ul_x13_lym_pz">
                            <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#concept_khl_sg5_lz" title="The HTTP Client destination writes data to an HTTP endpoint. The destination sends requests to an HTTP resource URL. Use the HTTP Client destination to perform a range of standard requests or use an expression to determine the request for each record.">New
                                    HTTP Client destination</a> - A destination that writes to an
                                HTTP endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/MQTTPublisher.html#concept_odz_txt_lz" title="The MQTT Publisher destination publishes messages to a topic on an MQTT broker. The destination functions as an MQTT client that publishes messages, writing each record as a message.">New MQTT Publisher destination</a> - A destination that
                                publishes messages to a topic on an MQTT broker.</li>

                            <li class="li"><a class="xref" href="../Destinations/WebSocketClient.html#concept_l4d_mjn_lz" title="The WebSocket Client destination writes data to a WebSocket endpoint. Use the destination to send data to a WebSocket resource URL.">New WebSocket Client destination</a> - A destination that
                                writes to a WebSocket endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_c2p_wzh_4z">Azure Data Lake Store destination enhancement</a> - You can
                                now configure an idle timeout for output files. </li>

                            <li class="li">Cassandra destination enhancements - The destination now supports
                                the Cassandra uuid and timeuuid data types. And you can now specify
                                the Cassandra batch type to use: Logged or Unlogged. Previously, the
                                destination used the Logged batch type.</li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht" title="The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log.">JDBC Producer enhancements</a> - The origin now includes a
                                Schema Name property for entering the schema name. For information
                                about possible upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_cmh_ryd_pz">Configure JDBC Producer Schema Names</a>.<p class="p">You can also use
                                    the Enclose Object Name property to enclose the database/schema,
                                    table, and column names in quotation marks when writing to the
                                    database. </p>
</li>

                            <li class="li"><a class="xref" href="../Destinations/MapRDBJSON.html#task_wq3_wkj_dy">MapR DB JSON destination enhancement</a> - You can now enter an
                            expression that evaluates to the name of the MapR DB JSON table to write
                            to.</li>

                        <li class="li"><a class="xref" href="../Destinations/MongoDB.html#concept_ppl_3qt_tz">MongoDB destination enhancements</a> - The destination can now
                            use LDAP authentication in addition to username/password authentication
                            to connect to MongoDB. You can also now include credentials in the
                            MongoDB connection string.</li>

                        <li class="li"><a class="xref" href="../Destinations/SDC_RPCdest.html#task_nbl_r2x_dt">SDC RPC destination enhancements</a> - The Back Off Period value
                            that you enter now increases exponentially after each retry, until it
                            reaches the maximum wait time of 5 minutes. Previously, there was no
                            limit to the maximum wait time. The maximum value for the Retries per
                            Batch property is now unlimited - previously it was 10 retries.</li>

                        <li class="li"><a class="xref" href="../Destinations/Solr.html#concept_z2g_q1r_wr" title="The Solr destination writes data to a Solr node or cluster.">Solr
                                destination enhancement</a> - You can now configure the action
                            that the destination takes when it encounters missing fields in the
                            record. The destination can discard the fields, send the record to
                            error, or stop the pipeline.</li>

                        </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ojc_3zm_pz">
                        <li class="li"><a class="xref" href="../Executors/Spark.html#concept_cvy_vxb_1z">New Spark
                                executor</a> - The executor starts a Spark application on a YARN
                            or Databricks cluster each time it receives an event.</li>

                        <li class="li"><a class="xref" href="../Executors/PipelineFinisher.html#concept_qzm_l4r_kz">New
                                Pipeline Finisher executor</a> - The executor stops the pipeline
                            and transitions it to a Finished state when it receives an event. Can be
                            used with the JDBC Query Consumer, JDBC Multitable Consumer, and
                            Salesforce origins to perform batch processing of available data.</li>

                        <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File
                                Metadata executor enhancement</a> - The executor can now create
                            an empty file upon receiving an event. The executor can also generate a
                            file-created event when generating events. </li>

                        <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce
                                executor enhancement</a> - When starting the provided Avro to
                            Parquet job, the executor can now overwrite any temporary files created
                            from a previous run of the job.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Functions</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ayq_pzm_pz">
                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New escape XML functions</a> - Three new string functions enable
                            you to escape and unescape XML.</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">New pipeline user function</a> - A new pipeline user function
                            enables you to determine the user who started the pipeline. </li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New function to generate UUIDs</a> - A new function that enables
                            you generate UUIDs.</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New function returns the number of available processors</a> -
                            The runtime:availableProcessors() function returns the number of
                            processors available to the Java virtual machine.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">General Enhancements</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ksn_tzm_pz">
                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#concept_pmr_sy5_nz">Data Collector Hadoop impersonation enhancement</a> - You can
                            use the
                                <code class="ph codeph">stage.conf_hadoop.always.impersonate.current.user</code>
                            <span class="ph">Data Collector</span> configuration property to ensure that <span class="ph">Data Collector</span> uses the current <span class="ph">Data Collector</span> user to read from or write to Hadoop systems. <div class="p">When enabled, you
                                cannot configure alternate users in the following Hadoop-related
                                    stages:<ul class="ul" id="concept_ddx_bpm_pz__ul_ztn_xzm_pz">
                                    <li class="li">Hadoop FS origin and destination</li>

                                    <li class="li">MapR FS origin and destination</li>

                                    <li class="li">HBase lookup and destination</li>

                                    <li class="li">MapR DB destination</li>

                                    <li class="li">HDFS File Metadata executor</li>

                                    <li class="li">Map Reduce executor</li>

                                </ul>
</div>
</li>

                        <li class="li">Stage precondition property enhancement - Records that do not meet all
                            preconditions for a stage are now processed based on error handling
                            configured in the stage. Previously, they were processed based on error
                            handling configured for the pipeline. See <a class="xref" href="../Upgrade/PostUpgrade.html#concept_gk3_s5l_nz">Evaluate Precondition Error Handling</a> for information about
                            upgrading.</li>

                        <li class="li">XML parsing enhancements - You can include field XPath expressions and
                            namespaces in the record with the <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_w3k_1ch_qz">Include Field XPaths property</a>. And use the new <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_jll_4wh_qz">Output Field Attributes property</a> to write XML attributes and
                            namespace declarations to field attributes rather than including them in
                            the record as fields. </li>

                        <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">Wrap long lines in properties</a> - You can now configure <span class="ph">Data Collector</span> to wrap long lines of text that you enter in properties, instead of
                            displaying the text with a scroll bar.</li>

                    </ul>

                </dd>

            
        </dl>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title21" id="concept_cf2_sdz_fz">
 <h2 class="title topictitle2" id="ariaid-title21">What's New in 2.4.1.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.4.1.0 includes the following new features and enhancements:<ul class="ul" id="concept_cf2_sdz_fz__ul_xdl_tdz_fz">
                <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_owv_nj5_2z">Salesforce origin enhancement</a> - When the origin processes existing
                    data and is not subscribed to notifications, it can now repeat the specified
                    query at regular intervals. The origin can repeat a full or incremental
                    query.</li>

                <li class="li"><a class="xref" href="../Administration/Administration_title.html#task_gbm_s3k_br">Log data
                        display</a> - You can stop and restart the automatic display of the most
                    recent log data on the Data Collector Logs page.</li>

                <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">New
                        time function</a> - The <code class="ph codeph">time:createDateFromStringTZ</code>
                    function enables creating Date objects adjusted for time zones from string
                    datetime values.</li>

                <li class="li">New stage library stage-type icons - The stage library now displays icons to
                    differentiate between different stage types.</li>

            </ul>
<div class="note note"><span class="notetitle">Note:</span> The Hive Drift Solution is now known as the "Drift Synchronization Solution
                for Hive" in the documentation.</div>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title22" id="concept_kzc_4sd_yy">
 <h2 class="title topictitle2" id="ariaid-title22">What's New in 2.4.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data Collector</span>
            version 2.4.0.0 includes the following new features and enhancements:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Pipeline Sharing and Permissions</dt>

                    <dd class="dd">Data Collector now provides pipeline-level permissions. Permissions
                        determine the access level that users and groups have on pipelines. To
                        create a multitenant environment, create groups of users and then share
                        pipelines with the groups to grant different levels of access.</dd>

                    <dd class="dd ddexpand">With this change, only the pipeline owner and users with the Admin role can
                        view a pipeline by default. If upgrading from a previous version of Data
                        Collector, see the following post-upgrade task, <a class="xref" href="../Upgrade/PostUpgrade.html#concept_zbn_fpw_xy">Configure Pipeline Permissions</a>.</dd>

                    <dd class="dd ddexpand">This feature includes the following components:<ul class="ul" id="concept_kzc_4sd_yy__ul_qhv_ds1_cz">
                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_i1p_hzd_yy">Pipeline permissions</a> - Pipelines now have read, write,
                                and execute permissions. Pipeline permissions overlay existing Data
                                Collector roles to provide greater security. For information, see
                                    <a class="xref" href="../Configuration/RolesandPermissions.html#concept_k1r_prc_yy">Roles and Permissions</a>.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#concept_jrg_1vy_wy">Pipeline sharing</a> - The pipeline owner and users with the
                                Admin role can configure pipeline permissions for users and
                                groups.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Data Collector pipeline access control property</a> - You
                                can enable and disable the use of pipeline permissions with the
                                pipeline.access.control.enabled configuration property. By default,
                                this property is enabled.</li>

                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_p11_msc_1z">Permissions transfer</a> - You can transfer all pipeline
                                permissions associated with a user or group to a different user or
                                group. Use pipeline transfer to easily migrate permissions after
                                registering with DPM or after a user or group becomes obsolete.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager (DPM)</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_f3t_zs1_cz">
                            <li class="li"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#task_a4y_v1g_xw" title="You can register a Data Collector with Control Hub from the Data Collector UI.">Register Data Collectors with DPM</a> - If Data Collector
                                uses file-based authentication and if you register the Data
                                Collector from the Data Collector UI, you can now create DPM user
                                accounts and groups during the registration process.</li>

                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_c53_pzp_yy">Aggregated statistics for DPM</a> - When working with DPM,
                                you can now configure a pipeline to write aggregated statistics to
                                SDC RPC. Write statistics to SDC RPC for development purposes only.
                                For a production environment, use a Kafka cluster or Amazon Kinesis
                                Streams to aggregate statistics.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_ad3_kt1_cz">
                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev SDC RPC with Buffering origin</a> - A new development
                                stage that receives records from an SDC RPC destination, temporarily
                                buffering the records to disk before passing the records to the next
                                stage in the pipeline. Use as the origin in an SDC RPC destination
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin enhancement</a> - You can configure a new
                                File Pool Size property to determine the maximum number of files
                                that the origin stores in memory for processing after loading and
                                sorting all files present on S3.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Other</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_hsj_tt1_cz">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release supports the
                                following new stage libraries:<ul class="ul" id="concept_kzc_4sd_yy__ul_xrv_5t1_cz">
                                    <li dir="ltr" class="li">Kudu versions 1.1 and 1.2</li>

                                    <li class="li">
                                        <p dir="ltr" class="p">Cloudera CDH version 5.10 distribution of
                                            Hadoop</p>

                                    </li>

                                    <li dir="ltr" class="li">Cloudera version 5.10 distribution of Apache Kafka
                                        2.1</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft">Install external libraries using the Data Collector user
                                    interface</a> - You can now use the Data Collector user
                                interface to install external libraries to make them available to
                                stages. For example, you can install JDBC drivers for stages that
                                use JDBC connections. Or, you can install external libraries to call
                                external Java code from the Groovy, Java, and Jython Evaluator
                                processors.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Custom header enhancement</a> - You can now use HTML in the
                                ui.header.title configuration property to configure a custom header
                                for the <span class="ph">Data Collector</span> UI. This allows you to specify the look and feel for any text
                                that you use, and to include small images in the header. </li>

                            <li class="li"><a class="xref" href="../Processors/Groovy.html#task_asl_bpt_gv">Groovy enhancement</a> - You can configure the processor to
                                use the invokedynamic bytecode instruction.</li>

                            <li class="li">Pipeline renaming - You can now rename a pipeline by clicking
                                directly on the pipeline name when editing the pipeline, in addition
                                to editing the Title general pipeline property.</li>

                        </ul>

                    </dd>

                
            </dl>

        </div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title23" id="concept_bml_dbt_wy">
 <h2 class="title topictitle2" id="ariaid-title23">What's New in 2.3.0.1</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.3.0.1 includes the following new features and enhancements:<ul class="ul" id="concept_bml_dbt_wy__ul_srs_hbt_wy">
                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle CDC Client
                        origin enhancement</a> - The origin can now track and adapt to schema
                    changes when reading the dictionary from redo logs. When using the dictionary in
                    redo logs, the origin can also generate events for each DDL that it reads. </li>

                <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New
                        Data Collector property</a> - The http.enable.forwarded.requests property
                    in the Data Collector configuration file enables handling X-Forwarded-For,
                    X-Forwarded-Proto, X-Forwarded-Port request headers issued by a reverse proxy or
                    load balancer.</li>

                <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_kx3_zrs_ns">MongoDB
                        origin enhancement</a> ­ The origin now supports using any string field
                    as the offset field. </li>

            </ul>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title24" id="concept_yym_xqt_5y">
 <h2 class="title topictitle2" id="ariaid-title24">What's New in 2.3.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span>
            version 2.3.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Multithreaded Pipelines</dt>

                    <dd class="dd">You can use a multithreaded origin to generate <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">multithreaded pipelines</a> to perform parallel processing. <p dir="ltr" class="p">The new multithreaded framework includes the following
                            changes:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_slg_mrt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y" title="The HTTP Server origin is a multithreaded origin that listens on an HTTP endpoint and processes the contents of all authorized HTTP POST and PUT requests. Use the HTTP Server origin to read high volumes of HTTP POST and PUT requests using multiple threads.">HTTP
                                        Server origin</a> - Listens on an HTTP endpoint and
                                    processes the contents of all authorized HTTP POST requests. Use
                                    the HTTP Server origin to receive high volumes of HTTP POST
                                    requests using multiple threads.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Enhanced Dev Data Generator origin</a> - Can create
                                    multiple threads for testing multithreaded pipelines.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_tdn_vwy_ry">Enhanced runtime statistics</a> - Monitoring a pipeline
                                    displays aggregated runtime statistics for all threads in the
                                    pipeline. You can also view the number of runners, i.e. threads
                                    and pipeline instances, being used.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">CDC/CRUD Enhancements</dt>

                    <dd class="dd">With this release, certain Data Collector stages enable you to easily
                        process change data capture (CDC) or transactional data in a pipeline. The
                        sdc.operation.type record header attribute is now used by all CDC-enabled
                        origins and CRUD-enabled stages:<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_iws_mhd_ty">CDC-enabled origins</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_vb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB Oplog and Salesforce origins are now
                                    enabled for processing changed data by including the CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p">Though previously CDC-enabled, the Oracle CDC Client and JDBC
                                    Query Consumer for Microsoft SQL Server now include CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                                <p class="p">Previous operation type header attributes are still supported for
                                    backward-compatibility. </p>

                            </li>

                        </ul>
<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_lfb_phd_ty">CRUD-enabled stages</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_wb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The JDBC Tee processor and JDBC Producer can now
                                    process changed data based on CRUD operations in record headers.
                                    The stages also include a default operation and unsupported
                                    operation handling. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB and Elasticsearch destinations now look for
                                    the CRUD operation in the sdc.operation.type record header
                                    attribute. The Elasticsearch destination includes a default
                                    operation and unsupported operation handling.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Multitable Copy</dt>

                    <dd class="dd">You can use the new <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y">JDBC
                            Multitable Consumer origin</a> when you need to copy multiple tables
                        to a destination system or for database replication. The JDBC Multitable
                        Consumer origin reads database data from multiple tables through a JDBC
                        connection. The origin generates SQL queries based on the table
                        configurations that you define.</dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_up5_ntt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Configuration/Authentication.html#concept_wgy_rxt_5x" title="If your organization does not use LDAP, configure Data Collector to use the default file-based authentication.">Groups for file-based authentication</a> - If you use
                                file-based authentication, you can now create groups of users when
                                multiple users use Data Collector. You configure groups in the
                                associated realm.properties file located in the Data Collector
                                configuration directory, $SDC_CONF. <p class="p">If you use file-based
                                    authentication, you can also now view all user accounts granted
                                    access to the Data Collector, including the roles and groups
                                    assigned to each user.</p>
</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/Authentication.html#concept_dns_dvg_h5" title="Data Collector can authenticate user accounts based on LDAP or files. Best practice is to use LDAP if your organization has it. By default, Data Collector uses file-based authentication.">LDAP authentication enhancements</a> - You can now
                                    configure Data Collector to use StartTLS to make secure
                                    connections to an LDAP server. You can also configure the
                                    userFilter property to define the LDAP user attribute used to
                                    log in to Data Collector. For example, a username, uid, or email
                                    address.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#concept_dmr_df5_5y" title="You can configure each registered Data Collector to use an authenticated HTTP or HTTPS proxy server for outbound requests made to Control Hub. Define the proxy properties in the SDC_JAVA_OPTS environment variable.">Proxy
                                        configuration for outbound requests</a> - You can now
                                    configure Data Collector to use an authenticated HTTP proxy for
                                    outbound requests to Dataflow Performance Manager (DPM).</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector logging</a> - Data Collector now
                                    enables logging for the Java garbage collector by default. Logs
                                    are written to $SDC_LOG/gc.log. You can disable the logging if
                                    needed. </p>

                            </li>

                            <li dir="ltr" class="li">Heap dump for out of memory errors - Data Collector now
                                produces a heap dump file by default if it encounters an out of
                                memory error. You can configure the location of the heap dump file
                                or you can disable this default behavior. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Administration/Administration_title.html#task_lkv_g2f_wy" title="If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level.">Modifying the log level</a> - You can now use the Data
                                Collector UI to modify the log level to display messages at another
                                severity level.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipelines</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_lzx_xtt_5y">
                            <li dir="ltr" class="li">Pipeline renaming - You can now rename pipelines by
                                editing the Title general pipeline property.</li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">Field attributes</a> - Data Collector now supports
                                    field-level attributes. Use the Expression Evaluator to add
                                    field attributes.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_ljw_15t_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y" title="The HTTP Server origin is a multithreaded origin that listens on an HTTP endpoint and processes the contents of all authorized HTTP POST and PUT requests. Use the HTTP Server origin to read high volumes of HTTP POST and PUT requests using multiple threads.">New HTTP Server origin</a> - A multithreaded origin that
                                listens on an HTTP endpoint and processes the contents of all
                                authorized HTTP POST requests. Use the HTTP Server origin to read
                                high volumes of HTTP POST requests using multiple threads. </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPtoKafka.html#concept_izh_mqd_dy">New
                                        HTTP to Kafka origin</a> - Listens on a HTTP endpoint and
                                    writes the contents of all authorized HTTP POST requests
                                    directly to Kafka. Use to read high volumes of HTTP POST
                                    requests and write them to Kafka. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MapRDBJSON.html#concept_ywh_k15_3y" title="The MapR DB JSON origin reads JSON documents from MapR DB JSON tables. The origin converts each document into a record.">New
                                        MapR DB JSON origin</a> - Reads JSON documents from MapR
                                    DB JSON tables.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MongoDBOplog.html#concept_mjn_yqw_4y">New
                                        MongoDB Oplog origin</a> - Reads entries from a MongoDB
                                    Oplog. Use to process change information for data or database
                                    operations. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/Directory.html#concept_xd5_5z4_4y" title="Use a file name pattern to define the files that the Directory origin processes. You can use either a glob pattern or a regular expression to define the file name pattern.">Directory origin enhancement</a> - You can use regular
                                    expressions in addition to glob patterns to define the file name
                                    pattern to process files. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPClient.html#concept_c13_zz1_5y" title="You can configure the HTTP Client origin to use the OAuth 2 protocol to connect to an HTTP service that uses basic, digest, or universal authentication, OAuth 2 client credentials, OAuth 2 username and password, or OAuth 2 JSON Web Tokens (JWT).">HTTP Client origin enhancement</a> - You can now
                                    configure the origin to use the OAuth 2 protocol to connect to
                                    an HTTP service.</p>

                            </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_qhf_hjr_bs">JDBC
                                    Query Consumer origin enhancements</a> - The JDBC Consumer
                                origin has been renamed to the JDBC Query Consumer origin. The
                                origin functions the same as in previous releases. It reads database
                                data using a user-defined SQL query through a JDBC connection. You
                                can also now configure the origin to enable auto-commit mode for the
                                JDBC connection and to disable validation of the SQL query.</li>

                            <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_bk4_2rs_ns">MongoDB
                                    origin enhancements</a> - You can now use a nested field as
                                the offset field. The origin supports reading the MongoDB BSON
                                timestamp for MongoDB versions 2.6 and later. And you can configure
                                the origin to connect to a single MongoDB server or node. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_z35_cwt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#task_g23_2tq_wq">Field Type Converter processor enhancement</a> - You can now
                                configure the processor to convert timestamp data in a long field to
                                a String data type. Previously, you had to use one Field Type
                                Converter processor to convert the long field to a datetime, and
                                then use another processor to convert the datetime field to a
                                string.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/HTTPClient.html#concept_ghx_ypr_fw">HTTP Client processor enhancements</a>  - You can now
                                    configure the processor to use the OAuth 2 protocol to connect
                                    to an HTTP service. You can also configure a rate limit for the
                                    processor, which defines the maximum number of requests to make
                                    per second.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/JDBCLookup.html#concept_ysc_ccy_hw" title="The JDBC Lookup processor uses a JDBC connection to perform lookups in a database table and pass the lookup values to fields. Use the JDBC Lookup to enrich records with additional data.">JDBC Lookup processor enhancements</a> - You can now
                                    configure the processor to enable auto-commit mode for the JDBC
                                    connection. You can also configure the processor to use a
                                    default value if the database does not return a lookup value for
                                    a column.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor enhancement</a> - You can
                                    now configure the processor to use a default value if Salesforce
                                    does not return a lookup value for a field.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5">XML
                                        Parser enhancement</a> - A new Multiple Values Behavior
                                    property allows you to specify the behavior when you define a
                                    delimiter element and the document includes more than one value:
                                    Return the first value as a record, return one record with a
                                    list field for each value, or return all values as records.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_uys_hwt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/MapRDBJSON.html#concept_i4h_2kj_dy">New
                                    MapR DB JSON destination</a> - Writes data as JSON documents
                                to MapR DB JSON tables.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> enhancement - You
                                    can now use the destination in cluster batch pipelines. You can
                                    also process binary and protobuf data, use record header
                                    attributes to write records to files and roll files, and
                                    configure a file suffix and the maximum number of records that
                                    can be written to a file. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/Elasticsearch.html#concept_u5t_vpv_4r" title="The Elasticsearch destination writes data to an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters). The destination uses the Elasticsearch HTTP API to write each record to Elasticsearch as a document.">Elasticsearch destination enhancement</a> - The
                                    destination now uses the Elasticsearch HTTP API. With this API,
                                    the Elasticsearch version 5 stage library is compatible with all
                                    versions of Elasticsearch. Earlier stage library versions have
                                    been removed. Elasticsearch is no longer supported on Java 7.
                                    You’ll need to verify that Java 8 is installed on the Data
                                    Collector machine and remove this stage from the blacklist
                                    property in $SDC_CONF/sdc.properties before you can use it. </p>

                                <p class="p">You can also now configure the destination to perform any of the
                                    following CRUD operations: create, update, delete, or index.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HiveMetastore.html#concept_x4p_fyc_rx">Hive Metastore destination enhancement</a> - New table
                                    events now include information about columns and partitions in
                                    the table.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_uv2_vfb_vy">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_s5n_ggc_vy">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_vvr_ngc_vy">MapR FS</a> destination enhancement - The destinations
                                    now support recovery after an unexpected stop of the pipeline by
                                    renaming temporary files when the pipeline restarts.</p>

                            </li>

                            <li dir="ltr" class="li">Redis destination enhancement - You can now configure a
                                timeout for each key that the destination writes to Redis.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_zqk_4wt_5y">
                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive
                                        Query executor enhancements</a>: <ul class="ul" id="concept_yym_xqt_5y__ul_gf2_qwt_5y">
                                        <li class="li">The executor can now execute multiple queries for each
                                            event that it receives.</li>

                                        <li class="li">It can also generate event records each time it
                                            processes a query.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                        <ul class="ul" id="concept_yym_xqt_5y__ul_brk_4wt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC
                                        Query executor enhancement</a> - You can now configure
                                    the executor to enable auto-commit mode for the JDBC
                                    connection.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hpw_zwt_5y">
                            <li class="li"><a class="xref" href="../Data_Formats/WholeFile.html#concept_prp_jzd_py">Whole File enhancement</a> - You can now specify a transfer
                                rate to help control the resources used to process whole files. You
                                can specify the rate limit in all origins that process whole
                                files.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hq3_2xt_5y">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">New pipeline functions</a> - You can use the following new
                                pipeline functions to return pipeline information:<ul class="ul" id="concept_yym_xqt_5y__ul_mls_bp1_cz">
                                    <li class="li">pipeline:id() - Returns the pipeline ID, a UUID that is
                                        automatically generated and used by Data Collector to
                                        identify the pipeline. <div class="note note"><span class="notetitle">Note:</span> The existing pipeline:name()
                                            function now returns the pipeline ID instead of the
                                            pipeline name since pipeline ID is the correct way to
                                            identify a pipeline.</div>
</li>

                                    <li class="li">
                                        <p class="p">pipeline:title() - Returns the pipeline title or
                                            name.</p>

                                    </li>

                                </ul>
</li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_p1z_ggv_1r" title="Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record.">New record functions</a> - You can use the following new
                                    record functions to work with field attributes:<ul class="ul" id="concept_yym_xqt_5y__ul_k4h_kxt_5y">
                                        <li class="li">record:fieldAttribute (&lt;field path&gt;, &lt;attribute
                                            name&gt;) - Returns the value of the specified field
                                            attribute. </li>

                                        <li class="li">record:fieldAttributeOrDefault (&lt;field path&gt;,
                                            &lt;attribute name&gt;, &lt;default value&gt;) - Returns the
                                            value of the specified field attribute. Returns the
                                            default value if the attribute does not exist or
                                            contains no value.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New string functions</a> - You can use the following new
                                    string functions to transform string data: <ul class="ul" id="concept_yym_xqt_5y__ul_knh_mxt_5y">
                                        <li class="li">str:urlEncode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            URL encoded string from a decoded string using the
                                            specified encoding format. </li>

                                        <li class="li">str:urlDecode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            decoded string from a URL encoded string using the
                                            specified encoding format.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">New time functions</a> - You can use the following new
                                    time functions to transform datetime data: <ul class="ul" id="concept_yym_xqt_5y__ul_yk5_pxt_5y">
                                        <li class="li">time:dateTimeToMilliseconds (&lt;Date object&gt;) -
                                            Converts a Date object to an epoch or UNIX time in
                                            milliseconds. </li>

                                        <li class="li">time:extractDateFromString(&lt;string&gt;, &lt;format
                                            string&gt;) - Extracts a Date object from a String, based
                                            on the specified date format. </li>

                                        <li class="li">time:extractStringFromDateTZ (&lt;Date object&gt;,
                                            &lt;timezone&gt;, &lt;format string&gt;) - Extracts a string
                                            value from a Date object based on the specified date
                                            format and time zone.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New and enhanced miscellaneous functions</a> - You can
                                    use the following new and enhanced miscellaneous functions: <ul class="ul" id="concept_yym_xqt_5y__ul_m2x_nxt_5y">
                                        <li class="li">offset:column(&lt;position&gt;) - Returns the value of the
                                            positioned offset column for the current table.
                                            Available only in the additional offset column
                                            conditions of the JDBC Multitable Consumer origin. </li>

                                        <li class="li">every function - You can now use the function with the
                                            hh() datetime variable in directory templates. This
                                            allows you to create directories based on the specified
                                            interval for hours.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title25" id="concept_wbf_dgk_fy">
 <h2 class="title topictitle2" id="ariaid-title25">What's New in 2.2.1.0</h2>

 <div class="body conbody">
        <div class="p"><span class="ph">Data Collector</span>
            version 2.2.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_y1c_1pm_3y">
                            <li class="li">New <a class="xref" href="../Processors/FieldZip.html#concept_o3b_t1k_yx">Field Zip processor</a> - Merges two List fields or two
                                List-Map fields in the same record.</li>

                            <li class="li">New <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor</a> - Performs lookups in a
                                Salesforce object and passes the lookup values to fields. Use the
                                Salesforce Lookup to enrich records with additional data.</li>

                            <li class="li"><a class="xref" href="../Processors/ValueReplacer.html#concept_ppg_ztk_3y">Value Replacer</a> enhancement - You can now replace field
                                values with nulls using a condition.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_vlq_dpm_3y">
                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_wsz_qj4_zx">Whole file support in the Azure Data Lake Store
                                    destination</a> - You can now use the whole file data format
                                to stream whole files to Azure Data Lake Store. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</article>
<article class="topic concept nested1" aria-labelledby="ariaid-title26" id="concept_oyv_zfk_fy">
 <h2 class="title topictitle2" id="ariaid-title26">What's New in 2.2.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data Collector</span> version
            2.2.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Event Framework</dt>

                    <dd class="dd">The Data Collector event framework enables the pipeline to trigger tasks in
                        external systems based on actions that occur in the pipeline, such as
                        running a MapReduce job after the pipeline writes a file to HDFS. You can
                        also use the event framework to store event information, such as when an
                        origin starts or completes reading a file. </dd>

                    <dd class="dd ddexpand">For details, see the <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Event Framework chapter</a>. </dd>

                    <dd class="dd ddexpand">The event framework includes the following new features and enhancements:<ul class="ul" id="concept_oyv_zfk_fy__ul_ojf_xgk_fy">
                            <li class="li"><a class="xref" href="../Executors/Executors-overview.html#concept_stt_2lk_fx">New executor stages</a>. A new type of stage that performs
                                tasks in external systems upon receiving an event. This release
                                includes the following executors:<ul class="ul" id="concept_oyv_zfk_fy__ul_bn4_ygk_fy">
                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File Metadata executor</a> - Changes file
                                        metadata such as the name, location, permissions, and ACLs. </li>

                                    <li class="li"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive Query executor</a> - Runs a Hive or Impala
                                        query. </li>

                                    <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor</a> - Runs a SQL query. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce executor</a> - Runs a custom MapReduce job
                                        or an Avro to Parquet MapReduce job. </li>

                                </ul>
</li>

                            <li class="li">Event generation. The following stages now generate events that you
                                can use in a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_sxd_bhk_fy">
                                    <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory</a> and <a class="xref" href="../Origins/FileTail.html#concept_gwn_c32_px">File Tail</a> origins - Generate events when they
                                        start and complete reading a file.</li>

                                    <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#concept_aqq_tt2_px">Amazon S3 destination</a> - Generates events when it
                                        completes writing to an object or streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_bvb_rxj_px">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_in1_fcm_px">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_bqd_3qb_rx">MapR FS</a> destinations - Generate events when they
                                        close an output file or complete streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors - Can run scripts
                                        that generate events. </li>

                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_vhl_mfj_rx">HDFS File Metadata executor</a> - Generates events
                                        when it changes file metadata. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_e1s_sm5_sx">MapReduce executor</a> - Generates events when it
                                        starts a MapReduce job.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev stages</a>. You can use the following stages to develop
                                and test event handling: <ul class="ul" id="concept_oyv_zfk_fy__ul_ysc_2hk_fy">
                                    <li class="li">Dev Data Generator enhancement - You can now configure the
                                        Dev Data Generator to generate event records as well as data
                                        records. You can also specify the number of records in a
                                        batch. </li>

                                    <li class="li">To Event - Generates event records using the incoming record
                                        as the body of the event record.</li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_q3x_pvm_3y">
                            <li class="li"><a class="xref" href="../Installation/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Installation requirements</a>:<ul class="ul" id="concept_oyv_zfk_fy__ul_zh5_qvm_3y">
                                    <li class="li">Java requirement - Oracle Java 7 is supported but now
                                        deprecated. Oracle announced the end of public updates for
                                        Java 7 in April 2015. StreamSets recommends migrating to
                                        Java 8, as Java 7 support will be removed in a future Data
                                        Collector release. </li>

                                    <li class="li">File descriptors requirement - Data Collector now requires a
                                        minimum of 32,768 open file descriptors. </li>

                                </ul>
</li>

                        </ul>

                        <ul class="ul" id="concept_oyv_zfk_fy__ul_jtf_ghk_fy">
                            <li class="li"><a class="xref" href="../Installation/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space.">Core installation</a> includes the basic stage library only
                                - The core RPM and tarball installations now include the basic stage
                                library only, to allow Data Collector to use less disk space.
                                Install additional stage libraries using the Package Manager for
                                tarball installations or the command line for RPM and tarball
                                installations. <p class="p">Previously, the core installation also included
                                    the Groovy, Jython, and statistics stage libraries.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5w_mhk_fy">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a>. Data Collector now supports the
                                following stage libraries: <ul class="ul" id="concept_oyv_zfk_fy__ul_oxv_nhk_fy">
                                    <li class="li">Apache Kudu version 1.0.x - Earlier Kudu versions are no
                                        longer supported. </li>

                                    <li class="li">Cloudera CDH version 5.9 distribution of Apache Hadoop. </li>

                                    <li class="li">Cloudera version 5.9 distribution of Apache Kafka 2.0. </li>

                                    <li class="li">Elasticsearch version 5.0.x. </li>

                                    <li class="li">Google Cloud Bigtable. </li>

                                    <li class="li">Hortonworks HDP version 2.5 distribution of Apache Hadoop. </li>

                                    <li class="li">MySQL Binary Log. </li>

                                    <li class="li">Salesforce. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/Authentication.html#concept_x2j_5ts_g5" title="If your organization uses LDAP, configure Data Collector to use LDAP authentication. After you configure LDAP authentication, users log in to Data Collector using their LDAP username and password.">LDAP authentication</a> - If you use LDAP authentication,
                                you can now configure Data Collector to connect to multiple LDAP
                                servers. You can also configure Data Collector to support an LDAP
                                deployment where members are defined by uid or by full DN. </li>

                            <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector</a> - Data Collector now uses the
                                Concurrent Mark Sweep (CMS) garbage collector by default. You can
                                configure Data Collector to use a different garbage collector by
                                modifying Java configuration options in the Data Collector
                                environment configuration file.</li>

                            <li class="li">Environment variables for <a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_vrx_4fg_qr">Java configuration options</a>. Data Collector now uses
                                three environment variables to define Java configuration options:
                                    <ul class="ul" id="concept_oyv_zfk_fy__ul_wym_thk_fy">
                                    <li class="li">SDC_JAVA_OPTS - Includes configuration options for all Java
                                        versions. SDC_JAVA7_OPTS - Includes configuration options
                                        used only when Data Collector is running Java 7.</li>

                                    <li class="li">SDC_JAVA8_OPTS - Includes configuration options used only
                                        when Data Collector is running Java 8. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">New time zone property</a> - You can configure the Data
                                Collector UI to use UTC, the browser time zone, or the Data
                                Collector time zone. The time zone property affects how dates and
                                times display in the UI. The default is the browser time zone.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_kq1_whk_fy">
                            <li class="li">New <a class="xref" href="../Origins/MySQLBinaryLog.html#concept_kqg_1yh_xx">MySQL Binary Log origin</a> - Reads MySQL binary logs to
                                generate records with change data capture information. </li>

                            <li class="li">New <a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx">Salesforce origin </a>- Reads data from Salesforce. The
                                origin can execute a SOQL query to read existing data from
                                Salesforce. The origin can also subscribe to the Force.com Streaming
                                API to receive notifications for changes to Salesforce data. </li>

                            <li class="li"><a class="xref" href="../Origins/Directory.html#concept_qpt_rg3_cy" title="When using the Last Modified Timestamp read order, the Directory origin can read files in subdirectories of the specified file directory.">Directory origin</a> enhancement - You can configure the
                                Directory origin to read files from all subdirectories when using
                                the last-modified timestamp for the read order. </li>

                            <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Query Consumer</a> and <a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client</a> origin enhancement - You can now
                                configure the transaction isolation level that the JDBC Query
                                Consumer and Oracle CDC Client origins use to connect to the
                                database. Previously, the origins used the default transaction
                                isolation level configured for the database.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_wfg_13k_fy">
                            <li class="li">New <a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx" title="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop.">Spark
                                    Evaluator processor</a> - Processes data based on a Spark
                                application that you develop. Use the Spark Evaluator processor to
                                develop a Spark application that performs custom processing within a
                                pipeline. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldFlattener.html#concept_vpx_zc1_xx">Field Flattener processor</a> enhancements - In addition to
                                flattening the entire record, you can also now use the Field
                                Flattener processor to flatten specific list or map fields in the
                                record. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#concept_sym_c4g_xx" title="You can use the Field Type Converter to change the scale of decimal fields. For example, you might have a decimal field with the value 12345.6789115, and you'd like to decrease the scale to 4 so that the value is 12345.6789.">Field Type Converter processor</a> enhancements - You can
                                now use the Field Type Converter processor to change the scale of a
                                decimal field. Or, if you convert a field with another data type to
                                the Decimal data type, you can configure the scale to use in the
                                conversion. </li>

                            <li class="li"><a class="xref" href="../Processors/ListPivoter.html#concept_ekg_313_qw">Field
                                    Pivoter processor</a> enhancements - The List Pivoter
                                processor has been renamed to the Field Pivoter processor. You can
                                now use the processor to pivot data in a list, map, or list-map
                                field. You can also use the processor to save the field name of the
                                first-level item in the pivoted field. </li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> and <a class="xref" href="../Processors/JDBCTee.html#task_qpj_ncy_hw">JDBC Tee</a> processor enhancement - You can now configure
                                the transaction isolation level that the JDBC Lookup and JDBC Tee
                                processors use to connect to the database. Previously, the origins
                                used the default transaction isolation level configured for the
                                database. </li>

                            <li class="li">Scripting processor enhancements - The <a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors can generate event records
                                and work with record header attributes. The sample scripts now
                                include examples of both and a new tip for generating unique record
                                IDs. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLFlattener.html#task_pmb_l55_sv" title="Configure an XML Flattener to flatten XML data embedded in a string field.">XML Flattener processor</a> enhancement - You can now
                                configure the XML Flattener processor to write the flattened data to
                                a new output field. Previously, the processor wrote the flattened
                                data to the same field. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5">XML
                                    Parser processor</a> enhancement. You can now generate
                                records from XML documents using simplified XPath expressions. This
                                enables reading records from deeper within XML documents. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5j_l3k_fy">
                            <li class="li">New <a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> - Writes data to
                                Microsoft Azure Data Lake Store. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Bigtable.html#concept_pl5_tmq_tx">Google Bigtable destination</a> - Writes data to Google
                                Cloud Bigtable. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">Salesforce destination</a> - Writes data to Salesforce. New
                                Wave Analytics destination. Writes data to Salesforce Wave
                                Analytics. The destination creates a dataset with external data. </li>

                            <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">Amazon S3 destination</a> change - The AWS KMS Key ID
                                property has been renamed AWS KMS Key ARN. Data Collector upgrades
                                existing pipelines seamlessly. </li>

                            <li class="li">File suffix enhancement. You can now configure a file suffix, such
                                as txt or json, for output files generated by <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_zvc_bv5_1r">Local
                                    FS</a>, <a class="xref" href="../Destinations/MapRFS.html#concept_spv_xlc_fv" title="The MapR FS destination writes files to MapR FS. You can write the data to MapR as flat files or Hadoop sequence files.">MapR
                                    FS</a>, and the <a class="xref" href="../Destinations/AmazonS3.html#concept_avx_bnq_rt" title="The Amazon S3 destination writes data to Amazon S3. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon Kinesis Streams, use the Kinesis Producer destination.">Amazon
                                    S3</a> destinations. </li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht" title="The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log.">JDBC Producer</a> destination enhancement - You can now
                                configure the transaction isolation level that the JDBC Producer
                                destination uses to connect to the database. Previously, the
                                destination used the default transaction isolation level configured
                                for the database. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#concept_dvg_vvj_wx">Kudu destination</a> enhancement - You can now configure the
                                destination to perform one of the following write operations:
                                insert, update, delete, or upsert.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_r3d_dkk_fy">
                            <li class="li"><a class="xref" href="../Data_Formats/XMLDFormat.html#concept_lty_42b_dy">XML processing</a> enhancement - You can now generate
                                records from XML documents using <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_zw2_mfk_dy">simplified XPath expressions</a> with origins that process
                                XML data and the XML Parser processor. This enables reading records
                                from deeper within XML documents. </li>

                            <li class="li">Consolidated data format properties - You now configure the data
                                format and related properties on a new Data Format tab. Previously,
                                data formats had individual configuration tabs, e.g., Avro,
                                Delimited, Log. <p class="p">Related properties, such as Charset, Compression
                                    Format, and Ignore Control Characters now appear on the Data
                                    Format tab as well. </p>
</li>

                            <li class="li"><a class="xref" href="../Data_Formats/WholeFile.html#concept_ojv_sr4_vx">Checksum generation for whole files</a> - Destinations that
                                stream whole files can now generate checksums for the files so you
                                can confirm the accurate transmission of the file.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Maintenance</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_h24_3kk_fy">
                            <li class="li">Add labels to pipelines from the Home page - You can now add labels
                                to multiple pipelines from the Data Collector Home page. Use labels
                                to group similar pipelines. For example, you might want to group
                                pipelines by database schema or by the test or production
                                environment. </li>

                            <li class="li">Reset the origin for multiple pipelines from the Home page - You can
                                now reset the origin for multiple pipelines at the same time from
                                the Data Collector Home page.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Rules and Alerts</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_sfk_lkk_fy">
                            <li class="li"><a class="xref" href="../Alerts/RulesAlerts_title.html#concept_ky2_g4f_qv">Metric
                                    rules and alerts</a> enhancements - The gauge metric type can
                                now provide alerts based on the number of input, output, or error
                                records for the last processed batch.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language Functions</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_bwr_nkk_fy">
                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_kxj_nyl_5x">file functions </a>- You can use the following new file
                                functions to work with file paths:<ul class="ul" id="concept_oyv_zfk_fy__ul_xdq_4kk_fy">
                                    <li class="li">file:fileExtension(&lt;filepath&gt;) - Returns the file
                                        extension from a path. </li>

                                    <li class="li">file:fileName(&lt;filepath&gt;) - Returns a file name from a
                                        path. </li>

                                    <li class="li">file:parentPath(&lt;filepath&gt;) - Returns the parent path of
                                        the specified file or directory. </li>

                                    <li class="li">file:pathElement(&lt;filepath&gt;, &lt;integer&gt;) - Returns the
                                        portion of the file path specified by a positive or negative
                                        integer. </li>

                                    <li class="li">file:removeExtension(&lt;filepath&gt;) - Removes the file
                                        extension from a path. </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">pipeline functions</a> - You can use the following new
                                pipeline functions to determine information about a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_lc5_rkk_fy">
                                    <li class="li">pipeline:name() - Returns the pipeline name. </li>

                                    <li class="li">pipeline:version() - Returns the pipeline version when the
                                        pipeline has been published to Dataflow Performance Manager
                                        (DPM). </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">time functions</a> - You can use the following new time
                                functions to transform datetime data:<ul class="ul" id="concept_oyv_zfk_fy__ul_znz_skk_fy">
                                    <li class="li"> time:extractLongFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a long value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:extractStringFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a string value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:millisecondsToDateTime(&lt;long&gt;) - Converts an epoch
                                        or UNIX time in milliseconds to a Date object. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</article>
</article>
</article></main></div>
                        
                        
                        
                    </div>
                    
                </div>
            </div>
        </div> <nav class="navbar navbar-default wh_footer">
  <div class=" footer-container text-center ">
    <!-- Copyright 2018 StreamSets Inc. --><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script>
  </div>
</nav>

        
        <div id="go2top">
            <span class="glyphicon glyphicon-chevron-up"></span>
        </div>
        
        <!-- The modal container for images -->
        <div id="modal_img_large" class="modal">
            <span class="close glyphicon glyphicon-remove"></span>
            <!-- Modal Content (The Image) -->
            <img class="modal-content" id="modal-img" />
            <!-- Modal Caption (Image Text) -->
            <div id="caption"></div>
        </div>
        
        <script src="../../../oxygen-webhelp/lib/bootstrap/js/bootstrap.min.js" type="text/javascript"></script>
        © Apache License, Version 2.0.
    </body>
</html>