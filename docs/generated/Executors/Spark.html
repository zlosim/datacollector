
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="description" content="The Spark executor starts a Spark application each time it receives an event. You can use the Spark executor with Spark on YARN or Spark on Databricks. The executor is not compatible with Spark on ..." /><meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Spark Executor" /><meta name="DC.Relation" scheme="URI" content="../Executors/Executors-title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_cvy_vxb_1z" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Spark Executor</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../Executors/Executors-title.html" title="Executors">Executors</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../Executors/Executors-title.html" title="Executors"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Executors</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_cvy_vxb_1z">
 <h1 class="title topictitle1">Spark Executor</h1>

 <div class="body conbody">
        <p class="p">The
            Spark executor starts a Spark application each time it receives an event. You can use
            the Spark executor with Spark on YARN or Spark on Databricks. The executor is not
            compatible with Spark on Mesos at this time. </p>

        <p class="p">Use the Spark executor to start a Spark application as part of an event stream. <span class="ph">You can use the executor in any logical way, such as running Spark
                        applications after the Hadoop FS, MapR FS, or Amazon S3 destination closes
                        files.</span> For
            example, you might use the executor to start a Spark application that converts Avro
            files to Parquet each time the Hadoop FS destination closes a file. </p>

        <p class="p">Note that the Spark executor starts an application in an external system. It does not
            monitor the application or wait for it to complete. The executor becomes available for
            additional processing as soon as it successfully submits an application. </p>

        <p class="p">When you configure the Spark executor, you specify the cluster manager used with Spark:
            YARN or Databricks. The cluster manager that you select determines the additional
            cluster manager properties, application details, and security options that you can
            configure. </p>

        <p class="p">Regardless of the cluster manager type, you can configure the executor to generate events
            for another event stream. <span class="ph">For more information about dataflow
                        triggers and the event framework, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Dataflow Triggers Overview</a>.</span></p>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_vbm_ywb_c1b">
 <h2 class="title topictitle2">Spark Versions and Stage Libraries</h2>

 <div class="body conbody">
  <p class="p">The Spark executor supports <span class="ph">Spark versions 1.3 through 2.1</span>. </p>

        <p class="p">When you use the Spark executor, make sure the Spark version is the same across all
            related components, as follows:</p>

        <ul class="ul" id="concept_vbm_ywb_c1b__ul_ebs_qxb_c1b">
            <li class="li">When using the executor to run an application on Spark on YARN, make sure <span class="ph">the Spark version used in the selected stage
                        library matches the Spark version used to build the application.</span>
                <p class="p">For example, if you use Spark 2.1 to build the application, use a Spark executor
                    provided in one of the Spark 2.1 stage libraries. </p>
<div class="p">
                    <div class="note note"><span class="notetitle">Note:</span> This requirement does not apply to applications built by Spark on
                        Databricks.</div>

                </div>
</li>

            <li class="li"><p class="p">When using the executor <span class="ph">in a cluster streaming pipeline, the Spark version in
                        the selected stage library must also match the Spark version used by the
                        cluster.</span></p>
<span class="ph">For example, if your cluster uses Spark 1.6, use a
                        stage library that includes Spark 1.6.</span></li>

        </ul>

        <p class="p">The Spark executor is <span class="ph">available in several CDH and MapR stage libraries. To verify the
                        Spark version that a stage library includes, see the CDH or MapR
                        documentation. For more information about the stage libraries that include
                        the Spark Evaluator, see <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Available Stage Libraries</a>.</span></p>

 </div>

</div>
<div class="topic concept nested1" id="concept_ckt_vcv_jz">
 <h2 class="title topictitle2">Spark on YARN</h2>

 <div class="body conbody">
  <p class="p">When using the
            Spark executor with YARN, the executor can run the application in client or cluster
            mode. Run the application in client mode only when resource use is not a concern.</p>

        <p class="p">When you configure the Spark executor, you can specify the number of worker nodes Spark
            should use, or you can enable dynamic allocation and specify the minimum and maximum
            number of worker nodes. Dynamic allocation allows Spark to use additional worker nodes as
            needed, within the specified range.</p>

        <p class="p">You can specify additional cluster manager properties to pass to Spark, such as the
            maximum amount of memory that the application driver and executor can use.</p>

        <p class="p">You can also configure additional Spark arguments and environment variables. Any
            arguments and variables that you enter override any previous definitions, including
            those in the Spark application, elsewhere in the Spark executor, and the <span class="ph">Data
                  Collector</span>
            machine.</p>

        <p class="p">You can specify custom Spark and Java home directories, and a Hadoop proxy user. You can
            also enter Kerberos credentials if needed.</p>

        <p class="p">When you configure the application details, you specify the language used to write the
            application and then define language-specific properties. </p>

        <p class="p">Before you use the Spark executor, make sure to perform the prerequisite task.</p>

 </div>

<div class="topic concept nested2" id="concept_pyb_gsg_gz">
 <h3 class="title topictitle3">YARN Prerequisite</h3>

    <div class="body conbody">
        <p class="p">Before you run a Spark executor pipeline that
            starts applications on YARN, you must enable the Spark executor to submit an
            application. </p>

        <div class="p">You can enable the Spark executor to submit an application in several different ways.
            Perform <u class="ph u">one</u> of the following tasks to enable the executor to submit applications: <dl class="dl">
                
                              <dt class="dt dlterm">Configure the YARN Minimum User ID property, min.user.id</dt>

                              <dd class="dd">The min.user.id property is set to 1000 by default. To allow job
                                          submission:<ol class="ol" id="concept_pyb_gsg_gz__d33389e4834">
                                          <li class="li">Verify the user ID being used by the <span class="ph">Data
                  Collector</span> user, typically named "sdc".</li>

                                          <li class="li">In Hadoop, configure the YARN min.user.id property.
                                                  <p class="p"> Set the property to equal to or lower than the
                                                  <span class="ph">Data
                  Collector</span> user ID.</p>
</li>

                                    </ol>
</dd>

                        
                
                              <dt class="dt dlterm">Configure the YARN Allowed System Users property,
                                    allowed.system.users</dt>

                              <dd class="dd">The allowed.system.users property lists allowed user names. To
                                    allow job submission: <ol class="ol" id="concept_pyb_gsg_gz__d33389e4857">
                                          <li class="li"> In Hadoop, configure the YARN allowed.system.users
                                                property. <p class="p">Add the <span class="ph">Data
                  Collector</span> user name, typically "sdc", to the list of
                                                  allowed users.</p>
</li>

                                    </ol>
</dd>

                        
                
                    <dt class="dt dlterm">Configure the Spark executor Proxy User property</dt>

                    <dd class="dd">In the Spark executor, the Proxy User property allows you to enter a user
                        name for the stage to use when submitting applications. To allow application
                            submission:<ol class="ol" id="concept_pyb_gsg_gz__ol_f3h_f2w_cy">
                            <li class="li">In the Spark executor stage, on the <span class="keyword wintitle">Spark</span> tab,
                                configure the <span class="ph uicontrol">Proxy User</span> property. <p class="p">Enter a
                                    user with an ID that is higher than the min.user.id property, or
                                    with a user name that is listed in the allowed.system.users
                                    property. </p>
</li>

                        </ol>
</dd>

                    <dd class="dd">For information about using a Hadoop User, see <a class="xref" href="Spark.html#concept_twh_wsg_gz">Using a Proxy Hadoop User</a>.</dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested2" id="concept_aht_p4h_c1b">
 <h3 class="title topictitle3">Spark Home Requirement</h3>

 <div class="body conbody">
  <p class="p">When running
            an application on YARN, the Spark executor requires access to the spark-submit script
            located in the Spark installation directory. </p>

        <p class="p">By default, the Spark executor uses the directory defined in the SPARK_HOME environment
            variable on the <span class="ph">Data
                  Collector</span>
            machine. The SPARK_HOME environment variable must be set before you start <span class="ph">Data
                  Collector</span>. </p>

        <p class="p">You can override the environment variable as needed by configuring the Custom Spark Home
            property in the executor stage properties. Use the Custom Spark Home property when the
            SPARK_HOME environment variable is not set, or when it points to a conflicting version
            of Spark. </p>

        <p class="p">For example, if you are using a Spark 2.1 stage library for the Spark executor and
            SPARK_HOME points to a Spark 1.6 installation, use the Custom Spark Home property to
            specify the location of the Spark 2.1 spark-submit script. </p>

 </div>

</div>
<div class="topic concept nested2" id="concept_lkk_rwx_jz">
 <h3 class="title topictitle3">Application Properties</h3>

 <div class="body conbody">
  <p class="p">When using the Spark executor with
            YARN, you specify an application name. The application name displays in the cluster
            manager and Spark server logs, so use a distinctive name to enable distinguishing the
            Spark application from others. For example, <span class="ph">SDC_&lt;pipeline name&gt;_&lt;app_type&gt;</span>. </p>

        <p class="p">In the executor, you can enable verbose logging to help test the pipeline and debug the
            application. </p>

        <div class="p">Configure additional application details based on the language used to write the application:<dl class="dl">
                
                    <dt class="dt dlterm">Java or Scala</dt>

                    <dd class="dd">For applications written in Java or Scala, you specify the main class and
                        application resource - the full path to the primary JAR or file.</dd>

                    <dd class="dd">You can specify additional arguments and JARs to use. You can also pass
                        additional files to the application using the <samp class="ph codeph">--files</samp>
                        protocol. </dd>

                
                
                    <dt class="dt dlterm">Python</dt>

                    <dd class="dd">For applications written in Python, you specify the application resource -
                        the full path to the primary Python file - and any required dependencies.
                        You can define application arguments and pass additional files to the
                        application using the <samp class="ph codeph">--files</samp> protocol. </dd>

                
            </dl>
</div>

        <div class="p">
            <div class="note note"><span class="notetitle">Note:</span> Make sure the user that runs <span class="ph">Data
                  Collector</span> - or the Hadoop proxy user, if configured - has read permission on all required
                paths.</div>

        </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_twh_wsg_gz">
 <h3 class="title topictitle3">Using a Proxy Hadoop User</h3>

    <div class="body conbody">
        <p class="p">You can configure the Spark executor to use a Hadoop user as
            a proxy user to submit applications to Spark on YARN. </p>

        <p class="p">By default, the <span class="ph">Data
                  Collector</span>
            uses the user account who started it to connect to external systems. When using
            Kerberos, the <span class="ph">Data
                  Collector</span>
            can use the Kerberos principal specified in the executor.</p>

        <div class="p">To use a Hadoop user, perform the following tasks:<ol class="ol" id="concept_twh_wsg_gz__ol_vmj_jtg_gz">
                <li class="li">On the external system, configure the <span class="ph">Data
                  Collector</span> user as a proxy user and authorize the <span class="ph">Data
                  Collector</span> user to impersonate the Hadoop user. <p class="p">For more information, see the Hadoop
                        documentation. </p>
</li>

                <li class="li">In the Spark executor, on the <span class="ph uicontrol">Spark</span> tab, configure the
                        <span class="ph uicontrol">Proxy User</span> property to use the Hadoop user name.</li>

            </ol>
</div>

    </div>

</div>
<div class="topic concept nested2" id="concept_bhk_fhg_gz">
 <h3 class="title topictitle3">Kerberos Authentication</h3>

    <div class="body conbody">
        <p class="p">You can use Kerberos authentication to
            connect to the destination system where output files are written. To enable this, on the
            Credentials tab of the Spark executor, enter the Kerberos principal and keytab for the
            YARN cluster where the application runs. </p>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_fdc_qrx_jz">
 <h2 class="title topictitle2">Spark on Databricks</h2>

 <div class="body conbody">
  <p class="p">When using
            the Spark executor with Databricks, you can run jobs based on notebooks or JARs.</p>

        <p class="p">Before you use the Spark executor, perform the necessary prerequisites. </p>

        <div class="p">When you configure the executor, you specify the cluster base URL, job type, job ID, and
            user credentials. You can optionally configure job parameters and you can use the
            expression language in job parameters. For example, when performing post-processing on
            an Amazon S3 object, you could use the following expression to retrieve the object key
            name from the event record:
            <pre class="pre codeblock"><samp class="ph codeph">${record:field('/objectKey')}</samp></pre>
</div>

        <p class="p">You also can configure an HTTP proxy and SSL/TLS details.</p>

 </div>

<div class="topic concept nested2" id="concept_esz_x3d_kz">
    <h3 class="title topictitle3">Databricks Prerequisites</h3>

    <div class="body conbody">
        <div class="p">Before you run a
            Spark executor pipeline that starts jobs on Databricks, perform the following tasks:<ol class="ol" id="concept_esz_x3d_kz__ol_rxc_rld_kz">
                <li class="li">Create the job. <p class="p">The Spark executor can start jobs based on notebooks or
                        JARs.</p>
</li>

                <li class="li"> Optionally configure the job to allow concurrent runs.<p class="p">By default, Databricks
                        does not allow running multiple instances of a job at the same time. With
                        the default, if the Spark executor receives multiple events in quick
                        succession, it starts multiple instances of the job but Databricks queues
                        those instances and runs them one by one. </p>
<p class="p">To enable parallel
                        processing, in Databricks, configure the job to allow concurrent runs. You
                        can configure the maximum number of concurrent runs through the Databricks
                        API with the max_concurrent_runs parameter, or through the UI using the Jobs
                        &gt; Advanced menu and the Maximum Concurrent Runs property.</p>
</li>

                <li class="li">Submit the job and note the job ID.<p class="p">When you submit the job, Databricks
                        generates a job ID. Use the job ID when you configure the Spark
                        executor.</p>
</li>

            </ol>
</div>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_xmx_1wg_gz">
 <h2 class="title topictitle2">Event Generation</h2>

    <div class="body conbody">
        <p class="p">The Spark executor can generate events that you
            can use in an event stream. When you enable event generation, the executor generates
            events each time it starts a Spark application. </p>

        <div class="p">Spark executor events can be used in any logical way. For example: <ul class="ul" id="concept_xmx_1wg_gz__ul_dzf_tpg_zx">
                <li class="li">With the Email executor to send a custom email
                              after receiving an event.<p class="p">For an example, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_t2t_lp5_xz">Case Study: Sending Email</a>.</p>
</li>

                <li class="li">With a destination to store event information.
                                    <p class="p">For an example, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_ocb_nnl_px">Case Study: Event Storage</a>.</p>
</li>

            </ul>
</div>

        <p class="p">Since Spark executor events include the application ID for each application that it
            starts, you might generate events to keep a log of the application IDs.</p>

        <p class="p"><span class="ph">For more information about dataflow
                        triggers and the event framework, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Dataflow Triggers Overview</a>.</span></p>

    </div>

<div class="topic concept nested2" id="concept_qk2_3wg_gz">
 <h3 class="title topictitle3">Event Records</h3>

    <div class="body conbody">
        <div class="p">Event records generated
            by the Spark executor have the following event-related record header attributes. Record
            header attributes are stored as String values:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_qk2_3wg_gz__table_smk_gp5_sx" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="30%" id="d192908e602">Record Header Attribute</th>

                            <th class="entry" valign="top" width="70%" id="d192908e605">Description</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d192908e602 ">sdc.event.type</td>

                            <td class="entry" valign="top" width="70%" headers="d192908e605 ">Event type. Uses one of the following types:<ul class="ul" id="concept_qk2_3wg_gz__ul_tmk_gp5_sx">
                                    <li class="li">AppSubmittedEvent - Generated when the executor starts a
                                        Spark application.</li>

                                </ul>
</td>

                        </tr>

                        <tr>
       <td class="entry" valign="top" width="30%" headers="d192908e602 ">sdc.event.version</td>

       <td class="entry" valign="top" width="70%" headers="d192908e605 ">An integer that indicates the version of the event record type.</td>

      </tr>

                        <tr>
       <td class="entry" valign="top" width="30%" headers="d192908e602 ">sdc.event.creation_timestamp</td>

       <td class="entry" id="concept_qk2_3wg_gz__d33478e2087" valign="top" width="70%" headers="d192908e605 ">Epoch timestamp when the stage created the event.
       </td>

      </tr>

                    </tbody>

                </table>
</div>
</div>

        <div class="p">Event records generated by the Spark executor have the following fields:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_qk2_3wg_gz__table_umk_gp5_sx" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="30%" id="d192908e661">Event Field Name</th>

                            <th class="entry" valign="top" width="70%" id="d192908e664">Description</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="30%" headers="d192908e661 ">APP_ID</td>

                            <td class="entry" valign="top" width="70%" headers="d192908e664 ">Application ID for the Spark application:<ul class="ul" id="concept_qk2_3wg_gz__ul_g53_4wg_gz">
                                    <li class="li">For Databricks, returns the run ID of the job.</li>

                                    <li class="li">For YARN, returns the YARN application ID.</li>

                                </ul>
</td>

                        </tr>

                    </tbody>

                </table>
</div>
</div>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_ibt_1tx_jz">
 <h2 class="title topictitle2">Monitoring</h2>

 <div class="body conbody">
        <p class="p"><span class="ph">Data
                  Collector</span>
            does not monitor Spark applications. Use your regular cluster monitor application to
            view the status of applications. </p>

        <p class="p">Applications started by the Spark executor display using the application name specified
            in the stage. The application name is the same for all instances of the application. You
            can find the application ID for a particular instance in the <span class="ph">Data
                  Collector</span>
            log.</p>

        <p class="p">The Spark executor also writes the application ID to the event record. To keep a record
            of all application IDs, enable event generation for the stage. </p>

    </div>

</div>
<div class="topic task nested1" id="task_cdw_wxb_1z">
    <h2 class="title topictitle2">Configuring a Spark Executor</h2>

    <div class="body taskbody">
        <div class="section context">
            <p class="p">Configure a
                Spark executor to start Spark applications each time the executor receives an event
                record. </p>

        </div>

        <ol class="ol steps"><li class="li step stepexpand">
                <span class="ph cmd">In the Properties panel, on the <span class="keyword wintitle">General</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_yxz_pvs_5x" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e766">General Property</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e769">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
       <td class="entry" valign="top" width="30%" headers="d192908e766 ">Name</td>

       <td class="entry" valign="top" width="70%" headers="d192908e769 ">Stage name.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d192908e766 ">Description</td>

       <td class="entry" valign="top" width="70%" headers="d192908e769 ">Optional description.</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d192908e766 ">Stage Library</td>

       <td class="entry" valign="top" width="70%" headers="d192908e769 ">Library version that you want to use. </td>

      </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e766 ">Produce Events <a class="xref" href="Spark.html#concept_xmx_1wg_gz">
                                            <img class="image" id="task_cdw_wxb_1z__image_qqz_cxs_5x" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e769 ">Generates event records when events occur. Use for event
        handling. <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">
         <img class="image" id="task_cdw_wxb_1z__d33478e1956" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d192908e766 ">Required Fields <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_dnj_bkm_vq">
         <img class="image" id="task_cdw_wxb_1z__d33478e1965" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d192908e769 ">Fields that must include data for the record to be passed into the stage. <div class="note tip"><span class="tiptitle">Tip:</span> You might include fields that the stage uses.</div>
<p class="p">Records
         that do not include all required fields are processed based on the error handling
         configured for the pipeline.</p>
</td>

      </tr>

                                <tr>
       <td class="entry" valign="top" width="30%" headers="d192908e766 ">Preconditions <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_msl_yd4_fs">
         <img class="image" id="task_cdw_wxb_1z__d33478e1982" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

       <td class="entry" valign="top" width="70%" headers="d192908e769 ">Conditions that must evaluate to TRUE to allow a record to enter the stage for
        processing. Click <span class="ph uicontrol">Add</span> to create additional preconditions. <p class="p">Records
         that do not meet all preconditions are processed based on the error handling configured for
         the stage.</p>
</td>

      </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand" id="task_cdw_wxb_1z__step-Sparktab">
                <span class="ph cmd">On the <span class="keyword wintitle">Spark</span> tab, configure the <span class="ph uicontrol">Cluster
                        Manager</span> property.</span>
                <div class="itemgroup info">Then, when using a <span class="ph uicontrol">Databricks</span> cluster manager,
                    configure the following property and continue to the next step.
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_p1f_ytl_gz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e889">Databricks Property</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e892">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e889 ">Cluster Base URL</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e892 ">The Databricks URL for your company. The URL uses the
                                        following format:
                                        <pre class="pre codeblock">https://&lt;your domain&gt;.cloud.databricks.com</pre>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>
</div>
                <div class="itemgroup info">Then, when using a <span class="ph uicontrol">YARN</span> cluster manager, configure the
                    following properties:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_f4s_j5l_gz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e930">YARN Property</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e933">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Deploy Mode</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Deploy mode for the application:<ul class="ul" id="task_cdw_wxb_1z__ul_swj_n5l_gz">
                                            <li class="li">Client - Runs the application in Spark client mode.
                                                Use only when resources are not a concern.</li>

                                            <li class="li">Cluster - Runs the application in Spark cluster
                                                mode. Cluster mode deploys the application on the
                                                YARN cluster.</li>

                                        </ul>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Driver Memory</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Maximum amount of memory the driver can use for the
                                            application.<p class="p">Enter the number and a standard Java unit
                                            of measure without additional spaces. For example, 10m.
                                            </p>
<p class="p">You can use k or K, m or M, or g or G.
                                        </p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Executor Memory</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Maximum amount of memory the executor can use. <p class="p">Enter
                                            the number and a standard Java unit of measure without
                                            additional spaces. For example, 100k. </p>
<p class="p">You can use
                                            k or K, m or M, or g or G. </p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Dynamic Allocation</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Enables the dynamic allocation of executors to start an
                                        applications.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Number of Worker Nodes</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">The exact number of worker nodes for Spark to use.
                                        Configure when not using dynamic allocation.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Minimum Number of Worker Nodes</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">The minimum number of worker nodes for Spark to use.
                                        Configure when using dynamic allocation.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Maximum Number of Worker Nodes</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">The maximum number of worker nodes for Spark to use.
                                        Configure when using dynamic allocation.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Proxy User <a class="xref" href="Spark.html#concept_twh_wsg_gz">
                                            <img class="image" id="task_cdw_wxb_1z__image_xqm_fjm_zx" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Hadoop user to connect to the external system and run the
                                        application. When using this property, make sure the
                                        external system is configured appropriately. <p class="p">By default,
                                            the pipeline uses the Data Collector user. </p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Custom Spark Home <a class="xref" href="Spark.html#concept_aht_p4h_c1b">
                                            <img class="image" id="task_cdw_wxb_1z__image_crp_srh_c1b" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Use to enter a custom Spark home directory. By default,
                                        the origin uses the directory specified in the SPARK_HOME
                                        environment variable on the <span class="ph">Data
                  Collector</span> machine. <p class="p">This property overrides the SPARK_HOME
                                            environment variable.</p>
<p class="p">Required if the environment
                                            variable is not set for the <span class="ph">Data
                  Collector</span> machine or if the variable is set for an incorrect
                                            version of Spark.</p>
<p class="p">For example, to run a job
                                            against Spark 2.1, point this property to the Spark 2.1
                                            directory if the SPARK_HOME environment variable points
                                            to an earlier version of Spark.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Custom Java Home</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Use to enter a custom Java home directory. By default,
                                        the origin uses the directory specified in the JAVA_HOME
                                        environment variable on the <span class="ph">Data
                  Collector</span> machine. <p class="p">This property overrides the <span class="ph">Data
                  Collector</span> environment variable.</p>
<p class="p">Required if the
                                            environment variable is not set for the <span class="ph">Data
                  Collector</span> machine.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Additional Spark Arguments</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Additional arguments to pass to Spark. Overrides any
                                        previous configuration for the specified arguments. For a
                                        list of available arguments, see the Spark documentation.
                                    </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Additional Spark Arguments and Values</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Additional arguments with values to pass to Spark.
                                        Overrides any previous configuration for the specified
                                        arguments. For a list of available arguments, see the Spark
                                        documentation.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e930 ">Environment Variables</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e933 ">Additional environment variables to use. Overrides any
                                        previous configuration for the specified arguments. For a
                                        list of valid environment variables, see the Spark
                                        documentation.</td>

                                </tr>

                            </tbody>

                        </table>
</div>
</div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">When using a Databricks cluster manager, click the
                        <span class="keyword wintitle">Application</span> tab and configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_uvj_jxm_gz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e1136">Databricks Application Details Property</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e1139">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1136 ">Job Type</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1139 ">Job type to run: Notebook or JAR.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1136 ">Job ID <a class="xref" href="Spark.html#concept_esz_x3d_kz">
                                            <img class="image" id="task_cdw_wxb_1z__image_tw4_jn2_kz" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1139 ">Job ID generated by Databricks after the job was
                                        submitted.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1136 ">Parameters</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1139 ">Parameters to pass to the job. Enter the parameters
                                        exactly as expected, and in the expected order. The executor
                                        does not validate the parameters.<div class="p">You can use the
                                            expression language in job parameters. For example, when
                                            performing post-processing on an Amazon S3 object, you
                                            can use the following expression to retrieve the object
                                            key name from the event record:
                                            <pre class="pre codeblock"><samp class="ph codeph">${record:field('/objectKey')}</samp></pre>
</div>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">When using a YARN cluster manager, click the <span class="keyword wintitle">Application
                        Details</span> tab, select the <span class="ph uicontrol">Language</span> used to
                    write the application, and then configure the following properties:</span>
                <div class="itemgroup info">For applications written in Java or Scala, configure the following
                        properties:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_udj_yyx_jz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e1214">Java/Scala Application Properties</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e1217">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1214 ">Application Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1217 ">Name to display in YARN resource manager and logs. Also
                                        displays in Spark server history pages.<div class="p">
                  <div class="note tip"><span class="tiptitle">Tip:</span> Use a name that distinguishes the application from those started
                        by other processes and other pipelines, such as <span class="ph" id="task_cdw_wxb_1z__d33389e4902">SDC_&lt;pipeline name&gt;_&lt;app_type&gt;</span>.</div>

            </div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1214 ">Application Resource</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1217 ">The full path to the JAR that contains the main
                                        class.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1214 ">Main Class</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1217 ">The full path to the main class for the Spark
                                        application. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1214 ">Application Arguments</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1217 ">You can add additional arguments to pass to the
                                        application. <p class="p">Enter the arguments exactly as expected, and
                                            in the expected order. The executor does not validate
                                            the arguments.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1214 ">Additional JARs</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1217 ">You can specify additional JARs to use. Enter the full
                                        path to the JAR.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1214 ">Additional Files</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1217 ">Additional files to pass to the application using the
                                            <samp class="ph codeph">--files</samp> protocol. Enter the full path
                                        to the files. <p class="p">For information about the protocol, see the
                                            Spark documentation.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1214 ">Enable Verbose Logging</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1217 ">Enables logging additional information to the <span class="ph">Data
                  Collector</span> log. <p class="p">To avoid filling the log with unnecessary
                                            information, enable this property only when testing the
                                            pipeline. </p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>
</div>
                <div class="itemgroup info">For applications written in Python, configure the following
                        properties:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_uzz_qr2_kz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e1327">Python Application Properties </th>

                                    <th class="entry" valign="top" width="70%" id="d192908e1330">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1327 ">Application Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1330 ">Name to display in YARN resource manager and logs. Also
                                        displays in Spark server history pages.<div class="p">
                  <div class="note tip"><span class="tiptitle">Tip:</span> Use a name that distinguishes the application from those started
                        by other processes and other pipelines, such as <span class="ph" id="task_cdw_wxb_1z__d33389e4902">SDC_&lt;pipeline name&gt;_&lt;app_type&gt;</span>.</div>

            </div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1327 ">Application Resource</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1330 ">The full path to the Python file to run. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1327 ">Application Arguments</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1330 ">You can add additional arguments to pass to the
                                        application. <p class="p">Enter the arguments exactly as expected, and
                                            in the expected order. The executor does not validate
                                            the arguments.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1327 ">Dependencies</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1330 ">Full path to any files the Python application resource
                                        requires. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1327 ">Additional Files</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1330 ">Additional files to pass to the application using the
                                            <samp class="ph codeph">--files</samp> protocol. Enter the full path
                                        to the files. <p class="p">For information about the protocol, see the
                                            Spark documentation.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1327 ">Enable Verbose Logging</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1330 ">Enables logging additional information to the <span class="ph">Data
                  Collector</span> log. <p class="p">To avoid filling the log with unnecessary
                                            information, enable this property only when testing the
                                            pipeline. </p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>
</div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">Optionally, click the <span class="keyword wintitle">Credentials</span> tab and configure the
                    following properties:</span>
                <div class="itemgroup info">To enter Databricks credentials, configure the following properties:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_e4n_jyj_kz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e1438">Databricks Credentials</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e1441">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1438 ">Username</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1441 ">Databricks user name.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1438 ">Password</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1441 ">Password for the account.</td>

                                </tr>

                            </tbody>

                        </table>
</div>
</div>
                <div class="itemgroup info">To use Kerberos authentication to access a destination system from YARN,
                    configure the following properties:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_hxn_syj_kz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e1483">YARN Kerberos Properties <a class="xref" href="Spark.html#concept_bhk_fhg_gz">
                                            <img class="image" id="task_cdw_wxb_1z__image_hlh_jzj_kz" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></th>

                                    <th class="entry" valign="top" width="70%" id="d192908e1490">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1483 ">Kerberos Principal</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1490 ">Kerberos principal for the YARN cluster where the
                                        application runs. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1483 ">Kerberos Keytab</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1490 ">Kerberos keytab for the YARN cluster where the
                                        application runs. </td>

                                </tr>

                            </tbody>

                        </table>
</div>
</div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">To use SSL/TLS with Databricks, click the <span class="keyword wintitle">SSL</span> tab and
                    configure the following properties:</span>
                <div class="itemgroup info">
                    <div class="note tip"><span class="tiptitle">Tip:</span> To secure sensitive information such as
                  passwords, you can use <a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_bs4_5nm_2s" title="Similar to runtime properties, runtime resources are values that you define in a file local to the Data Collector and call from within a pipeline. But with runtime resources, you can restrict the permissions for the files to secure sensitive information. Use runtime resources to load sensitive information from files at runtime.">runtime resources</a> or <span class="ph"><a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential stores.</a></span></div>

                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_b5p_p1k_kz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e1564">SSL/TLS Property</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e1567">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1564 ">Path to Truststore</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1567 ">Absolute path to the truststore. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1564 ">Password</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1567 ">Truststore password.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1564 ">Path to Keystore</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1567 ">Absolute path to the keystore. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1564 ">Password</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1567 ">Keystore password.</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">To use an HTTP proxy, on the <span class="keyword wintitle">Proxy</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_cdw_wxb_1z__table_vy5_dbk_kz" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d192908e1637">HTTP Proxy Property</th>

                                    <th class="entry" valign="top" width="70%" id="d192908e1640">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1637 ">Proxy URI</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1640 ">Proxy URI.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1637 ">Username</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1640 ">Proxy user name.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d192908e1637 ">Password</td>

                                    <td class="entry" valign="top" width="70%" headers="d192908e1640 ">Proxy password.<div class="note tip"><span class="tiptitle">Tip:</span> To secure sensitive information such as
                  usernames and passwords, you can use <a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_bs4_5nm_2s" title="Similar to runtime properties, runtime resources are values that you define in a file local to the Data Collector and call from within a pipeline. But with runtime resources, you can restrict the permissions for the files to secure sensitive information. Use runtime resources to load sensitive information from files at runtime.">runtime resources</a> or <span class="ph"><a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential stores.</a></span></div>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
</ol>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../Executors/Executors-title.html" title="Executors"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Executors</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"></div>
</body>
</html>