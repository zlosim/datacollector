
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Spark Evaluator" /><meta name="abstract" content="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop." /><meta name="description" content="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop." /><meta name="DC.Relation" scheme="URI" content="../Processors/Processors_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_cpx_1lm_zx" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Spark Evaluator</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../Processors/Processors_title.html" title="Processors">Processors</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../Processors/Processors_title.html" title="Processors"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Processors</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_cpx_1lm_zx">
 <h1 class="title topictitle1">Spark Evaluator</h1>

 
 <div class="body conbody"><p class="shortdesc">The Spark Evaluator performs custom processing within a pipeline based on a Spark
        application that you develop.</p>

        <p class="p">The Spark Evaluator processor is especially
            useful when you need to perform heavy custom processing within a pipeline, such as image
            classification.</p>

        <div class="p">Complete the following general tasks to use the Spark Evaluator:<ol class="ol" id="concept_cpx_1lm_zx__ol_pmh_33p_zx">
                <li class="li">Write the Spark application using Java or Scala. Then, package a JAR file
                    containing the application.</li>

                <li class="li">Install the application and its dependencies.</li>

                <li class="li">Configure the Spark Evaluator processor to submit the Spark application.<p class="p">When
                        you configure the processor, you define the name of the custom Spark class
                        that you developed and define the arguments to pass to the
                            <samp class="ph codeph">init</samp> method in the custom Spark class.</p>
</li>

            </ol>
<div class="note note"><span class="notetitle">Note:</span> The stage libraries that include the Spark Evaluator include all dependencies
                required for the processor. The Spark Evaluator does not require any installation
                prerequisites.</div>
</div>

    </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_nbj_1jb_c1b">
 <h2 class="title topictitle2">Spark Versions and Stage Libraries</h2>

 <div class="body conbody">
  <p class="p">The Spark Evaluator processor supports <span class="ph">Spark versions 1.3 through 2.1</span>. </p>

        <div class="p">When you use the Spark Evaluator processor, make sure the Spark version is the same
            across all related components, as follows:<ul class="ul" id="concept_nbj_1jb_c1b__ul_yct_nqb_c1b">
                <li class="li">
                    <p class="p">Make sure <span class="ph">the Spark version used in the selected stage
                        library matches the Spark version used to build the application.</span></p>

                    <p class="p">For example, if you use Spark 2.1 to build the application, use a Spark
                        Evaluator provided in one of the Spark 2.1 stage libraries. </p>

                </li>

                <li class="li"><p class="p">When using the processor <span class="ph">in a cluster streaming pipeline, the Spark version in
                        the selected stage library must also match the Spark version used by the
                        cluster.</span></p>
<span class="ph">For example, if your cluster uses Spark 1.6, use a
                        stage library that includes Spark 1.6.</span></li>

            </ul>
</div>

        <p class="p">The Spark Evaluator is <span class="ph">available in several CDH and MapR stage libraries. To verify the
                        Spark version that a stage library includes, see the CDH or MapR
                        documentation. For more information about the stage libraries that include
                        the Spark Evaluator, see <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Available Stage Libraries</a>.</span></p>

 </div>

</div>
<div class="topic concept nested1" id="concept_hpm_tns_hz">
 <h2 class="title topictitle2">Standalone Pipelines</h2>

 <div class="body conbody">
        <p class="p">When used in a standalone pipeline, the Spark
            Evaluator processor starts a local Spark application. It passes a batch of data to the
            Spark application as a Resilient Distributed Dataset (RDD). The Spark application - or
                <samp class="ph codeph">SparkContext</samp> - runs as long as the pipeline runs. The Spark
            application submits jobs to the StreamSets Spark Transformer API, processing the data
            and then returning the results and errors back to the pipeline for further processing. </p>

        <p class="p">When you use the Spark Evaluator in a standalone pipeline, define a parallelism value for
            the Spark Evaluator. The Spark application creates this number of partitions for each
            batch of records. The Spark Transformer processes each partition in parallel, and then
            returns the results and errors back to the pipeline.</p>

        <div class="note note"><span class="notetitle">Note:</span> When you write your custom Spark application for use in a standalone pipeline, do not
            use the <samp class="ph codeph">RDD.checkpoint()</samp> method. Checkpointing RDDs when the Spark
            Evaluator is in a standalone pipeline can cause the pipeline to fail.</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_b1l_5lv_jz">
 <h2 class="title topictitle2">Cluster Pipelines</h2>

    
 <div class="body conbody"><p class="shortdesc">You can use the Spark Evaluator processor in cluster pipelines that process data from
        a Kafka or MapR cluster in cluster streaming mode. You cannot use the processor in cluster
        pipelines that process data from a MapR cluster in cluster batch mode or from HDFS. </p>

        <p class="p">In a cluster pipeline, the cluster manager spawns a <span class="ph">Data
                  Collector</span>
            worker for each topic partition in the Kafka or MapR cluster. Spark Streaming generates
            a batch for the pipeline every few seconds. Then each <span class="ph">Data
                  Collector</span>
            worker processes the batches from a single partition.</p>

        <p class="p">Resilient Distributed Datasets (RDDs) are generated from data received across all of the
                <span class="ph">Data
                  Collector</span> workers. The RDDs are then passed to the Spark Transformer running on the driver.</p>

        <p class="p">The RDD passed to the Spark Transformer points to data across the cluster. The Spark
            Transformer processes all of the data received across all <span class="ph">Data
                  Collector</span>
            workers in the cluster, processing multiple batches at the same time. Data moves from
            one node to another only if your custom Spark code shuffles the data. The Spark
            Transformer returns the results and errors back to each Spark Evaluator processor
            running on the <span class="ph">Data
                  Collector</span>
            workers. The <span class="ph">Data
                  Collector</span>
            workers process the remaining pipeline stages in parallel.</p>

        <p class="p">The following image shows how the Spark Transformer processes data received across all
            workers in the cluster:</p>

        <p class="p"><img class="image" id="concept_b1l_5lv_jz__image_swx_xwv_jz" src="../Graphics/SparkEvaluatorCluster.png" height="204" width="826" /></p>

        <div class="p">Use the following rules and guidelines when you write the Spark application and configure
            the processor for a cluster pipeline:<ul class="ul" id="concept_b1l_5lv_jz__ul_dg5_nmv_jz">
                <li class="li">Ensure that the Spark version you use to build the application matches the Spark
                    version used by the cluster and the Spark Evaluator stage library.</li>

                <li class="li">To maintain the states of the RDDs that are used more than once in the
                    application, use the <samp class="ph codeph">RDD.checkpoint()</samp> or
                        <samp class="ph codeph">RDD.cache()</samp> method in your custom Spark application. When
                    you checkpoint or cache the RDDs, the Spark Transformer can access the RDDs from
                    previous batches.<p class="p">The Spark Evaluator processor cleans up checkpoints when the
                        pipeline restarts or when the Spark application restarts. To maintain the
                        state of each RDD between pipeline runs or between restarts of the Spark
                        application, you must maintain the RDDs in an external store such as HDFS or
                        HBase.</p>
</li>

                <li class="li">When you configure the processor, do not define a parallelism value for the
                    processor. The number of partitions defined in the Kafka Consumer origin
                    determines the number of partitions used throughout the cluster pipeline.</li>

                <li class="li">When you configure the processor, use constant values for <samp class="ph codeph">init</samp>
                    method arguments. In a cluster pipeline, the Spark Evaluator processor cannot
                    evaluate <span class="ph">Data
                  Collector</span> expressions in <samp class="ph codeph">init</samp> method arguments.</li>

                <li class="li">When you monitor the running pipeline, the record throughput for
                    intervals less than 15 minutes might show an inaccurate count. To accurately
                    monitor the throughput rate, evaluate the throughput for 15 minutes or
                    more.</li>

            </ul>
</div>

    </div>

    <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br />
<div class="related_link"><a class="navheader_parent_path" href="../Cluster_Mode/ClusterPipelines_title.html#concept_hmh_kfn_1s" title="A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode.">Cluster Pipeline Overview</a></div>
</div>
</div>
</div>
<div class="topic concept nested1" id="concept_lfl_dvd_1y">
 <h2 class="title topictitle2">Developing the Spark Application</h2>

 <div class="body conbody">
        <p class="p">To develop a custom Spark
            application, you write the Spark application and then package a JAR file containing the
            application.</p>

        <p class="p">Use Java or Scala to write a custom Spark class that implements the StreamSets Spark
            Transformer API: <a class="xref" href="https://github.com/streamsets/datacollector-plugin-api/tree/master/streamsets-datacollector-spark-api" target="_blank">https://github.com/streamsets/datacollector-plugin-api/tree/master/streamsets-datacollector-spark-api</a>.</p>

        <p class="p">Include the following methods in the custom class:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">init</dt>

                <dd class="dd">Optional. The <samp class="ph codeph">init</samp> method is called once when the pipeline
                    starts to read arguments that you configure in the Spark Evaluator processor.
                    Use the <samp class="ph codeph">init</samp> method to make connections to external systems or
                    to read configuration details or existing data from external systems.</dd>

            
            
                <dt class="dt dlterm">transform</dt>

                <dd class="dd">Required. The <samp class="ph codeph">transform</samp> method is called for each batch of
                    records that the pipeline processes. The Spark Evaluator processor passes a
                    batch of data to the <samp class="ph codeph">transform</samp> method as a Resilient
                    Distributed Dataset (RDD). The method processes the data according to the custom
                    code.</dd>

            
            
                <dt class="dt dlterm">destroy</dt>

                <dd class="dd">Optional. If you include an <samp class="ph codeph">init</samp> method that makes connections
                    to external systems, you should also include the <samp class="ph codeph">destroy</samp> method
                    to close the connections. The <samp class="ph codeph">destroy</samp> method is called when the
                    pipeline stops.</dd>

            
        </dl>

        <p class="p">When you finish writing the custom Spark class, package a JAR file containing the Spark
            application. Compile against the same stage library version that you use for the Spark
            Evaluator processor. For example, if you are using the Spark Evaluator processor
            included in the stage library for the Cloudera CDH version 5.9 distribution of Hadoop,
            build the application against Spark integrated into Cloudera CDH version 5.9.</p>

        <p class="p">If you used Scala to write the custom Spark class, build the application so that it is
            compatible with Scala 2.10.</p>

    </div>

</div>
<div class="topic task nested1" id="task_dr2_gvd_1y">
    <h2 class="title topictitle2">Installing the Application</h2>

    
    <div class="body taskbody"><p class="shortdesc">Install the Spark application JAR file as an external library for <span class="ph">Data
                  Collector</span>. If
        your custom Spark application depends on external libraries other than the <span class="ph">streamsets-datacollector-api,
                        streamsets-datacollector-spark-api, and spark-core libraries</span>, install those libraries in the same location as well. </p>

        <div class="section context"><div class="note tip"><span class="tiptitle">Tip:</span> To include all dependent libraries used in your custom Spark
                application, you can use the Apache Maven Dependency Plugin. For more information
                about the Dependency Plugin, see <a class="xref" href="http://maven.apache.org/plugins/maven-dependency-plugin/" target="_blank">http://maven.apache.org/plugins/maven-dependency-plugin/</a>.</div>
<span class="ph">For information about installing additional drivers, see
                              <a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft" title="After you've set up the external directory, use the Package Manager within Data Collector to install external libraries.To manually install external libraries, use the required procedure for your installation type.">Install External Libraries</a>.</span><div class="p">
                <div class="note note"><span class="notetitle">Note:</span> The Spark Evaluator processor is included in several CDH and MapR stage
                    libraries. When you install external libraries for the processor, you must make
                    them accessible to the stage library that you are using.</div>

            </div>
</div>

    </div>

    
</div>
<div class="topic task nested1" id="task_g1p_gqn_zx">
    <h2 class="title topictitle2">Configuring a Spark Evaluator</h2>

    <div class="body taskbody">
        <div class="section context">
            <p class="p">Configure a Spark Evaluator
                to process data based on a custom Spark application.</p>

        </div>

        <ol class="ol steps"><li class="li step stepexpand">
                <span class="ph cmd">In the Properties panel, on the <span class="keyword wintitle">General</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_g1p_gqn_zx__d79646e5043" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d466977e427">General Property</th>

                                    <th class="entry" valign="top" width="70%" id="d466977e430">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e427 ">Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e430 ">Stage name.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e427 ">Description</td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e430 ">Optional description.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e427 ">Stage Library</td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e430 ">Library version that you want to use. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e427 ">Required Fields <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_dnj_bkm_vq">
                                            <img class="image" id="task_g1p_gqn_zx__d79646e5098" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e430 ">Fields that must include data for the record to be passed
                                        into the stage. <div class="note tip"><span class="tiptitle">Tip:</span> You might
                                            include fields that the stage uses.</div>
<p class="p">Records
                                            that do not include all required fields are processed
                                            based on the error handling configured for the
                                            pipeline.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e427 ">Preconditions <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_msl_yd4_fs">
                                            <img class="image" id="task_g1p_gqn_zx__d79646e5114" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e430 ">Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <span class="ph uicontrol">Add</span> to create additional
                                        preconditions. <p class="p">Records that do not meet all preconditions
                                            are processed based on the error handling configured for
                                            the stage.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e427 ">On Record Error <a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_atr_j4y_5r">
                                            <img class="image" id="task_g1p_gqn_zx__d79646e5132" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e430 ">Error record handling for the stage: <ul class="ul" id="task_g1p_gqn_zx__d79646e5136">
                                            <li class="li">Discard - Discards the record.</li>

                                            <li class="li">Send to Error - Sends the record to the pipeline for
                                                error handling.</li>

                                            <li class="li">Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>

                                        </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="ph uicontrol">Spark</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_g1p_gqn_zx__table_ufq_xf4_zx" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d466977e551">Spark Property</th>

                                    <th class="entry" valign="top" width="70%" id="d466977e554">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e551 ">Parallelism</td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e554 ">For standalone mode, the number of partitions to create
                                        per batch of records. For example, if set to 4, then the
                                        Spark Transformer simultaneously runs 4 parallel jobs to
                                        process the batch. <p class="p">Set the value based on the number of
                                            available processors on the <span class="ph">Data
                  Collector</span> machine.</p>
<div class="note note"><span class="notetitle">Note:</span> Not used when the processor is
                                            included in a cluster pipeline. The Spark Transformer
                                            uses the number of partitions defined in the Kafka
                                            Consumer or MapR Streams Consumer origin.</div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e551 ">Application Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e554 ">For standalone mode, the name of the Spark application.
                                        Spark displays this application name in the log files. <p class="p">If
                                            you run pipelines that include multiple Spark Evaluator
                                            processors, be sure to use a unique application name for
                                            each to make debugging simpler.</p>
<div class="p">Default is "SDC Spark App".
                                                <div class="note note"><span class="notetitle">Note:</span> Not used when the processor is included in a cluster pipeline.</div>
</div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e551 ">Spark Transformer Class
                                        <a class="xref" href="Spark.html#concept_lfl_dvd_1y">
                                            <img class="image" id="task_g1p_gqn_zx__image_h4p_p5v_yq" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e554 ">Fully qualified name of the custom Spark class that
                                        implements the StreamSets Spark Transformer API. Enter the
                                        class name using the following
                                            format:<pre class="pre codeblock">com.streamsets.spark.&lt;custom class&gt;</pre>
<div class="p">For
                                            example, let's assume that you developed a <samp class="ph codeph">GetCreditCardType</samp> class that
                                            implemented the Spark Transformer API as follows:<pre class="pre codeblock">public class GetCreditCardType extends SparkTransformer implements Serializable {
...
}</pre>
</div>
<div class="p">Then
                                            you would enter the class name as
                                            follows:<pre class="pre codeblock">com.streamsets.spark.GetCreditCardType</pre>
</div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d466977e551 ">Init Method Arguments
                                        <a class="xref" href="Spark.html#concept_lfl_dvd_1y">
                                            <img class="image" id="task_g1p_gqn_zx__image_h3p_p6v_yq" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d466977e554 ">Arguments to pass to the <samp class="ph codeph">init</samp> method in
                                        the custom Spark class. Enter arguments as required by your
                                        custom Spark class.<p class="p">In standalone mode, enter a constant
                                            or an expression for the argument value. In cluster
                                            mode, enter constant values only. In a cluster pipeline,
                                            the Spark Evaluator processor cannot evaluate Data
                                            Collector expressions defined in <samp class="ph codeph">init</samp>
                                            method arguments.</p>
<p class="p">Using <a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">simple or bulk edit mode</a>, click the
                                                <span class="ph uicontrol">Add</span> icon to add additional
                                            arguments.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
</ol>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../Processors/Processors_title.html" title="Processors"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Processors</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"></div>
</body>
</html>