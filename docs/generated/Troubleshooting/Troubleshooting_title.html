
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="description" content="Informational and error messages display in different locations based on the type of information: Pipeline configuration issues The Data Collector UI provides guidance and error details as follows: ..." /><meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Troubleshooting" /><meta name="DC.Relation" scheme="URI" content="../Tutorial/ExtendedTutorial.html#concept_w4n_gjt_ls" /><meta name="DC.Relation" scheme="URI" content="../Glossary/Glossary_title.html#concept_xbx_rs1_tq" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_sh3_frm_tq" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Troubleshooting</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../Tutorial/ExtendedTutorial.html#concept_w4n_gjt_ls" title="Extended Tutorial"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Extended Tutorial</span></a></span>  
<span class="navnext"><a class="link" href="../Glossary/Glossary_title.html#concept_xbx_rs1_tq" title="Glossary"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Glossary</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_sh3_frm_tq">
 <h1 class="title topictitle1">Troubleshooting</h1>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_ivc_mll_2s">
 <h2 class="title topictitle2">Accessing Error Messages</h2>

 <div class="body conbody">
  <p class="p">Informational and error messages display in
      different locations based on the type of information: </p>

  <div class="p">
   <dl class="dl">
    
     <dt class="dt dlterm">Pipeline configuration issues</dt>

     <dd class="dd">The <span class="ph">Data
                  Collector</span>
      UI provides guidance and error details as follows:<ul class="ul" id="concept_ivc_mll_2s__ul_ht3_cml_2s">
       <li class="li">Issues found by implicit validation display in the Issues list.</li>

       <li class="li">An error icon displays at the stage where the problem occurs or on the canvas for
        pipeline configuration issues. </li>

       <li class="li">Issues discovered by explicit validation displays in a warning message on the
        canvas.</li>

      </ul>
</dd>

    
    
          <dt class="dt dlterm">Runtime error information</dt>

          <dd class="dd">You can view error information when you monitor a running pipeline. In the canvas, the
            pipeline displays error record counts for each stage generating error records. </dd>

          <dd class="dd">On the Errors tab, you can view error record statistics and the latest set of error
            records with error messages. If the error was produced by an exception, you can click
              <span class="ph uicontrol">View Stack Trace</span> to view the full stack trace.</dd>

          <dd class="dd">
            <div class="note note"><span class="notetitle">Note:</span> This information becomes unavailable when you stop the pipeline. To preserve
              information about error records, use the Error Records pipeline property to save error
              records.</div>

          </dd>

        
    
     <dt class="dt dlterm">Error record information</dt>

     <dd class="dd">You can use the Error Records pipeline properties to write error records and related
      details to<a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_kgc_l4y_5r">
       another system for review</a>. The information in the following record header attributes
      can help you determine the problem that occurred. For more information, see <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_itf_55z_dz">Internal Attributes</a>.</dd>

     <dd class="dd">For more information about error records and error record handling, see <a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_pm4_txm_vq">Error Record Handling</a>.</dd>

    
    
     <dt class="dt dlterm"><span class="ph">Data
                  Collector</span>
      errors</dt>

     <dd class="dd">You can view information and errors related to the general <span class="ph">Data
                  Collector</span>
      functionality in the <span class="ph">Data
                  Collector</span> log. You
      can view or download the logs from the <span class="ph">Data
                  Collector</span> UI. <span class="ph">For details, see <a class="xref" href="../Administration/Administration_title.html#task_gbm_s3k_br" title="If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level.">Viewing Data Collector Logs</a>.</span></dd>

     <dd class="dd">By default, <span class="ph">Data
                  Collector</span> logs
      messages at the INFO severity level. You can modify the log level to display messages at
      another severity level. <span class="ph">For details, see <a class="xref" href="../Administration/Administration_title.html#task_lkv_g2f_wy" title="If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level.">Modifying the Log Level</a>.</span></dd>

    
   </dl>

  </div>

 </div>

</div>
<div class="topic concept nested1" id="concept_vxd_tsm_tq">
  <h2 class="title topictitle2">Pipeline Basics</h2>

  <div class="body conbody">
    <p class="p">Use the following tips for help with pipeline basics:</p>

    <div class="p">
      <dl class="dl">
        
          <dt class="dt dlterm">When I go to the <span class="ph">Data
                  Collector</span> UI,
            I get a "Webpage not available" error message.</dt>

          <dd class="dd">The <span class="ph">Data
                  Collector</span> is
            not running. Start the <span class="ph">Data
                  Collector</span>.</dd>

        
        
          <dt class="dt dlterm">Why isn't the Start icon enabled? </dt>

          <dd class="dd">You can start a pipeline when it is valid. Use the Issues icon to review the list of
            issues in your pipeline. When you resolve the issues, the Start icon becomes
            enabled.</dd>

        
        
          <dt class="dt dlterm">Why doesn't the Select Fields with Preview Data option work? No preview data
            displays.</dt>

          <dd class="dd">Select Fields with Preview Data works when the pipeline is valid for data preview and
            when <span class="ph">Data
                  Collector</span> is configured to run preview in the background. Make sure all stages are connected
            and required properties are configured. Also verify that preview is running in the
            background by clicking <span class="ph menucascade"><span class="ph uicontrol">Help</span> &gt; <span class="ph uicontrol"> Settings</span></span>.</dd>

        
        
          <dt class="dt dlterm">Sometimes I get a list of available fields and sometimes I don't. What's up with
            that?</dt>

          <dd class="dd">The pipeline can display a list of available fields when the pipeline is valid for
            data preview and when Data Collector is configured to run preview in the background.
            Make sure all stages are connected and required properties are configured. Also verify
            that preview is running in the background by clicking <span class="ph menucascade"><span class="ph uicontrol">Help</span> &gt; <span class="ph uicontrol"> Settings</span></span>.</dd>

        
        
          <dt class="dt dlterm">The data reaching the destination is not what I expect - what do I do?</dt>

          <dd class="dd">If the pipeline is still running, take a couple snapshots of the data being processed,
            then stop the pipeline and enter data preview and use the snapshot as the source data.
            In data preview, you can step through the pipeline and see how each stage alters the
            data.</dd>

          <dd class="dd">If you already stopped the pipeline, perform data preview using the origin data. You
            can step through the pipeline to review how each stage processes the data and update the
            pipeline configuration as necessary. </dd>

          <dd class="dd">You can also edit the data to test for cases that do not occur in the preview data
            set. </dd>

        
      </dl>

    </div>

  </div>

<div class="topic concept nested2" id="concept_tz4_fhm_js">
 <h3 class="title topictitle3">Data Preview </h3>

 <div class="body conbody">
  <div class="p">Use the following tips
   for help with data preview:<dl class="dl">
    
     <dt class="dt dlterm">Why isn't the Preview icon enabled?</dt>

     <dd class="dd">You can preview data after you connect all stages in the pipeline and configure required
            properties. You can use any valid value as a placeholder for required properties.</dd>

    
    
     <dt class="dt dlterm">Why doesn't the data preview show any data?</dt>

     <dd class="dd">If data preview doesn't show any data, one of the following issues might have occurred:<ul class="ul" id="concept_tz4_fhm_js__ul_r5n_jgm_2s">
       <li class="li">The origin might not be configured correctly. <p class="p">In the Preview panel, check the
                  Configuration tab for the origin for related issues. For some origins, you can use
                  Raw Preview to see if the configuration information is correct.</p>
</li>

       <li class="li">The origin might not have any data at the moment. <p class="p">Some origins, such as Directory,
                  File Tail, and Kafka Consumer, can display processed data for data preview.
                  However, most origins require incoming data to enable data preview. </p>
</li>

      </ul>
</dd>

    
    
     <dt class="dt dlterm">Why am I only getting 10 records to preview when I'm asking for more?</dt>

     <dd class="dd">The <span class="ph">Data
                  Collector</span>
      maximum preview batch size overrides the data preview batch size. The <span class="ph">Data
                  Collector</span> default is
      10 records. </dd>

     <dd class="dd">When you request data preview, you can request up to the <span class="ph">Data
                  Collector</span> preview
       batch size, or you can increase the <strong class="ph b">preview.maxBatchSize</strong> property in the <span class="ph">Data
                  Collector</span>
       configuration file, <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>. </dd>

    
        
          <dt class="dt dlterm">In data preview, I edited stage configuration and clicked Run with Changes, but I
            don't see any change in the data.</dt>

          <dd class="dd">This might happen if the configuration change is in the origin. Run with Changes uses
            the existing preview data. To see how changes to origin configuration affects preview
            data, use Refresh Preview. </dd>

        
   </dl>
</div>

 </div>

</div>
<div class="topic concept nested2" id="concept_ydy_pj5_vt">
 <h3 class="title topictitle3">General Validation Errors</h3>

 <div class="body conbody">
  <div class="p">Use the
            following tips for help with general pipeline validation errors:<dl class="dl">
                
                    <dt class="dt dlterm">The pipeline has the following set of validation errors for a stage:</dt>

                    <dd class="dd">
                        <pre class="pre codeblock">CONTAINER_0901 - Could not find stage definition for &lt;stage library name&gt;:&lt;stage name&gt;.
CREATION_006 - Stage definition not found. Library &lt;stage library name&gt;. Stage &lt;stage name&gt;. 
Version &lt;version&gt;
VALIDATION_0006 - Stage definition does not exist, library &lt;stage library name&gt;, 
name &lt;stage name&gt;, version &lt;version&gt;</pre>

                    </dd>

                    <dd class="dd">The pipeline uses a stage that is not installed on the <span class="ph">Data
                  Collector</span>. This might happen if you imported a pipeline from a different version of
                        the <span class="ph">Data
                  Collector</span> and the current <span class="ph">Data
                  Collector</span> is not enabled to use the stage. </dd>

                    <dd class="dd">If the <span class="ph">Data
                  Collector</span> uses a different version of the stage, you might delete the invalid
                        version and replace it with a local valid version. For example, if the
                        pipeline uses an older version of the Hadoop FS destination, you might
                        replace it with a version used by this <span class="ph">Data
                  Collector</span>. </dd>

                    <dd class="dd">If you need to use a stage that is not installed on the <span class="ph">Data
                  Collector</span>, install the related stage library. <span class="ph">For information about installing additional drivers, see
                              <a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft" title="After you've set up the external directory, use the Package Manager within Data Collector to install external libraries.To manually install external libraries, use the required procedure for your installation type.">Install External Libraries</a>.</span></dd>

                
            </dl>
</div>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_fwl_5cl_gs">
 <h2 class="title topictitle2">Origins</h2>

 <div class="body conbody">
  <p class="p">Use
      the following tips for help with origin stages and systems.</p>

 </div>

<div class="topic concept nested2" id="concept_axb_spb_ys">
 <h3 class="title topictitle3">Directory</h3>

 <div class="body conbody">
  <div class="p">
   <dl class="dl">
    
     <dt class="dt dlterm">Why isn't the Directory origin reading all of my files?</dt>

     <dd class="dd">Directory reads a set of files based on the configured file name pattern, read order, and
      first file to process. If new files arrive after Directory has passed their position in the
      read order, Directory does not read the files unless you reset the origin.</dd>

     <dd class="dd">When using the last-modified timestamp read order, arriving files should have timestamps
      that are later than the files in the directory.</dd>

     <dd class="dd">Similarly, when using the lexicographically ascending file name read order, make sure the
      naming convention for the files are lexicographically ascending. For example, filename-1.log,
      filename-2.log, etc., works fine until filename-10.log. If filename-10.log arrives after
      Directory completes reading filename-2.log, Directory does not read filename-10.log since it
      is lexicographically earlier than filename-2.log.</dd>

     <dd class="dd">For more information, see <a class="xref" href="../Origins/Directory.html#concept_b4d_fym_xv">Read Order</a>.</dd>

    
   </dl>

  </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_ocw_b3g_vs">
 <h3 class="title topictitle3">Hadoop FS</h3>

 <div class="body conbody">
  <dl class="dl">
   
    <dt class="dt dlterm">In the pipeline, the Hadoop FS origin has an error icon with the following message:</dt>

    <dd class="dd">
     <pre class="pre codeblock">Validation_0071 - Stage '&lt;stage id&gt;' does not support 'Standalone' execution mode</pre>

    </dd>

    <dd class="dd">You're using the Hadoop FS origin in pipeline configured for standalone execution mode. Use
          the Hadoop FS origin in cluster mode pipelines.</dd>

    <dd class="dd">Workaround: In the pipeline properties, set the <span class="ph uicontrol">Execution Mode</span> to
      <span class="ph uicontrol">Cluster</span>. Or if you want to run the pipeline in standalone mode, use the
     Directory or File Tail origins to process file data. </dd>

   
  </dl>

 </div>

</div>
<div class="topic concept nested2" id="concept_ggx_s23_ks">
 <h3 class="title topictitle3">JDBC Origins</h3>

 <div class="body conbody">
  <div class="p">
   <dl class="dl">
    
     <dt class="dt dlterm">My MySQL JDBC Driver 5.0 fails to validate the query in my JBDC Query Consumer origin. </dt>

     <dd class="dd">This can occur when you use a LIMIT clause in your query. </dd>

     <dd class="dd">Workaround: Upgrade to version 5.1.</dd>

    
    
     <dt class="dt dlterm">I'm using a JDBC origin to read MySQL data. Why are datetime value set to zero being
      treated like error records?</dt>

     <dd class="dd">MySQL treats invalid dates as an exception, so both the JDBC Query Consumer and the JDBC
      Multitable Consumer create error records for invalid dates. </dd>

     <dd class="dd">You can override this behavior by setting a JDBC configuration property in the origin. Add
      the zeroDateTimeBehavior property and set the value to "convertToNull".</dd>

     <dd class="dd">For more information about this and other MySQL-specific JDBC configuration properties, see
       <a class="xref" href="http://dev.mysql.com/doc/connector-j/en/connector-j-reference-configuration-properties.html" target="_blank">http://dev.mysql.com/doc/connector-j/en/connector-j-reference-configuration-properties.html</a>.</dd>

    
    
     <dt class="dt dlterm">A pipeline using the JDBC Query Consumer origin keeps stopping with the following
      error:</dt>

     <dd class="dd">
      <pre class="pre codeblock">JDBC_77 &lt;db error message&gt; attempting to execute query '&lt;query&gt;'. Giving up 
after &lt;error count&gt; errors as per stage configuration. First error: &lt;first db error&gt;.</pre>

     </dd>

     <dd class="dd">This occurs when the origin cannot successfully execute a query. To handle transient
      connection or network errors, try increasing the value for the Number of Retries Upon Query
      Error property on the JDBC tab of the origin. </dd>

    
   </dl>

  </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_err_w23_ks">
 <h3 class="title topictitle3">Kafka Consumer</h3>

 <div class="body conbody">
  <dl class="dl">
   
    <dt class="dt dlterm">Why isn't my pipeline reading existing data from my Kafka topic? </dt>

    <dd class="dd">By default, the Kafka Consumer reads data written to the topic after you start the pipeline.
     Records already in the topic are ignored. </dd>

    <dd class="dd">To read the oldest unread data in a topic, add the <span class="ph uicontrol">auto.offset.reset</span>
     Kafka Configuration property to the origin and set it to <span class="ph uicontrol">smallest</span>.
      <div class="note note"><span class="notetitle">Note:</span> If you already started the pipeline or ran a preview without this option, the offset has
      already been committed. To read the oldest unread data in a topic, set
       <span class="ph uicontrol">auto.offset.reset</span> to <span class="ph uicontrol">smallest</span> and then
      temporarily change the Consumer Group name to a different value. Run data preview. Then,
      change the Consumer Group back to the correct value and start the pipeline.</div>
</dd>

   
   
    <dt class="dt dlterm">How can I reset the offset for a Kafka Consumer?</dt>

    <dd class="dd">Since the offset for a Kafka Consumer is stored with the ZooKeeper for the Kafka cluster,
     you cannot reset the offset through the <span class="ph">Data
                  Collector</span>. For
     information about resetting an offset through Kafka, see the Apache Kafka documentation. </dd>

   
   
    <dt class="dt dlterm">The Kafka Consumer with Kerberos enabled cannot connect to an HDP 2.3 distribution of
     Kafka.</dt>

    <dd class="dd">
     <p class="p">When enabling Kerberos, by default, HDP 2.3 sets the
       <span class="ph uicontrol">security.inter.broker.protocol</span> Kafka broker configuration property to
       <samp class="ph codeph">PLAINTEXTSASL</samp>, which is not supported.</p>

     <p class="p">To correct the issue, set <span class="ph uicontrol">security.inter.broker.protocol</span> to
       PLAINTEXT.</p>

    </dd>

   
  </dl>

 </div>

</div>
<div class="topic concept nested2" id="concept_lsb_gm3_jx">
 <h3 class="title topictitle3">Oracle CDC Client</h3>

 <div class="body conbody">
  <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Data preview continually times out for my Oracle CDC Client pipeline</dt>

                    <dd class="dd">Pipelines that use the Oracle CDC Client can take longer to initiate for
                        data preview. If data preview times out, try increasing the Preview Timeout
                        property to 120,000 milliseconds.</dd>

                
            </dl>

        </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_f5z_s3q_qbb">
 <h3 class="title topictitle3">SQL Server CDC Client</h3>

 <div class="body conbody">
        <dl class="dl">
            
                <dt class="dt dlterm">A pipeline with the SQL Server CDC Client origin cannot establish a connection.
                    The pipeline fails with the following error:</dt>

                <dd class="dd">
                    <pre class="pre codeblock">java.sql.SQLTransientConnectionException: HikariPool-3 - 
   Connection is not available, request timed out after 30004ms.
at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:213)
at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:163)
at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.
  java:85)
at com.streamsets.pipeline.lib.jdbc.multithread.ConnectionManager.
  getNewConnection(ConnectionManager.java:45)
at com.streamsets.pipeline.lib.jdbc.multithread.ConnectionManager.
  getConnection(ConnectionManager.java:57)
at com.streamsets.pipeline.stage.origin.jdbc.cdc.sqlserver.
  SQLServerCDCSource.getCDCTables(SQLServerCDCSource.java:181)</pre>

                </dd>

                <dd class="dd">
                    <p class="p">This can occur when the origin is configured to use a certain number of
                        threads, but the thread pool is not set high enough. On the Advanced tab,
                        check the settings for the Maximum Pool Size and Minimum Idle Connections
                        properties. </p>

                </dd>

                <dd class="dd">When using <a class="xref" href="../Origins/SQLServerChange.html#concept_ofh_gns_r1b">multithreaded processing</a>, you want these properties to be set to
                    greater than or equal to the value of the Number of Threads property on the JDBC
                    tab.</dd>

                <dd class="dd">
                    <p class="p">Also, <a class="xref" href="../Origins/SQLServerCDC.html#concept_nxm_1lp_qbb">allowing late table processing</a> requires the origin to use an
                        additional background thread. When you enable processing late tables, you
                        should set the Maximum Pool Size and Minimum Idle Connections properties to
                        one thread more than the Number of Threads property.</p>

                </dd>

            
            
                <dt class="dt dlterm">After dropping and recreating a table, the origin won't seem to read the data in
                    the table. What's the problem?</dt>

                <dd class="dd">The SQL Server CDC Client origin stores the offset for every table that it
                    processes to track its progress. If you drop a table and recreate it using the
                    same name, the origin assumes it is the same table and uses the last-saved
                    offset for the table. </dd>

                <dd class="dd">If you need the origin to process data earlier than the last-saved offset, you
                    might need to <a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_hdg_j1s_5q" title="You can reset the origin when you want the Data Collector to process all available data instead of processing data from the last-saved offset. Reset the origin when the pipeline is not running.">reset
                        the origin</a>. </dd>

                <dd class="dd">Note that after you reset the origin, the origin drops all stored offsets. And
                    when you restart the pipeline, the origin processes all available data in the
                    specified tables. You cannot reset the origin for a particular table.</dd>

            
        </dl>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_m4g_qyk_2s">
 <h2 class="title topictitle2">Destinations </h2>

 <div class="body conbody">
  <p class="p">Use
            the following tips for help with destination stages and systems.</p>

  </div>

<div class="topic concept nested2" id="concept_a3w_z3m_js">
 <h3 class="title topictitle3">Cassandra</h3>

 <div class="body conbody">
  <div class="p">
   <dl class="dl">
    
     <dt class="dt dlterm">Why is the pipeline failing entire batches when only a few records have a problem?</dt>

     <dd class="dd">Due to Cassandra requirements, when you write to a Cassandra cluster, batches are atomic.
      This means than an error in a one or more records causes the entire batch to fail.</dd>

    
    
     <dt class="dt dlterm">Why is all of my data being sent to error? Every batch is failing.</dt>

     <dd class="dd">When every batch fails, you might have a data type mismatch. Cassandra requires the data
      type of the data to exactly match the data type of the Cassandra column. </dd>

     <dd class="dd">To determine the issue, check the error messages associated with the error records. If you
      see a message like the following, you have a data type mismatch. The following error message
      indicates that data type mismatch is for Integer data being unsuccessfully written to a Varchar
      column:
      <pre class="pre codeblock">CASSANDRA_06 - Could not prepare record 'sdk:': 
Invalid type for value 0 of CQL type varchar, expecting class java.lang.String but class java.lang. 
Integer provided`</pre>
</dd>

     <dd class="dd">To correct the problem, you might use a Field Type Converter processor to convert field
      data types. In this case, you would convert the integer data to string.</dd>

    
   </dl>

  </div>

 </div>

</div>
<div class="topic concept nested2" id="unique_490975729">
 <h3 class="title topictitle3">Hadoop FS</h3>

 <div class="body conbody">
  <div class="p">
   <dl class="dl">
    
     <dt class="dt dlterm">I'm writing text data to HDFS. Why are my files all empty? </dt>

     <dd class="dd">You might not have the pipeline or Hadoop FS destination configured correctly. </dd>

     <dd class="dd">The Hadoop FS destination uses a single field to write text data to HDFS. </dd>

     <dd class="dd">The pipeline should collapse all data to a single field. And the Hadoop FS destination must
      be configured to use that field. By default, Hadoop FS uses a field named /text. </dd>

    
   </dl>

  </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_rp1_ghd_zs">
 <h3 class="title topictitle3">HBase</h3>

 <div class="body conbody">
  <div class="p">
   <dl class="dl">
    
     <dt class="dt dlterm">I get the following error when validating or starting a pipeline with an HBase
      destination:</dt>

     <dd class="dd">
      <pre class="pre codeblock">HBASE_06 - Cannot connect to cluster: org.apache.hadoop.hbase.MasterNotRunningException: 
com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: 
Call to node00.local/&lt;IP_address&gt;:60000 failed on local exception: 
org.apache.hadoop.hbase.exceptions.ConnectionClosingException: 
Connection to node00.local/&lt;IP_address&gt;:60000 is closing. Call id=0, waitTime=58</pre>

     </dd>

     <dd class="dd">Is your HBase master is running? If so, then you might trying to connect to a secure HBase
      cluster without configuring the HBase destination to use Kerberos authentication. In the HBase
      destination properties, select <span class="ph uicontrol">Kerberos Authentication</span> and try
      again.</dd>

    
   </dl>

  </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_mth_cjm_js">
 <h3 class="title topictitle3">Kafka Producer</h3>

 <div class="body conbody">
    <div class="p">
      <dl class="dl">
        
          <dt class="dt dlterm">Can the Kafka Producer create topics?</dt>

          <dd class="dd">The Kafka Producer can create a topic when all of the following are true:<ul class="ul" id="concept_mth_cjm_js__ul_j4q_25y_3r">
              <li class="li">You configure the Kafka Producer to write to a topic name that does not
                exist.</li>

              <li class="li">At least one of the Kafka brokers defined for the Kafka Producer has the
                auto.create.topics.enable property enabled.</li>

              <li class="li">The broker with the enabled property is up and available when the Kafka Producer
                looks for the topic.</li>

            </ul>
</dd>

        
        
          <dt class="dt dlterm">A pipeline that writes to Kafka keeps failing and restarting in an endless cycle.</dt>

          <dd class="dd">This can happen when the pipeline tries to write message to Kafka 0.8 that is longer
            than the Kafka maximum message size. </dd>

          <dd class="dd">Workaround: Reconfigure Kafka brokers to allow larger messages or ensure that incoming
            records are within the configured limit. </dd>

        
      </dl>

    </div>

    <dl class="dl">
      
        <dt class="dt dlterm">The Kafka Producer with Kerberos enabled cannot connect to the HDP 2.3 distribution of
          Kafka. </dt>

        <dd class="dd">
          <p class="p">When enabling Kerberos, by default, HDP 2.3 sets the
              <span class="ph uicontrol">security.inter.broker.protocol</span> Kafka broker configuration
            property to <samp class="ph codeph">PLAINTEXTSASL</samp>, which is not supported.</p>

          <p class="p">To correct the issue, set <span class="ph uicontrol">security.inter.broker.protocol</span> to
              PLAINTEXT.</p>

        </dd>

      
    </dl>

  </div>

</div>
<div class="topic concept nested2" id="concept_gt1_s41_rt">
 <h3 class="title topictitle3">SDC RPC</h3>

 <div class="body conbody">
  <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">A pipeline fails to start with the following validation error:</dt>

                    <dd class="dd">
                        <pre class="pre codeblock">IPC_DEST_15 Could not connect to any SDC RPC destination : [&lt;host name&gt;: 
java.net.ConnectException: Connection refused]</pre>

                    </dd>

                    <dd class="dd">You configured the pipeline to write error records to a pipeline, but the
                        configuration information for the error records pipeline is invalid. </dd>

                    <dd class="dd">To write error records to a pipeline, you need a valid destination pipeline
                        that includes an RPC origin.</dd>

                
            </dl>

        </div>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_wms_z44_nbb">
 <h2 class="title topictitle2">Executors</h2>

 <div class="body conbody">
  <p class="p">Use
            the following tips for help with executors.</p>

 </div>

<div class="topic concept nested2" id="concept_njt_1p4_nbb">
 <h3 class="title topictitle3">Hive Query</h3>

 <div class="body conbody">
  <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">When I enter the name of the Impala JDBC driver in the stage, I receive an
                        error saying that the driver is not present in the class path:</dt>

                    <dd class="dd">
                        <pre class="pre codeblock">HIVE_15 - Hive JDBC Driver &lt;driver name&gt; not present in the class path.</pre>

                    </dd>

                
            </dl>

        </div>

        <p class="p">To use an Impala JDBC driver with the Hive Query executor, the driver must be installed
            as an external library. And it must be installed for the stage library that the Hive
            Query executor uses. </p>

        <p class="p">If you already installed the driver, verify that you have it installed for the correct
            stage library. For more information, see <a class="xref" href="../Executors/HiveQuery.html#concept_rfq_xk4_nbb">Installing the Impala Driver</a>.</p>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_vrq_qh1_hy">
 <h2 class="title topictitle2">JDBC Connections</h2>

 <div class="body conbody">
        <p class="p">Use the following
            tips for help with stages that use JDBC connections to connect to databases. </p>

        <p class="p">The following stages use a JDBC connection: </p>

        <ul class="ul" id="concept_vrq_qh1_hy__ul_f1l_fnb_cz">
                  <li class="li">JDBC Multitable Consumer origin</li>

                  <li class="li">JDBC Query Consumer origin</li>

                  <li class="li">JMS Consumer origin</li>

                  <li class="li">MySQL Binary Log origin</li>

                  <li class="li">Oracle CDC Client origin</li>

                  <li class="li">SQL Server CDC Client origin</li>

                  <li class="li">SQL Server Change Tracking origin</li>

                  <li class="li">JDBC Lookup processor</li>

                  <li class="li">JDBC Tee processor</li>

                  <li class="li">JDBC Producer destination</li>

                  <li class="li">JMS Producer destination</li>

                  <li class="li">JDBC Query executor</li>

            </ul>

    </div>

<div class="topic concept nested2" id="concept_oqc_431_hy">
 <h3 class="title topictitle3">No Suitable Driver</h3>

 <div class="body conbody">
  <div class="p">When <span class="ph">Data
                  Collector</span>
            cannot find the JDBC driver for a stage, the following error message
            displays:<pre class="pre codeblock">JDBC_00 - Cannot connect to specified database: com.streamsets.pipeline.api.StageException:
JDBC_06 - Failed to initialize connection pool: java.sql.SQLException: No suitable driver</pre>
</div>

        <p class="p">Verify that you have followed the instructions to install additional drivers, as
            explained in <a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft" title="After you've set up the external directory, use the Package Manager within Data Collector to install external libraries.To manually install external libraries, use the required procedure for your installation type.">Install External Libraries</a>.</p>

        <p class="p">You can also use these additional tips to help resolve the issue:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">The JDBC connection string is not correct.</dt>

                <dd class="dd">The <span class="ph uicontrol">JDBC Connection String</span> property for the stage must
                    include the <samp class="ph codeph">jdbc:</samp> prefix. For example, a Postgres connection
                    string might be <samp class="ph codeph">jdbc:postgresql://&lt;database host&gt;/&lt;database
                        name&gt;</samp>. </dd>

                <dd class="dd">Check your database documentation for the required connection string format. For
                    example, if you are using a non-standard port, you must specify it in the
                    connection string. </dd>

            
            
                <dt class="dt dlterm">The JDBC driver is not stored in the correct directory.</dt>

                <dd class="dd">You must store the JDBC driver in the following directory: <samp class="ph codeph">&lt;external
                        directory&gt;/streamsets-datacollector-jdbc-lib/lib/</samp>.</dd>

                <dd class="dd">
                    <p class="p">For example, assuming that you defined the external directory as
                            <samp class="ph codeph">/opt/sdc-extras</samp>, store the JDBC JAR files in
                            <samp class="ph codeph">/opt/sdc-extras/streamsets-datacollector-jdbc-lib/lib/</samp>.</p>

                </dd>

            
            
                <dt class="dt dlterm">STREAMSETS_LIBRARIES_EXTRA_DIR is not set correctly.</dt>

                <dd class="dd">You must set the <samp class="ph codeph">STREAMSETS_LIBRARIES_EXTRA_DIR</samp> environment
                    variable to tell <span class="ph">Data
                  Collector</span> where the JDBC drivers and other additional libraries are located. The
                    location should be external to the Data Collector installation directory. </dd>

                <dd class="dd">For example, to use <samp class="ph codeph">/opt/sdc-extras</samp> as the external directory
                    for additional libraries, then you would set
                        <samp class="ph codeph">STREAMSETS_LIBRARIES_EXTRA_DIR</samp> as follows:
                    <pre class="pre codeblock">export STREAMSETS_LIBRARIES_EXTRA_DIR="/opt/sdc-extras/"</pre>
</dd>

                <dd class="dd">
                    <p class="p">Modify the <a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_rng_qym_qr" title="You can edit the Data Collector environment configuration file to modify the directories used to store configuration, data, log, and resource files.When you run Data Collector as a service, Data Collector runs as the system user account and group defined in environment variables. The default system user and group are named sdc.Increase or decrease the Data Collector Java heap size as necessary, based on the resources available on the host machine. By default, the Java heap size is 1024 MB. You can enable remote debugging to debug a Data Collector instance running on a remote machine. You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.Data Collector includes a Java Security Manager that is enabled by default. You can edit the Data Collector environment configuration file to configure the path to JAR files to be added to the Data Collector root classloader.">environment configuration file</a> used by your installation type.</p>

                </dd>

            
            
                <dt class="dt dlterm">The security policy is not set.</dt>

                <dd class="dd">You must grant permission for code in the external directory. Ensure that the
                        <samp class="ph codeph">$SDC_CONF/sdc-security.policy</samp> file contains the following
                    lines:
                    <pre class="pre codeblock">// user-defined external directory
grant codebase "file://&lt;external directory&gt;-" {
  permission java.security.AllPermission;
};</pre>
</dd>

                <dd class="dd">For
                    example:<pre class="pre codeblock">// user-defined external directory
grant codebase "file:///opt/sdc-extras/-" {
  permission java.security.AllPermission;
};</pre>
</dd>

            
            
                <dt class="dt dlterm">The JDBC driver does not support auto-loading.</dt>

                <dd class="dd">Even if the connection string is correctly configured, the JDBC driver JAR file
                    is in the correct location, the environment variable is correctly set, and
                    you've set the correct security policy, the JDBC driver might not correctly
                    support <a class="xref" href="http://archive.oreilly.com/pub/a/onjava/2006/08/02/jjdbc-4-enhancements-in-java-se-6.html" target="_blank">JDBC 4.0 auto-loading</a>. </dd>

                <dd class="dd">If you have checked all of the above, and you are still seeing the "No suitable
                    driver" error message, add the class name for the driver in the <span class="ph uicontrol">JDBC
                        Class Driver Name</span> property in the <span class="ph uicontrol">Legacy
                        Drivers</span> tab for the stage. </dd>

            
            
                <dt class="dt dlterm">The sdc user does not have correct permissions on the JDBC driver.</dt>

                <dd class="dd">When you run <span class="ph">Data
                  Collector</span> as a service, the default system user named <samp class="ph codeph">sdc</samp> is used to
                    start the service. The user must have read access to the JDBC driver and all
                    directories in its path.</dd>

                <dd class="dd">To verify the permissions, run the following
                    command:<pre class="pre codeblock">sudo -u sdc file &lt;external directory&gt;/streamsets-datacollector-jdbc-lib/lib/&lt;driver jar file&gt;</pre>
</dd>

                <dd class="dd">For example, let's assume that you are using an external directory of
                        <samp class="ph codeph">/opt/sdc-extras</samp> and the MySQL JDBC driver. If you receive
                    the following output when you run the command, then the <samp class="ph codeph">sdc</samp>
                    user does not have read or execute access on one or more of the directories in
                    the
                    path:<pre class="pre codeblock">/opt/sdc-extras/streamsets-datacollector-jdbc-lib/lib/mysql-connector-java-5.1.40-bin.jar: cannot open `/opt/sdc-extras/streamsets-datacollector-jdbc-lib/lib/mysql-connector-java-5.1.40-bin.jar' (Permission denied)</pre>
</dd>

                <dd class="dd">
                    <div class="p">To resolve this issue, identify the relevant directories and grant the
                            <samp class="ph codeph">sdc</samp> user read and execute access on those directories.
                        For example, run the following command to grant the user access on the root
                        of the external
                        directory:<pre class="pre codeblock">chmod 755 /opt/sdc-extras</pre>
</div>

                </dd>

                <dd class="dd">If you receive the following output when you run the command, then the
                        <samp class="ph codeph">sdc</samp> user does not have read permission on the JDBC
                    driver:<pre class="pre codeblock">/opt/sdc-extras/streamsets-datacollector-jdbc-lib/lib/mysql-connector-java-5.1.40-bin.jar: regular file, no read permission</pre>
</dd>

                <dd class="dd">To resolve this issue, run the following command to grant the user read access
                    to the
                    driver:<pre class="pre codeblock">chmod 644 /opt/sdc-extras/streamsets-datacollector-jdbc-lib/lib/mysql-connector-java-5.1.40-bin.jar</pre>
</dd>

            
        </dl>

 </div>

</div>
<div class="topic concept nested2" id="concept_cx2_p31_hy">
 <h3 class="title topictitle3">Cannot Connect to Database</h3>

 <div class="body conbody">
        <p class="p">When <span class="ph">Data
                  Collector</span> cannot connect to the database, an error message like the following displays - the
            exact message can vary depending on the driver:</p>

        <pre class="pre codeblock">JDBC_00 - Cannot connect to specified database: com.zaxxer.hikari.pool.PoolInitializationException:
Exception during pool initialization: The TCP/IP connection to the host 1.2.3.4, port 1234 has failed</pre>

        <div class="p">In this case, verify that the <span class="ph">Data
                  Collector</span>
            machine can access the database machine on the relevant port. You can use tools such as
            ping and netcat (nc) for this purpose. For example, to verify that the host 1.2.3.4 is
            accessible:<pre class="pre codeblock">$ ping 1.2.3.4 
PING 1.2.3.4 (1.2.3.4): 56 data bytes 
64 bytes from 1.2.3.4: icmp_seq=0 ttl=57 time=12.063 ms 
64 bytes from 1.2.3.4: icmp_seq=1 ttl=57 time=11.356 ms 
64 bytes from 1.2.3.4: icmp_seq=2 ttl=57 time=11.626 ms 
^C
--- 1.2.3.4 ping statistics --- 
3 packets transmitted, 3 packets received, 0.0% packet loss 
round-trip min/avg/max/stddev = 11.356/11.682/12.063/0.291 ms</pre>
</div>

        <div class="p">Then to verify that port 1234 can be
            reached:<pre class="pre codeblock">$ nc -v -z -w2 1.2.3.4 1234 
nc: connectx to 1.2.3.4 port 1234 (tcp) failed: Connection refused</pre>
</div>

        <p class="p">If the host or port is not accessible, check the routing and firewall configuration.</p>

    </div>

</div>
<div class="topic concept nested2" id="concept_hnf_wkt_sy">
 <h3 class="title topictitle3">MySQL JDBC Driver and Time Values</h3>

 <div class="body conbody">
  <p class="p">Due to a MySQL JDBC driver issue, the driver cannot return time values to the millisecond.
            Instead, the driver returns the values to the second.</p>

        <p class="p">For example, if a column has a value of 20:12:50.581, the driver reads the value as
            20:12:50.000.</p>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_ay2_w1l_2s">
 <h2 class="title topictitle2">Performance</h2>

 <div class="body conbody">
  <div class="p">Use the following tips
   for help with performance:<dl class="dl">
    
     <dt class="dt dlterm">Why is my batch size only 1000 records when I configured my origin for larger batches?</dt>

     <dd class="dd">The <span class="ph">Data
                  Collector</span>
      maximum batch size overrides the maximum batch size configured in origins. The <span class="ph">Data
                  Collector</span> default is
      1000 records. </dd>

     <dd class="dd">When you configure the origin batch size, you can request up to the <span class="ph">Data
                  Collector</span> maximum
      batch size, or you can increase the <strong class="ph b">production.maxBatchSize</strong> property in the
       <span class="ph">Data
                  Collector</span>
       configuration file, <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>. </dd>

    
        
          <dt class="dt dlterm">How can I decrease the delay between reads from the origin system?</dt>

          <dd class="dd">A long delay can occur between reads from the origin system when a pipeline reads
            records faster than it can process them or write them to the destination system. Because
            a pipeline processes one batch at a time, the pipeline must wait until a batch is
            committed to the destination system before reading the next batch, preventing the
            pipeline from reading at a steady rate. Reading data at a steady rate provides better
            performance than reading sporadically.</dd>

          <dd class="dd">If you cannot increase the throughput for the processors or destination, limit the
            rate at which the pipeline reads records from the origin system. Configure the
              <span class="ph uicontrol">Rate Limit</span> property for the pipeline to define the maximum
            number of records that the pipeline can read in a second.</dd>

        
     
          <dt class="dt dlterm">When I try to start one or more pipelines, I receive an error that not enough threads
            are available.</dt>

          <dd class="dd">By default, <span class="ph">Data
                  Collector</span> can
            run approximately 22 standalone pipelines at the same time. If you run a larger number
            of standalone pipelines at the same time, you might receive the following
            error:<pre class="pre codeblock">CONTAINER_0166 - Cannot start pipeline '&lt;pipeline name&gt;' as there are not enough threads available</pre>
</dd>

          <dd class="dd">To resolve this error, increase the value of the <strong class="ph b">runner.thread.pool.size</strong>
            property in the <span class="ph">Data
                  Collector</span>
            configuration file, <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>.</dd>

          <dd class="dd">For more information, see <a class="xref" href="../Configuration/DCConfig.html#task_nkc_ydl_bw" title="By default, Data Collector can run approximately 22 standalone pipelines concurrently. If you plan to run a larger number of pipelines at the same time, increase the thread pool size.">Running Multiple Concurrent Pipelines</a>.</dd>

        
    
          <dt class="dt dlterm">How can I tell what's slowing down my pipeline?</dt>

          <dd class="dd">Review the information available in the <span class="ph">Data
                  Collector</span> UI
            in Monitor mode. Charts provide information about the record count, record throughput,
            and batch throughput for the pipeline. To determine where processing slows, you can
            click each stage to view the count and throughput details for the stage.</dd>

          <dd class="dd">If the origin is the issue, you might tune the batch size or batch wait time
            properties or adjust related properties in the origin system. If the destinations cause
            the problem, try adjusting any performance-related properties in the destination or
            related properties in the destination system.</dd>

          <dd class="dd">If a processor causes the problem, you might take a snapshot of the pipeline to review
            how data passes through the pipeline and consider options for streamlining processing.
          </dd>

        
    
     <dt class="dt dlterm">How can I improve the general pipeline performance? </dt>

     <dd class="dd">You might improve performance by adjusting the batch size used by the pipeline. The batch
      size determines how much data passes through the pipeline at one time. By default, the batch
      size is 1000 records. </dd>

     <dd class="dd">You might adjust the batch size based on the size of the records or the speed of their
      arrival. For example, if your records are extremely big, you might reduce the batch size to
      increase the processing speed. Or if the records are small and arrive quickly, you might
      increase the batch size.</dd>

     <dd class="dd">Experiment with the batch size and review the results<span class="ph"> in Monitor
              mode</span>. </dd>

     <dd class="dd">To change the batch size, configure the <strong class="ph b">production.maxBatchSize</strong> property in the <span class="ph">Data
                  Collector</span>
       configuration file, <samp class="ph codeph">$SDC_CONF/sdc.properties</samp>. </dd>

    
   </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_zjl_nnl_2s">
 <h2 class="title topictitle2">Cluster Execution Mode </h2>

 <div class="body conbody">
  <div class="p">Use the following tips
      for help with pipelines in cluster mode:<dl class="dl">
        
          <dt class="dt dlterm">I got the following validation error when configuring a cluster pipeline. What does it
            mean?</dt>

          <dd class="dd">
            <pre class="pre codeblock">Validation_0071 - Stage '&lt;stage id&gt;' does not support 'Standalone' execution mode</pre>

          </dd>

          <dd class="dd">This message can appear when you include a non-cluster origin in a cluster pipeline.
            You can use the cluster version of the Kafka Consumer and the Hadoop FS origin in a
            cluster pipeline. </dd>

          <dd class="dd">The message can also appear if you choose the Write to File option for  pipeline error
            handling. Write to File is not supported for cluster mode. </dd>

        
        
          <dt class="dt dlterm">Why isn't the <span class="ph">Data
                  Collector</span>
            reading data from my new Kafka partition?</dt>

          <dd class="dd">If you create a new partition in the Kafka topic, to launch a new <span class="ph">Data
                  Collector</span>
            worker to read from the partition, you need to restart the pipeline.</dd>

        
        
          <dt class="dt dlterm">My pipeline fails to start with the following error: </dt>

          <dd class="dd"><pre class="pre codeblock">Pipeline Status: START_ERROR: Unexpected error starting pipeline:java.lang.IllegalStateException: 
Timed out after waiting 121 seconds for cluster application to start. Submit command is not alive.</pre>
Check
            the <span class="ph">Data
                  Collector</span> log for more information. It's possible that the Spark on YARN client configuration
            is not in place, the installation is out of date, or the node being used is not a
            gateway node.</dd>

        
        
          <dt class="dt dlterm">My pipeline fails to start with the following error: </dt>

          <dd class="dd">
            <pre class="pre codeblock">Pipeline Status: START_ERROR: IO Error while trying to start the pipeline: java.io.IOException: 
Kerberos Error: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]</pre>

          </dd>

          <dd class="dd">It's likely that the cluster is configured to use Kerberos, but the <span class="ph">Data
                  Collector</span> is
            not configured to use Kerberos. <span class="ph">For more information about enabling Kerberos authentication
                        for <span class="ph">Data
                  Collector</span>, see <a class="xref" href="../Configuration/DCConfig.html#concept_hnm_n4l_xs" title="You can use Kerberos authentication to connect to external systems as well as YARN clusters.">Kerberos Authentication</a>.</span></dd>

        
        
          <dt class="dt dlterm">My pipeline stopped unexpectedly.</dt>

          <dd class="dd">Check the Spark Application Master logs in the YARN Resource Manager UI for more
            information about the problem.</dd>

        
        
          <dt class="dt dlterm">Why does my pipeline take so long to start?</dt>

          <dd class="dd">The start time for a pipeline can vary based on how busy the YARN cluster is.
            Typically, a cluster pipeline should start in 30-90 seconds. </dd>

        
      </dl>
</div>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navprev"><a class="link" href="../Tutorial/ExtendedTutorial.html#concept_w4n_gjt_ls" title="Extended Tutorial"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Extended Tutorial</span></a></span>  
<span class="navnext"><a class="link" href="../Glossary/Glossary_title.html#concept_xbx_rs1_tq" title="Glossary"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Glossary</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

--><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>