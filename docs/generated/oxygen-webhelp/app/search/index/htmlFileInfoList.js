define(function () {
return ["datacollector/UserGuide/Administration/Administration_title.html@@@Administration@@@You can view the directories that the Data Collector uses. You might check the directories being used to access a file in the directory or to increase the amount of available space for a directory...","datacollector/UserGuide/Alerts/RulesAlerts_title.html@@@Rules and Alerts@@@Define rules to enable capturing information about a running pipeline. You can enable an alert for any rule to be notified when the specified condition occurs...","datacollector/UserGuide/Apx-DataFormats/DataFormat_Title.html@@@Data Formats by Stage@@@The following table lists the data formats supported by each origin. Origin Avro Binary Datagram Delimited Excel JSON Log Protobuf SDC Record Text Whole File XML Amazon S3 Amazon SQS Consumer \u00A0 \u00A0...","datacollector/UserGuide/Apx-GrokPatterns/GrokPatterns_title.html@@@Grok Patterns@@@You can use the grok patterns in this appendix to define the structure of log data...","datacollector/UserGuide/Apx-RegEx/RegEx-Title.html@@@Regular Expressions@@@A regular expression, also known as regex, describes a pattern for a string...","datacollector/UserGuide/Cluster_Mode/AmazonS3Requirements.html@@@Amazon S3 Requirements@@@Cluster EMR batch and cluster batch mode pipelines can process data from Amazon S3...","datacollector/UserGuide/Cluster_Mode/ClusterPipelines.html@@@Cluster Pipeline Overview@@@A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode...","datacollector/UserGuide/Cluster_Mode/ClusterPipelines_title.html@@@Cluster Pipelines@@@...","datacollector/UserGuide/Cluster_Mode/HDFSRequirements.html@@@HDFS Requirements@@@Cluster mode pipelines that read from HDFS require the Cloudera distribution of Hadoop (CDH) or Hortonworks Data Platform (HDP). For a list of the supported CDH or HDP versions, see Available Stage...","datacollector/UserGuide/Cluster_Mode/KafkaRequirements.html@@@Kafka Cluster Requirements@@@When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to Kafka...","datacollector/UserGuide/Cluster_Mode/MapRRequirements.html@@@MapR Requirements@@@Complete the following steps to configure a cluster pipeline to read from MapR in cluster streaming mode...","datacollector/UserGuide/Cluster_Mode/StageLimitations.html@@@Cluster Pipeline Limitations@@@Please note the following limitations in cluster pipelines: Non-cluster origins - Do not use non-cluster origins in cluster pipelines. For a description of the origins to use, see Cluster Batch and...","datacollector/UserGuide/Configuration/Authentication.html@@@User Authentication@@@Data Collector can authenticate user accounts based on LDAP or files. Best practice is to use LDAP if your organization has it. By default, Data Collector uses file-based authentication...","datacollector/UserGuide/Configuration/Config_title.html@@@Configuration@@@...","datacollector/UserGuide/Configuration/CredentialStores.html@@@Credential Stores@@@Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system...","datacollector/UserGuide/Configuration/CustomStageLibraries.html@@@Custom Stage Libraries@@@If you develop custom stages, store the stage libraries in a local directory external to the Data Collector installation directory. Use an external directory to enable use of the custom stage...","datacollector/UserGuide/Configuration/DCConfig.html@@@Data Collector Configuration@@@You can edit the Data Collector configuration file, $SDC_CONF/sdc.properties, to configure properties such as the host name and port number and account information for email alerts...","datacollector/UserGuide/Configuration/DCEnvironmentConfig.html@@@Data Collector Environment Configuration@@@Data Collector includes environment variables that define the directories used to store configuration,\n        data, log, and resource files...","datacollector/UserGuide/Configuration/ExternalLibs.html@@@Install External Libraries@@@After you&apos;ve set up the external directory, use the Package Manager within Data Collector to install external libraries...","datacollector/UserGuide/Configuration/JMXMetrics-EnableExternalTools.html@@@Enabling External JMX Tools@@@Data Collector uses JMX metrics to generate the graphical display of the status of a running pipeline. You can provide the same JMX metrics to external tools if desired...","datacollector/UserGuide/Configuration/PublishMetadata.html@@@Working with Data Governance Tools@@@You can configure Data Collector to integrate with data governance tools, giving you visibility into data movement - where the data came from, where it\u2019s going to, and who is interacting with it...","datacollector/UserGuide/Configuration/RolesandPermissions.html@@@Roles and Permissions@@@Data Collector allows you to assign roles and pipeline permissions to users and groups. Roles, such as Creator or Manager, enable users to perform different Data Collector tasks. Each user needs at...","datacollector/UserGuide/Configuration/Vault-Overview.html@@@Accessing Vault Secrets with Vault Functions (Deprecated)@@@Data Collector can use Vault functions to access information, a.k.a. secrets , stored in Hashicorp Vault. However, the Vault functions are now deprecated and will be removed in a future release. We...","datacollector/UserGuide/DPM/AggregatedStatistics.html@@@Pipeline Statistics@@@A Control Hub job defines the pipeline to run and the Data Collectors that run the pipeline. When you start a job, Control Hub remotely runs the pipeline on the group of Data Collectors. To monitor the job statistics and metrics within Control Hub, you must configure the pipeline to write statistics to Control Hub or to another system...","datacollector/UserGuide/DPM/DPM.html@@@Meet StreamSets Control Hub@@@StreamSets Control HubTM is a central point of control for all of your dataflow pipelines. Control Hub allows teams to build and execute large numbers of complex dataflows at scale...","datacollector/UserGuide/DPM/DPMConfiguration.html@@@Control Hub Configuration File@@@You can customize how a registered Data Collector works with StreamSets Control Hub by editing the Control Hub configuration file, $SDC_CONF/dpm.properties, located in the Data Collector installation...","datacollector/UserGuide/DPM/DPM_title.html@@@StreamSets Control Hub@@@...","datacollector/UserGuide/DPM/OrgUserAccount.html@@@Request a Control Hub Organization and User Account@@@To register a Data Collector with StreamSets Control Hub,\n        you must have a Control Hub user account within an organization...","datacollector/UserGuide/DPM/PipelineManagement.html@@@Pipeline Management with Control Hub@@@After you register a Data Collector with StreamSets Control Hub,\n        you can manage how the pipelines work with Control Hub...","datacollector/UserGuide/DPM/RegisterSDCwithDPM.html@@@Register Data Collector with Control Hub@@@You must register a Data Collector to work with StreamSets Control Hub.\n        When you register a Data Collector, Data Collector generates an authentication token that it uses to issue authenticated requests to Control Hub...","datacollector/UserGuide/DPM/UnregisterSDCwithDPM.html@@@Unregister Data Collector from Control Hub@@@You can unregister a Data Collector from StreamSets Control Hub when you no longer want to use that Data Collector installation with Control Hub...","datacollector/UserGuide/DPM/WorkingWithDPM.html@@@Working with Control Hub@@@Before you can work with StreamSets Control Hub, you must have an Control Hub organization and user account...","datacollector/UserGuide/Data_Formats/DataFormats-Overview.html@@@Data Formats Overview@@@Data formats - such as Avro, JSON, and log - are methods to encode data that adhere to generally accepted specifications. The way that stages process data can be similar based on the stage type and...","datacollector/UserGuide/Data_Formats/DataFormats-Title.html@@@Data Formats@@@...","datacollector/UserGuide/Data_Formats/DelimitedDataRootFieldTypes.html@@@Delimited Data Root Field Type@@@When reading delimited data, Data Collector can create records that use the list or list-map root field type. When Data Collector creates records for delimited data, it creates a single root field of...","datacollector/UserGuide/Data_Formats/Excel.html@@@Excel Data Format@@@You can use file-based origins, such as Directory and Google Cloud Storage , to process Microsoft Excel .xls or .xlsx files. When processing Excel files, an origin creates a record for every row in...","datacollector/UserGuide/Data_Formats/LogFormats.html@@@Log Data Format@@@When you use an origin to read log data, you define the format of the log files to be read...","datacollector/UserGuide/Data_Formats/NetFlow_Overview.html@@@NetFlow Data Processing@@@You can use Data Collector to process NetFlow 5 and NetFlow 9 data. When processing NetFlow 5 data, Data Collector processes flow records based on information in the packet header. Data Collector...","datacollector/UserGuide/Data_Formats/Protobuf-Prerequisites.html@@@Protobuf Data Format Prerequisites@@@Perform the following prerequisites before reading or writing protobuf data...","datacollector/UserGuide/Data_Formats/TextCDelim.html@@@Text Data Format with Custom Delimiters@@@By default, the text data format creates records based on line breaks, creating a record for each line of text. You can configure origins to create records based on custom delimiters. Use custom...","datacollector/UserGuide/Data_Formats/WholeFile.html@@@Whole File Data Format@@@You can use the whole file data format to transfer entire files from an origin system to a destination system. With the whole file data format, you can transfer any type of file...","datacollector/UserGuide/Data_Formats/WritingXML.html@@@Writing XML Data@@@When writing XML data, destinations create a valid XML document for each record. The destination requires the record to have a single root field that contains the rest of the record data. When writing...","datacollector/UserGuide/Data_Formats/XMLDFormat.html@@@Reading and Processing XML Data@@@You can parse XML documents from an origin system with an origin enabled for the XML data format. You can also parse XML documents in a field in a Data Collector record with the XML Parser processor...","datacollector/UserGuide/Data_Preview/DataPreview_Title.html@@@Data Preview@@@You can preview data to help build or fine-tune a pipeline. When using Control Hub, you can also use data preview when developing pipeline fragments...","datacollector/UserGuide/Destinations/Aerospike.html@@@Aerospike@@@The Aerospike destination writes data to Aerospike...","datacollector/UserGuide/Destinations/AmazonS3.html@@@Amazon S3@@@The Amazon S3 destination writes data to Amazon S3. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon Kinesis Streams, use the Kinesis Producer destination...","datacollector/UserGuide/Destinations/AzureEventHubProducer.html@@@Azure Event Hub Producer@@@The Azure Event Hub Producer writes data to Microsoft Azure Event Hub. To write to Microsoft Azure Data Lake Store, use the Azure Data Lake Store destination. To write to Microsoft Azure IoT Hub, use...","datacollector/UserGuide/Destinations/AzureIoTHub.html@@@Azure IoT Hub Producer@@@Data Collector functions as a simulated device that sends messages to Azure IoT Hub. Before you configure the Azure IoT Hub Producer destination, register Data Collector as a device in your IoT hub...","datacollector/UserGuide/Destinations/BigQuery.html@@@Google BigQuery@@@The Google BigQuery destination maps fields from records to BigQuery columns in existing tables based on matching names and compatible data types. If needed, the destination converts Data Collector data types to BigQuery data types...","datacollector/UserGuide/Destinations/Bigtable.html@@@Google Bigtable@@@The Google Bigtable destination requires the BoringSSL library. You must download and install the external library so that the Google Bigtable destination can access it...","datacollector/UserGuide/Destinations/Cassandra.html@@@Cassandra@@@The Cassandra destination writes data to a Cassandra cluster...","datacollector/UserGuide/Destinations/CoAPClient.html@@@CoAP Client@@@Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Client destination writes data to a CoAP endpoint. Use the destination to send requests to a CoAP resource URL...","datacollector/UserGuide/Destinations/Couchbase.html@@@Couchbase@@@The Couchbase destination writes data to Couchbase Server. Couchbase Server is a distributed NoSQL document-oriented database...","datacollector/UserGuide/Destinations/DataLakeStore.html@@@Azure Data Lake Store@@@The Azure Data Lake Store destination writes data to the Microsoft Azure Data Lake Store. You can use the Azure Data Lake Store destination in standalone and cluster batch pipelines. The destination...","datacollector/UserGuide/Destinations/Destinations-title.html@@@Destinations@@@...","datacollector/UserGuide/Destinations/Destinations_overview.html@@@Destinations@@@A destination stage represents the target for a pipeline. You can use one or more destinations in a pipeline...","datacollector/UserGuide/Destinations/Elasticsearch.html@@@Elasticsearch@@@The Elasticsearch destination writes data to an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters). The destination uses the Elasticsearch HTTP API to write each record to Elasticsearch as a document...","datacollector/UserGuide/Destinations/Flume.html@@@Flume@@@The Flume destination writes data to a Flume source. When you write data to Flume, you pass data to a Flume client. The Flume client passes data to hosts based on client configuration properties...","datacollector/UserGuide/Destinations/GCS.html@@@Google Cloud Storage@@@The Google Cloud Storage destination writes data to objects in Google Cloud Storage. You can use other destinations to write to Google BigQuery , Google Bigtable , and Google Pub/Sub . The destination...","datacollector/UserGuide/Destinations/HBase.html@@@HBase@@@The HBase destination writes data to an HBase cluster. The destination can write data to HBase as text, binary data, or JSON strings. You can define the data format for each column written to HBase...","datacollector/UserGuide/Destinations/HTTPClient.html@@@HTTP Client@@@The HTTP Client destination writes data to an HTTP endpoint. The destination sends requests to an HTTP resource URL. Use the HTTP Client destination to perform a range of standard requests or use an expression to determine the request for each record...","datacollector/UserGuide/Destinations/HadoopFS-destination.html@@@Hadoop FS@@@You can use the HDP stage libraries to write to Azure Blob storage using the WASB protocol. This enables the Hadoop FS destination to write directly to Azure HDInsight...","datacollector/UserGuide/Destinations/Hive.html@@@Hive Streaming@@@The Hive Streaming destination writes data to Hive tables stored in the ORC (Optimized Row Columnar) file format...","datacollector/UserGuide/Destinations/HiveMetastore.html@@@Hive Metastore@@@The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive...","datacollector/UserGuide/Destinations/InfluxDB.html@@@InfluxDB@@@The InfluxDB destination writes data to an InfluxDB database...","datacollector/UserGuide/Destinations/JDBCProducer.html@@@JDBC Producer@@@The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log...","datacollector/UserGuide/Destinations/JMSProducer.html@@@JMS Producer@@@Before you use the JMS Producer, install the JMS drivers for the implementation that you are using...","datacollector/UserGuide/Destinations/KProducer.html@@@Kafka Producer@@@The Kafka Producer destination writes data to a Kafka cluster...","datacollector/UserGuide/Destinations/KinFirehose.html@@@Kinesis Firehose@@@The Kinesis Firehose destination writes data to an Amazon Kinesis Firehose delivery stream. Firehose automatically delivers the data to the Amazon S3 bucket or Amazon Redshift table that you specify in the delivery stream...","datacollector/UserGuide/Destinations/KinProducer.html@@@Kinesis Producer@@@The Kinesis Producer destination writes data to Amazon Kinesis Streams. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon S3, use the Amazon S3 destination...","datacollector/UserGuide/Destinations/KineticaDB.html@@@KineticaDB@@@The KineticaDB destination writes data to a table in a Kinetica cluster using the Kinetica bulk inserter. When you configure the KineticaDB destination, you specify the URL for the Kinetica head node...","datacollector/UserGuide/Destinations/Kudu.html@@@Kudu@@@The Kudu destination writes data to a Kudu cluster...","datacollector/UserGuide/Destinations/LocalFS.html@@@Local FS@@@Use the Local FS destination to write records to files in a local file system. When you configure a Local FS destination, you can define a directory template and time basis to determine the output...","datacollector/UserGuide/Destinations/MQTTPublisher.html@@@MQTT Publisher@@@The MQTT Publisher destination publishes messages to a topic on an MQTT broker. The destination functions as an MQTT client that publishes messages, writing each record as a message...","datacollector/UserGuide/Destinations/MapRDB.html@@@MapR DB@@@The MapR DB destination writes data to MapR DB binary tables. The destination can write data to MapR DB as text, binary data, or JSON strings. You can define the data format for each column written to MapR DB...","datacollector/UserGuide/Destinations/MapRDBJSON.html@@@MapR DB JSON@@@MapR DB uses a row key to uniquely identify each row in a JSON table. The row key is defined by the _id field of the JSON document stored in the row...","datacollector/UserGuide/Destinations/MapRFS.html@@@MapR FS@@@The MapR FS destination writes files to MapR FS. You can write the data to MapR as flat files or Hadoop sequence files...","datacollector/UserGuide/Destinations/MapRStreamsProd.html@@@MapR Streams Producer@@@The MapR Streams Producer destination writes messages to MapR Streams...","datacollector/UserGuide/Destinations/MongoDB.html@@@MongoDB@@@The MongoDB destination writes data to MongoDB. To write to MongoDB, records must include a CRUD operation record header attribute. The CRUD operation header attribute indicates the operation to...","datacollector/UserGuide/Destinations/NamedPipe.html@@@Named Pipe@@@The Named Pipe destination writes data to a UNIX named pipe...","datacollector/UserGuide/Destinations/PubSubPublisher.html@@@Google Pub/Sub Publisher@@@When configured to use the Google Cloud service account credentials file, the destination checks for the file defined in the destination properties...","datacollector/UserGuide/Destinations/RabbitMQ.html@@@RabbitMQ Producer@@@RabbitMQ Producer writes AMQP messages to a single RabbitMQ queue...","datacollector/UserGuide/Destinations/Redis.html@@@Redis@@@The Redis destination writes data to Redis...","datacollector/UserGuide/Destinations/SDC_RPCdest.html@@@SDC RPC@@@The SDC RPC destination enables connectivity between two SDC RPC pipelines. The SDC RPC destination passes data to one or more SDC RPC origins. Use the SDC RPC destination as part of an SDC RPC origin pipeline...","datacollector/UserGuide/Destinations/Salesforce.html@@@Salesforce@@@The Salesforce destination writes data to Salesforce objects...","datacollector/UserGuide/Destinations/Solr.html@@@Solr@@@The Solr destination writes data to a Solr node or cluster...","datacollector/UserGuide/Destinations/Splunk.html@@@Splunk@@@The Splunk destination writes data to Splunk using the Splunk HTTP Event Collector\n        (HEC)...","datacollector/UserGuide/Destinations/ToError.html@@@To Error@@@The To Error destination passes records to the pipeline for error handling. Use the To Error destination to send a stream of records to pipeline error handling...","datacollector/UserGuide/Destinations/Trash.html@@@Trash@@@The Trash destination discards records. Use the Trash destination as a visual representation of records discarded from the pipeline. Or, you might use the Trash destination during development as a temporary placeholder...","datacollector/UserGuide/Destinations/WaveAnalytics.html@@@Einstein Analytics@@@The Einstein Analytics destination writes data to Salesforce Einstein Analytics. The destination connects to Einstein Analytics to upload external data to a dataset...","datacollector/UserGuide/Destinations/WebSocketClient.html@@@WebSocket Client@@@The WebSocket Client destination writes data to a WebSocket endpoint. Use the destination to send data to a WebSocket resource URL...","datacollector/UserGuide/Edge_Mode/CaseStudy-IoT.html@@@Example: IoT Preventative Maintenance@@@Let&apos;s say that you have a factory with a network of machine tools. Sensors are installed on each machine that measure the temperature, relative humidity, and pressure of the machine. You need to continuously monitor this sensor data, and shut down any machine that exceeds the allowed limits...","datacollector/UserGuide/Edge_Mode/DownloadPipelines.html@@@Downloading Pipelines from SDC Edge@@@When SDC Edge is running and is accessible by the Data Collector machine, you can download edge pipelines from SDC Edge into Data Collector.\n        When you download pipelines from SDC Edge, you download all edge pipelines deployed to that SDC Edge in addition to all sample edge pipelines included with SDC Edge...","datacollector/UserGuide/Edge_Mode/EdgePipelineTypes.html@@@Edge Pipelines@@@An edge sending pipeline uses an origin specific to the edge device to read local data residing on the device. The pipeline can perform minimal processing on the data before sending the data to a Data Collector receiving pipeline...","datacollector/UserGuide/Edge_Mode/EdgePipelines_Deploy.html@@@Deploy Pipelines to SDC Edge@@@After designing edge pipelines in Data Collector, you deploy the edge pipelines to SDC Edge installed on an edge device. You run the edge pipelines on SDC Edge...","datacollector/UserGuide/Edge_Mode/EdgePipelines_Manage.html@@@Manage Pipelines on SDC Edge@@@After designing edge pipelines in Data Collector and then deploying the edge pipelines to SDC Edge, you can manage the pipelines on SDC Edge.\n        Managing edge pipelines includes previewing, validating, starting, stopping, resetting the origin, and monitoring the pipelines...","datacollector/UserGuide/Edge_Mode/EdgePipelines_Overview.html@@@Edge Pipelines Overview@@@An edge pipeline is a pipeline that runs on an edge device with limited resources. Use edge pipelines to read data from the edge device or to receive data from another pipeline and then act on that data to control the edge device...","datacollector/UserGuide/Edge_Mode/EdgePipelines_title.html@@@Edge Pipelines@@@...","datacollector/UserGuide/Edge_Mode/GettingStartedSamples.html@@@Getting Started with Sample Edge Pipelines@@@Data Collector Edge\n            (SDC Edge)\n        includes several sample pipelines that make it easy to get started. You simply create the appropriate Data Collector receiving pipeline, download and install SDC Edge on the edge device, and then run the sample edge pipeline...","datacollector/UserGuide/Edge_Mode/SDCEInstall.html@@@Install SDC Edge@@@Download and install SDC Edge on each edge device where you want to run edge pipelines...","datacollector/UserGuide/Edge_Mode/SDCReceivingPipelines.html@@@Data Collector Receiving Pipelines@@@Data Collector receiving pipelines run in the standalone execution mode. You design and run receiving pipelines in Data Collector...","datacollector/UserGuide/Edge_Mode/SDCeAdminister.html@@@Administer SDC Edge@@@Administering SDC Edge involves configuring, starting, shutting down, and viewing logs for the agent...","datacollector/UserGuide/Edge_Mode/SDCeUninstall.html@@@Uninstalling SDC Edge@@@To uninstall SDC Edge , shut down SDC Edge and then remove the SDC Edge home directory. To shut down SDC Edge running on the edge device, type Ctrl+C in the command prompt. Remove the SDC Edge home...","datacollector/UserGuide/Edge_Mode/SupportedPlatforms.html@@@Supported Platforms@@@Install Data Collector Edge\n            (SDC Edge)\n        on each edge device where you want to run edge pipelines...","datacollector/UserGuide/Event_Handling/EventFramework-Title.html@@@Dataflow Triggers@@@Dataflow triggers are instructions for the event framework to kick off tasks in response to events that occur in the pipeline. For example, you can use dataflow triggers to start a MapReduce job after...","datacollector/UserGuide/Executors/AmazonS3.html@@@Amazon S3 Executor@@@When Data Collector uses the Amazon S3 executor, it must pass credentials to Amazon Web Services...","datacollector/UserGuide/Executors/Email.html@@@Email Executor@@@The Email executor sends the configured email to the specified recipients upon receiving an event. You can also configure the executor to send an email based on a condition, such as the arrival of a...","datacollector/UserGuide/Executors/Executors-overview.html@@@Executors@@@An executor stage triggers a task when it receives an event. Executors do not write or store events. Use executors as part of a dataflow trigger in an event stream to perform event-driven...","datacollector/UserGuide/Executors/Executors-title.html@@@Executors@@@...","datacollector/UserGuide/Executors/HDFSMetadata.html@@@HDFS File Metadata Executor@@@The HDFS File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in HDFS or a local file system each time it receives an event. You cannot perform multiple...","datacollector/UserGuide/Executors/HiveQuery.html@@@Hive Query Executor@@@The Hive Query executor connects to Hive or Impala and performs one or more user-defined Hive or Impala queries each time it receives an event record. Use the Hive Query executor as part of an event...","datacollector/UserGuide/Executors/JDBCQuery.html@@@JDBC Query Executor@@@The JDBC Query executor connects through JDBC to a database and performs a user-defined SQL query each time it receives an event record. Use the JDBC Query executor as part of an event stream in the...","datacollector/UserGuide/Executors/MapRFSFileMeta.html@@@MapR FS File Metadata Executor@@@The MapR FS File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in MapR FS each time it receives an event. You cannot perform multiple tasks in the same...","datacollector/UserGuide/Executors/MapReduce.html@@@MapReduce Executor@@@The MapReduce executor includes two predefined jobs: Avro to ORC and Avro to Parquet...","datacollector/UserGuide/Executors/PipelineFinisher.html@@@Pipeline Finisher Executor@@@When it receives an event, the Pipeline Finisher executor stops a pipeline and transitions it to a Finished state. This allows the pipeline to complete all expected processing before stopping. Use the...","datacollector/UserGuide/Executors/Shell.html@@@Shell Executor@@@When Data Collector is registered with Control Hub, you can configure Data Collector to use an abbreviated version of the Control Hub user ID for shell impersonation mode...","datacollector/UserGuide/Executors/Spark.html@@@Spark Executor@@@The Spark executor starts a Spark application each time it receives an event. You can use the Spark executor with Spark on YARN or Spark on Databricks. The executor is not compatible with Spark on...","datacollector/UserGuide/Expression_Language/Constants.html@@@Constants@@@The expression language provides constants for use in expressions...","datacollector/UserGuide/Expression_Language/DateTimeVariables.html@@@Datetime Variables@@@The expression language provides datetime variables for use in expressions...","datacollector/UserGuide/Expression_Language/ExpressionLanguage_overview.html@@@Expression Language@@@The StreamSets expression language enables you to create expressions that evaluate or modify data. The StreamSets expression language is based on the JSP 2.0 expression language. Use the expression...","datacollector/UserGuide/Expression_Language/ExpressionLanguage_title.html@@@Expression Language@@@...","datacollector/UserGuide/Expression_Language/Functions.html@@@Functions@@@Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record...","datacollector/UserGuide/Expression_Language/Literals.html@@@Literals@@@The expression language includes the following literals...","datacollector/UserGuide/Expression_Language/Operators.html@@@Operators@@@The precedence of operators highest to lowest, left to right is as follows...","datacollector/UserGuide/Expression_Language/ReservedWords.html@@@Reserved Words@@@The following words are reserved for the expression language and should not be used as identifiers...","datacollector/UserGuide/Getting_Started/GettingStarted_Title.html@@@Getting Started@@@StreamSets Data Collector is a lightweight, powerful engine that streams data in real time. Use Data Collector to route and process data in your data streams...","datacollector/UserGuide/Glossary/Glossary_title.html@@@Glossary@@@batch A set of records that passes through a pipeline. Data Collector processes data in batches. CDC-enabled origin An origin that can process changed data and place CRUD operation information in the...","datacollector/UserGuide/Hive_Drift_Solution/HiveDriftSolution_title.html@@@Drift Synchronization Solution for Hive@@@Now to process the metadata records - and to automatically create and update tables in Hive - you need the Hive Metastore destination...","datacollector/UserGuide/Installation/AddtionalStageLibs.html@@@Install Additional Stage Libraries@@@Install additional stage libraries to use stages that are not included in the core RPM or core tarball installation of Data Collector . This is an optional step, but generally you&apos;ll want to install...","datacollector/UserGuide/Installation/CMInstall-Overview.html@@@Installation with Cloudera Manager@@@When administering Data Collector with Cloudera Manager, configure all Data Collector configuration properties and environment variables through Cloudera Manager...","datacollector/UserGuide/Installation/CloudInstall.html@@@Installation with Cloud Service Providers@@@You can install the full Data Collector using cloud service providers such as Microsoft Azure or Microsoft Azure HDInsight. When you install Data Collector using a cloud service provider, you install Data Collector as a service...","datacollector/UserGuide/Installation/CoreInstall_Overview.html@@@Core Installation@@@You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space...","datacollector/UserGuide/Installation/CreateAnotherDC.html@@@Creating Another Data Collector Instance@@@You can create another instance of a Data Collector tarball or RPM installation on the same machine with the create-dc command.\n        The additional Data Collector instance uses the same configuration as the original Data Collector instance. You can modify the configuration properties as needed...","datacollector/UserGuide/Installation/FullInstall_ServiceStart.html@@@Full Installation and Launch (Service Start)@@@You can install the Data Collector RPM package and start it as a service on CentOS or Red Hat Enterprise Linux...","datacollector/UserGuide/Installation/Install_title.html@@@Installation@@@...","datacollector/UserGuide/Installation/InstallationAndConfig.html@@@Installation@@@You can install Data Collector and start it manually or run it as a service...","datacollector/UserGuide/Installation/Installing_the_DC-Docker.html@@@Run Data Collector from Docker@@@You can run the Data Collector image from Docker Hub...","datacollector/UserGuide/Installation/Installing_the_DC.html@@@Full Installation and Launch (Manual Start)@@@You can install the full Data Collector tarball and start it manually on all supported operating systems...","datacollector/UserGuide/Installation/MapR-Prerequisites.html@@@MapR Prerequisites@@@Due to licensing restrictions, StreamSets cannot distribute MapR libraries with Data Collector. As a result, you must perform additional steps to enable the Data Collector machine to connect to MapR. Data Collector does not display MapR origins and destinations in stage library lists nor the MapR Streams statistics aggregator in the pipeline properties until you perform these prerequisites...","datacollector/UserGuide/Installation/Uninstall.html@@@Uninstallation@@@You can uninstall a Data Collector instance that was installed from the tarball, from the RPM package, or from Cloudera Manager. To uninstall a Data Collector instance that was installed from the...","datacollector/UserGuide/JDBC_DriftSolution/JDBC_DriftSyncSolution_title.html@@@Drift Synchronization Solution for PostgreSQL@@@The Drift Synchronization Solution for PostgreSQL detects drift in incoming data and automatically creates or alters corresponding PostgreSQL tables as needed before the data is written. For example...","datacollector/UserGuide/Multithreaded_Pipelines/MultithreadedPipelines.html@@@Multithreaded Pipelines@@@A multithreaded pipeline is a pipeline with an origin that supports parallel execution, enabling one pipeline to run in multiple threads. Multithreaded pipelines enable processing high volumes of data...","datacollector/UserGuide/Origins/AmazonS3.html@@@Amazon S3@@@When Data Collector reads data from an Amazon S3 origin, it must pass credentials to Amazon Web Services...","datacollector/UserGuide/Origins/AmazonSQS.html@@@Amazon SQS Consumer@@@When Data Collector reads data from an Amazon SQS Consumer origin, it must pass credentials to Amazon Simple Queue Services...","datacollector/UserGuide/Origins/AzureEventHub.html@@@Azure IoT/Event Hub Consumer@@@The Azure IoT/Event Hub Consumer origin reads data from Microsoft Azure Event Hub. The origin can use multiple threads to enable parallel processing of data from a single Azure event hub. Before you...","datacollector/UserGuide/Origins/BigQuery.html@@@Google BigQuery@@@The Google BigQuery origin executes a query job and reads the result from Google BigQuery...","datacollector/UserGuide/Origins/CoAPServer.html@@@CoAP Server@@@Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Server origin is a multithreaded origin that listens on a CoAP endpoint and processes the contents of all authorized CoAP requests...","datacollector/UserGuide/Origins/Directory.html@@@Directory@@@The Directory origin reads data from files in a directory. The origin can use multiple threads to enable the parallel processing of files...","datacollector/UserGuide/Origins/Elasticsearch.html@@@Elasticsearch@@@The Elasticsearch origin is a multithreaded origin that reads data from an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters). The origin generates a record for each Elasticsearch document...","datacollector/UserGuide/Origins/FileTail.html@@@File Tail@@@The File Tail origin reads lines of data as they are written to an active file after reading related archived files in the same directory. File Tail generates a record for each line of data...","datacollector/UserGuide/Origins/GCS.html@@@Google Cloud Storage@@@The Google Cloud Storage origin reads objects stored in Google Cloud Storage. The objects must be fully written and reside in a single bucket. The object names must share a prefix pattern. With the...","datacollector/UserGuide/Origins/HDFSStandalone.html@@@Hadoop FS Standalone@@@The Hadoop FS Standalone origin reads files in HDFS. The origin can use multiple threads to enable the parallel processing of files. The files to be processed must all share a file name pattern and be fully written. You can also configure the origin to read from Azure HDInsight...","datacollector/UserGuide/Origins/HTTPClient-ResponseHeaderFields.html@@@Response Header Fields in Header Attributes@@@The HTTP Client origin includes response header fields \u2013 such as Content-Encoding, Content-Type, or any custom response header field \u2013 in records as record header attributes. The attribute names match...","datacollector/UserGuide/Origins/HTTPClient.html@@@HTTP Client@@@You can use pagination to retrieve a large volume of data from a paginated API...","datacollector/UserGuide/Origins/HTTPServer.html@@@HTTP Server@@@The HTTP Server origin is a multithreaded origin that listens on an HTTP endpoint and processes the contents of all authorized HTTP POST and PUT requests. Use the HTTP Server origin to read high volumes of HTTP POST and PUT requests using multiple threads...","datacollector/UserGuide/Origins/HTTPtoKafka.html@@@HTTP to Kafka (Deprecated)@@@The HTTP to Kafka origin listens on an HTTP endpoint and writes the contents of all authorized HTTP POST requests directly to Kafka. However, the HTTP to Kafka origin is now deprecated and will be...","datacollector/UserGuide/Origins/HadoopFS-origin.html@@@Hadoop FS@@@The Hadoop FS origin reads data from the Hadoop Distributed File System (HDFS), Amazon S3, or other file systems using the Hadoop FileSystem interface...","datacollector/UserGuide/Origins/JDBCConsumer.html@@@JDBC Query Consumer@@@JDBC Query Consumer uses an offset column and initial offset value to determine where to start reading data within a table. Include both the offset column and the offset value in the WHERE clause of the SQL query...","datacollector/UserGuide/Origins/JMS.html@@@JMS Consumer@@@The JMS Consumer origin reads data from a Java Messaging Service (JMS)...","datacollector/UserGuide/Origins/KConsumer.html@@@Kafka Consumer@@@The first time that a Kafka Consumer origin identified by a consumer group receives messages from a topic, an offset entry is created for that consumer group and topic. The offset entry is created in ZooKeeper or Kafka, depending on your Kafka version and broker configuration...","datacollector/UserGuide/Origins/KafkaMultiConsumer.html@@@Kafka Multitopic Consumer@@@The first time that a Kafka Multitopic Consumer origin identified by a consumer group receives messages from a topic, an offset entry is created for that consumer group and topic.\n    The offset entry is created in Kafka...","datacollector/UserGuide/Origins/KinConsumer.html@@@Kinesis Consumer@@@The Kinesis Consumer origin reads data from Amazon Kinesis Streams...","datacollector/UserGuide/Origins/MQTTSubscriber.html@@@MQTT Subscriber@@@The MQTT Subscriber origin subscribes to topics on an MQTT broker to read messages from the broker. The origin functions as an MQTT client that receives messages, generating a record for each message...","datacollector/UserGuide/Origins/MapRDBJSON.html@@@MapR DB JSON@@@The MapR DB JSON origin reads JSON documents from MapR DB JSON tables. The origin converts each document into a record...","datacollector/UserGuide/Origins/MapRFS.html@@@MapR FS@@@The MapR FS origin reads files from MapR FS. Use this origin only in pipelines configured for cluster batch pipeline execution mode...","datacollector/UserGuide/Origins/MapRFSStandalone.html@@@MapR FS Standalone@@@The MapR FS Standalone origin reads files in MapR. The origin can use multiple threads to enable the parallel processing of files. The files to be processed must all share a file name pattern and be fully written...","datacollector/UserGuide/Origins/MapRStreamsCons.html@@@MapR Streams Consumer@@@The MapR Streams Consumer origin reads messages from MapR Streams...","datacollector/UserGuide/Origins/MapRStreamsMultiConsumer.html@@@MapR Multitopic Streams Consumer@@@You can add custom configuration properties to the MapR Multitopic Streams Consumer. You can use any MapR or Kafka property supported by MapR Streams. For more information, see the MapR Streams documentation...","datacollector/UserGuide/Origins/MapRdbCDC.html@@@MapR DB CDC@@@The MapR DB CDC origin reads changed data from MapR DB that has been written to MapR Streams. The origin can use multiple threads to enable parallel processing of data. You might use this origin to...","datacollector/UserGuide/Origins/MongoDB.html@@@MongoDB@@@The MongoDB origin reads data from MongoDB. Data Collector generates a record for every MongoDB document. To read change data capture information from the MongoDB Oplog, use the MongoDB Oplog origin...","datacollector/UserGuide/Origins/MongoDBOplog.html@@@MongoDB Oplog@@@The MongoDB Oplog origin reads entries from MongoDB Oplog. MongoDB stores information about changes to the database in a local capped collection called an Oplog. The Oplog contains information about...","datacollector/UserGuide/Origins/MultiTableJDBCConsumer.html@@@JDBC Multitable Consumer@@@You define the group of tables that the JDBC Multitable Consumer origin reads by defining schema and table name patterns for the table configuration. The origin reads all tables whose names match the table pattern in the schemas whose names match the schema pattern...","datacollector/UserGuide/Origins/MySQLBinaryLog.html@@@MySQL Binary Log@@@The MySQL Binary Log origin can process binary logs from a MySQL server configured to use row-based logging...","datacollector/UserGuide/Origins/OPCUAClient.html@@@OPC UA Client@@@The OPC UA Client origin processes data from an OPC UA server. The OPC UA Client origin can poll the server at regular intervals, returning the latest data from all specified nodes. Or it can...","datacollector/UserGuide/Origins/Omniture.html@@@Omniture@@@The Omniture origin processes JSON website usage reports generated by the Omniture reporting APIs. Omniture is also known as the Adobe Marketing Cloud...","datacollector/UserGuide/Origins/OracleCDC.html@@@Oracle CDC Client@@@The Oracle CDC Client origin provides an alternate PEG parser that you can try when concerned about pipeline performance...","datacollector/UserGuide/Origins/Origins_overview.html@@@Origins@@@An origin stage represents the source for the pipeline. You can use a single origin stage in a pipeline...","datacollector/UserGuide/Origins/Origins_title.html@@@Origins@@@...","datacollector/UserGuide/Origins/PostgreSQL.html@@@PostgreSQL CDC Client@@@The PostgreSQL CDC Client origin processes Write-Ahead Logging (WAL) data to generate change data capture records for a PostgreSQL database. Use the PostgreSQL CDC Client origin to process WAL data from PostgreSQL 9.4 or later. Earlier versions do not support WAL...","datacollector/UserGuide/Origins/PubSub.html@@@Google Pub/Sub Subscriber@@@The Google Pub/Sub Subscriber origin consumes messages from a Google Pub/Sub subscription...","datacollector/UserGuide/Origins/RabbitMQ.html@@@RabbitMQ Consumer@@@RabbitMQ Consumer reads AMQP messages from a single RabbitMQ queue...","datacollector/UserGuide/Origins/Redis.html@@@Redis Consumer@@@The Redis Consumer origin reads messages from Redis...","datacollector/UserGuide/Origins/SDCRPCtoKafka.html@@@SDC RPC to Kafka (Deprecated)@@@You can specify the maximum number of requests the SDC RPC to Kafka origin handles at one time...","datacollector/UserGuide/Origins/SDC_RPCorigin.html@@@SDC RPC@@@The SDC RPC origin enables connectivity between two SDC RPC pipelines. The SDC RPC origin reads data passed from an SDC RPC destination. Use the SDC RPC origin as part of an SDC RPC destination pipeline...","datacollector/UserGuide/Origins/SFTP.html@@@SFTP/FTP Client@@@The SFTP/FTP Client origin reads files from a server using the Secure File Transfer Protocol (SFTP) or the File Transfer Protocol (FTP)...","datacollector/UserGuide/Origins/SQLServerCDC.html@@@SQL Server CDC Client@@@You can define the initial order that the origin uses to read the tables...","datacollector/UserGuide/Origins/SQLServerChange.html@@@SQL Server Change Tracking@@@You can define the initial order that the origin uses to read the tables...","datacollector/UserGuide/Origins/Salesforce.html@@@Salesforce@@@The Salesforce origin can execute a query to read existing data from Salesforce. Use the Salesforce Object Query Language (SOQL) to write the query...","datacollector/UserGuide/Origins/SystemMetrics.html@@@System Metrics@@@The System Metrics origin reads system metrics from the edge device where StreamSets Data Collector Edge\n            (SDC Edge)\n        is installed. Use the System Metrics origin only in pipelines configured for edge execution mode...","datacollector/UserGuide/Origins/TCPServer.html@@@TCP Server@@@The TCP Server origin listens at the specified port numbers, establishes TCP sessions with clients that initiate TCP connections, and then processes the incoming data. The origin can operate in...","datacollector/UserGuide/Origins/UDP.html@@@UDP Source@@@The UDP Source origin reads messages from one or more UDP ports. To use multiple threads for pipeline processing, use the UDP Multithreaded Source . For a discussion about the differences between the...","datacollector/UserGuide/Origins/UDPMulti.html@@@UDP Multithreaded Source@@@The UDP Multithreaded Source origin reads messages from one or more UDP ports. The origin can create multiple worker threads to enable parallel processing in a multithreaded pipeline. UDP...","datacollector/UserGuide/Origins/UDPtoKafka.html@@@UDP to Kafka (Deprecated)@@@When you use a UDP to Kafka origin in a pipeline, connect the origin to a Trash destination...","datacollector/UserGuide/Origins/WebSocketClient.html@@@WebSocket Client@@@The WebSocket Client origin reads data from a WebSocket server endpoint. Use the origin to read data from a WebSocket resource URL...","datacollector/UserGuide/Origins/WebSocketServer.html@@@WebSocket Server@@@The WebSocket Server origin is a multithreaded origin that listens on a WebSocket endpoint and processes the contents of all authorized WebSocket client requests. Use the WebSocket Server origin to read high volumes of WebSocket client requests using multiple threads...","datacollector/UserGuide/Origins/WindowsLog.html@@@Windows Event Log@@@The Windows Event Log origin reads data from a Microsoft Windows event log located on a Windows machine. The origin generates a record for each event in the log...","datacollector/UserGuide/Pipeline_Configuration/ConfiguringAPipeline.html@@@Configuring a Pipeline@@@Configure a pipeline to define the stream of data. After you configure the pipeline,\n        you can start the pipeline...","datacollector/UserGuide/Pipeline_Configuration/DataCollectorUI-Config.html@@@Data Collector UI - Edit Mode@@@The following image shows the Data Collector UI when you configure a pipeline: Area / Icon Name Description 1 Pipeline canvas Displays the pipeline. Use to configure the pipeline data flow. 2 Pipeline...","datacollector/UserGuide/Pipeline_Configuration/EventGeneration.html@@@Event Generation@@@The event framework generates events for standalone pipelines when the pipeline starts and stops. You can pass the event to an executor or to another pipeline for additional processing. By default...","datacollector/UserGuide/Pipeline_Configuration/Expressions.html@@@Expression Configuration@@@Precede all expressions with a dollar sign and enclose them with curly brackets, as follows: ${&lt;expression&gt;}...","datacollector/UserGuide/Pipeline_Configuration/Notifications.html@@@Notifications@@@You can configure a pipeline to send an email or webhook when the pipeline changes to specified states. For example, you might send an email when someone manually stops the pipeline, causing it to...","datacollector/UserGuide/Pipeline_Configuration/PipelineConfiguration_title.html@@@Pipeline Configuration@@@...","datacollector/UserGuide/Pipeline_Configuration/PipelineMemory.html@@@Pipeline Memory@@@When you enable the monitor.memory Data Collector configuration property, you can use the Max Pipeline Memory pipeline property to define the maximum amount of memory Data Collector uses when it runs...","datacollector/UserGuide/Pipeline_Configuration/PipelineRateLimit.html@@@Rate Limit@@@You can limit the rate at which a pipeline processes records by defining the maximum number of records that the pipeline can read in a second...","datacollector/UserGuide/Pipeline_Configuration/Retry.html@@@Retrying the Pipeline@@@By default, when Data Collector encounters a stage-level error that might cause a standalone pipeline to fail, it retries the pipeline. That is, it waits a period of time, and then tries again to run the pipeline...","datacollector/UserGuide/Pipeline_Configuration/RuntimeValues.html@@@Runtime Values@@@Runtime values are values that you define outside of the pipeline and use for stage and pipeline properties. You can change the values for each pipeline run without having to edit the pipeline...","datacollector/UserGuide/Pipeline_Configuration/SSL-TLS.html@@@SSL/TLS Configuration@@@Some stages allow you use SSL/TLS to connect to the external system. When you enable TLS, you can generally configure properties on the TLS tab of the stage. The properties that are available can...","datacollector/UserGuide/Pipeline_Configuration/SimpleBulkEdit.html@@@Simple and Bulk Edit Mode@@@Some pipeline and pipeline stage properties include an Add icon ( ) where you add additional configurations. You can add the configurations in simple or bulk edit mode. By default, each property uses...","datacollector/UserGuide/Pipeline_Configuration/Validation.html@@@Implicit and Explicit Validation@@@Data Collector performs two types of validation: Implicit validation Implicit validation occurs by default as the Data Collector UI saves your changes. Implicit validation lists missing or incomplete...","datacollector/UserGuide/Pipeline_Configuration/Webhooks.html@@@Webhooks@@@You can configure a pipeline use webhooks. A webhook is a user-defined HTTP callback - an HTTP request that the pipeline sends automatically when certain actions occur. You can use webhooks to...","datacollector/UserGuide/Pipeline_Design/CDC-Overview.html@@@Processing Changed Data@@@Certain stages enable you to easily process change capture data (CDC) or transactional data in a pipeline. CDC-enabled origins can read change capture data. Some exclusively read change capture data...","datacollector/UserGuide/Pipeline_Design/ControlCharacters.html@@@Control Character Removal@@@You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records...","datacollector/UserGuide/Pipeline_Design/DatainMotion.html@@@Data in Motion@@@Data passes through the pipeline in batches. This is how it works...","datacollector/UserGuide/Pipeline_Design/DesigningDataFlow.html@@@Designing the Data Flow@@@You can branch and merge streams in the pipeline...","datacollector/UserGuide/Pipeline_Design/DevStages.html@@@Development Stages@@@You can use several development stages to help develop and test pipelines. Note: Do not use development stages in production pipelines. You can use the following stages when developing or testing...","datacollector/UserGuide/Pipeline_Design/DroppingUnwantedRecords.html@@@Dropping Unwanted Records@@@You can drop records from the pipeline at each stage by defining required fields or preconditions for a record to enter a stage...","datacollector/UserGuide/Pipeline_Design/ErrorHandling.html@@@Error Record Handling@@@You can configure error record handling at a stage level and at a pipeline level. You can also specify the version of the record to use as the basis for the error record. When an error occurs as a...","datacollector/UserGuide/Pipeline_Design/FieldAttributes.html@@@Field Attributes@@@Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed...","datacollector/UserGuide/Pipeline_Design/PipelineDesign_title.html@@@Pipeline Concepts and Design@@@...","datacollector/UserGuide/Pipeline_Design/RecordHeaderAttributes.html@@@Record Header Attributes@@@Record header attributes are attributes in record headers that you can use in pipeline logic, as needed. Some stages create record header attributes for a particular purpose. For example, CDC-enabled...","datacollector/UserGuide/Pipeline_Design/SDCRecordFormat.html@@@SDC Record Data Format@@@SDC Record is a proprietary data format that Data Collector uses to generate error records. Data Collector can also use the data format to read and write data...","datacollector/UserGuide/Pipeline_Design/TestOrigin.html@@@Test Origin for Preview@@@A test origin can provide test data for data preview to aid in pipeline development.\n        In Control Hub, you can also use test origins when developing pipeline fragments. Test origins are not used when running a pipeline...","datacollector/UserGuide/Pipeline_Design/What_isa_Pipeline.html@@@What is a Pipeline?@@@A pipeline describes the flow of data from the origin system to destination systems and defines how to transform the data along the way...","datacollector/UserGuide/Pipeline_Maintenance/PipelineMaintenance_title.html@@@Pipeline Maintenance@@@If you defined runtime parameters for a pipeline, you can specify the parameter values to use when you start the pipeline...","datacollector/UserGuide/Pipeline_Monitoring/PipelineMonitoring_title.html@@@Pipeline Monitoring@@@When the Data Collector runs a pipeline, you can view real-time statistics about the pipeline, examine a sample of the data being processed, and create rules and alerts...","datacollector/UserGuide/Processors/Aggregator-SampleEvents.html@@@CRUD Operation Header Attributes@@@When generating records, the Oracle CDC Client origin specifies the operation type in the sdc.operation.type and oracle.cdc.operation record header attributes. The SQL Parser supports the same record...","datacollector/UserGuide/Processors/Aggregator.html@@@Aggregator@@@The Aggregator processor performs one or more aggregations within a window of time. The Aggregator processor displays the results in Monitor mode and can write the results to event records. The...","datacollector/UserGuide/Processors/Base64Decoder.html@@@Base64 Field Decoder@@@The Base64 Field Decoder decodes Base64 encoded data to binary data. Use the processor to decode Base64 encoded data before evaluating data in the field...","datacollector/UserGuide/Processors/Base64Encoder.html@@@Base64 Field Encoder@@@The Base64 Field Encoder encodes binary data using Base64. Use the processor to encode binary data that must be sent over channels that expect ASCII data...","datacollector/UserGuide/Processors/DataParser.html@@@Data Parser@@@The Data Parser processor allows you to parse supported data formats embedded in a field. You can parse NetFlow embedded in a byte array field or syslog messages embedded in a string field...","datacollector/UserGuide/Processors/Delay.html@@@Delay@@@The Delay processor delays passing a batch to rest of the pipeline by the specified number of milliseconds. Use the Delay processor to delay pipeline processing. To limit the volume of data that can...","datacollector/UserGuide/Processors/Expression.html@@@Expression Evaluator@@@The Expression Evaluator performs calculations and writes the results to new or existing fields. You can also use the Expression Evaluator to add or modify record header attributes and field...","datacollector/UserGuide/Processors/FieldFlattener.html@@@Field Flattener@@@The Field Flattener flattens list and map fields. The processor can flatten the entire record to produce a record with no nested fields. Or it can flatten specific list or map fields. Use the Field...","datacollector/UserGuide/Processors/FieldHasher.html@@@Field Hasher@@@The Field Hasher uses an algorithm to encode data. Use Field Hasher to encode highly-sensitive data. For example, you might use Field Hasher to encode social security or credit card numbers...","datacollector/UserGuide/Processors/FieldMasker.html@@@Field Masker@@@The Field Masker masks string values based on the selected mask type. You can use variable-length, fixed-length, custom, or regular expression masks. Custom masks can reveal part of the string value...","datacollector/UserGuide/Processors/FieldMerger.html@@@Field Merger@@@The Field Merger merges one or more fields in a record to a different location in the record. Use only for records with a list or map structure...","datacollector/UserGuide/Processors/FieldOrder.html@@@Field Order@@@The Field Order processor orders fields in a map or list-map field and outputs the fields into a list-map or list root field...","datacollector/UserGuide/Processors/FieldRemover.html@@@Field Remover@@@The Field Remover removes fields from records. Use the Field Remover to discard field data that you do not need in the pipeline. You configure the Field Remover to complete one of the following...","datacollector/UserGuide/Processors/FieldRenamer.html@@@Field Renamer@@@Use the Field Renamer to rename fields in a record. You can specify individual fields to rename or use regular expressions to rename sets of fields...","datacollector/UserGuide/Processors/FieldReplacer.html@@@Field Replacer@@@The Field Replacer replaces values in fields with nulls or with new values. Use the Field Replacer to update values or to replace invalid values...","datacollector/UserGuide/Processors/FieldSplitter.html@@@Field Splitter@@@The Field Splitter splits string data based on a regular expression and passes the separated data to new fields. Use the Field Splitter to split complex string values into logical components...","datacollector/UserGuide/Processors/FieldTypeConverter.html@@@Field Type Converter@@@The Field Type Converter converts the data types of fields to compatible data types. You might use the Field Type Converter to convert the data types of fields before performing calculations. You can also use the Field Type Converter to change the scale of decimal data...","datacollector/UserGuide/Processors/FieldZip.html@@@Field Zip@@@The Field Zip processor merges list data from two fields into a single field. You can use the Field Zip processor to merge two List fields or List-Map fields. Use the processor to merge related lists...","datacollector/UserGuide/Processors/GeoIP.html@@@Geo IP@@@The Geo IP processor is a lookup processor that can return geolocation and IP intelligence information for a specified IP address. The Geo IP processor uses MaxMind GeoIP2 database files for the...","datacollector/UserGuide/Processors/Groovy.html@@@Groovy Evaluator@@@You can choose the processing mode that the Groovy Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode...","datacollector/UserGuide/Processors/HBaseLookup.html@@@HBase Lookup@@@The HBase Lookup processor performs key-value lookups in HBase and passes the lookup values to fields. Use the HBase Lookup to enrich records with additional data...","datacollector/UserGuide/Processors/HTTPClient.html@@@HTTP Client@@@The HTTP Client processor sends requests to an HTTP resource URL and writes the results to a field in the record. You can use the HTTP Client processor to perform a range of standard requests or you...","datacollector/UserGuide/Processors/HiveMetadata.html@@@Hive Metadata@@@The Hive Metadata processor queries Hive for information and caches the results. When possible, it uses the cache for record comparison to avoid unnecessary Hive queries...","datacollector/UserGuide/Processors/JDBCLookup.html@@@JDBC Lookup@@@The JDBC Lookup processor uses a JDBC connection to perform lookups in a database table and pass the lookup values to fields. Use the JDBC Lookup to enrich records with additional data...","datacollector/UserGuide/Processors/JDBCMetadata.html@@@PostgreSQL Metadata@@@When you configure the schemas and tables where records should be written, you can use the actual schema and table names or expressions that resolve to the schemas and tables to use...","datacollector/UserGuide/Processors/JDBCTee.html@@@JDBC Tee@@@The JDBC Tee processor uses a JDBC connection to write data to a database table, and then pass generated database column values to fields. Use the JDBC Tee to write some or all record fields to a...","datacollector/UserGuide/Processors/JSONGenerator.html@@@JSON Generator@@@The JSON Generator serializes data in a field to a JSON-encoded string. You can serialize data from List, Map, or List-Map fields. When you configure the processor, you select the field that you want...","datacollector/UserGuide/Processors/JSONParser.html@@@JSON Parser@@@Configure a JSON Parser to parse a JSON object in a String field...","datacollector/UserGuide/Processors/JavaScript.html@@@JavaScript Evaluator@@@You can choose the processing mode that the JavaScript Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode...","datacollector/UserGuide/Processors/Jython.html@@@Jython Evaluator@@@You can choose the processing mode that the Jython Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode...","datacollector/UserGuide/Processors/KuduLookup.html@@@Kudu Lookup@@@The Kudu Lookup processor performs lookups in a Kudu table and passes the lookup values to fields. Use the Kudu Lookup to enrich records with additional data...","datacollector/UserGuide/Processors/ListPivoter.html@@@Field Pivoter@@@Configure a Field Pivoter to pivot data in a list, map, or list-map field and generate a record for each item in the field...","datacollector/UserGuide/Processors/LogParser.html@@@Log Parser@@@When you use Log Parser to parse log data, you define the format of the log files to be read...","datacollector/UserGuide/Processors/Processors_overview.html@@@Processors@@@A processor stage represents a type of data processing that you want to perform. You can use as many processors in a pipeline as you need. You can use different processors based on the execution mode...","datacollector/UserGuide/Processors/Processors_title.html@@@Processors@@@...","datacollector/UserGuide/Processors/RDeduplicator.html@@@Record Deduplicator@@@The Record Deduplicator evaluates records for duplicate data and routes data to two streams -  one for unique records and one for duplicate records. Use the Record Deduplicator to discard duplicate data or route duplicate data through different processing logic...","datacollector/UserGuide/Processors/RedisLookup.html@@@Redis Lookup@@@The Redis Lookup processor performs key-value lookups in Redis and passes the lookup values to fields. Use the Redis Lookup to enrich records with additional data...","datacollector/UserGuide/Processors/SQLParser.html@@@SQL Parser@@@The SQL Parser parses a SQL query in a string field. When parsing a query, the processor generates fields based on the fields defined in the SQL query and specifies the CRUD operation, table, and...","datacollector/UserGuide/Processors/SalesforceLookup.html@@@Salesforce Lookup@@@The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data...","datacollector/UserGuide/Processors/SchemaGenerator.html@@@Schema Generator@@@The Schema Generator processor generates a schema based on the structure of a record and writes the schema into a record header attribute. The Schema Generator generates Avro schemas at this time. You...","datacollector/UserGuide/Processors/Spark.html@@@Spark Evaluator@@@The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop...","datacollector/UserGuide/Processors/StaticLookup.html@@@Static Lookup@@@The Static Lookup processor performs lookups of key-value pairs that are stored in local memory and passes the lookup values to fields. Use the Static Lookup to store String values in memory that the pipeline can look up at runtime to enrich records with additional data...","datacollector/UserGuide/Processors/StreamSelector.html@@@Stream Selector@@@The Stream Selector passes data to streams based on conditions. Define a condition for each stream of data that you want to create. The Stream Selector uses a default stream to pass records that do not match user-defined conditions...","datacollector/UserGuide/Processors/ValueReplacer.html@@@Value Replacer (Deprecated)@@@The Value Replacer replaces values in fields. However, the Value Replacer is now deprecated and will be removed in a future release. We recommend using the Field Replacer processor. You can use the...","datacollector/UserGuide/Processors/WholeFileTransformer.html@@@Whole File Transformer@@@The Whole File Transformer processor transforms fully written Avro files to highly efficient, columnar Parquet files. Use the Whole File Transformer in a pipeline that reads Avro files as whole files and writes the transformed Parquet files as whole files...","datacollector/UserGuide/Processors/XMLFlattener.html@@@XML Flattener@@@Configure an XML Flattener to flatten XML data embedded in a string field...","datacollector/UserGuide/Processors/XMLParser.html@@@XML Parser@@@Configure an XML Parser to parse XML data in a string field...","datacollector/UserGuide/RPC_Pipelines/SDC_RPCpipelines_title.html@@@SDC RPC Pipelines@@@Data Collector Remote Protocol Call pipelines, a.k.a. SDC RPC pipelines, are a set of StreamSets pipelines that pass data from one pipeline to another without writing to an intermediary system...","datacollector/UserGuide/Troubleshooting/Troubleshooting_title.html@@@Troubleshooting@@@Informational and error messages display in different locations based on the type of information: Pipeline configuration issues The Data Collector UI provides guidance and error details as follows...","datacollector/UserGuide/Tutorial/BasicTutorial.html@@@Basic Tutorial@@@The basic tutorial creates a pipeline that reads a file from a directory, processes the data in two branches, and writes all data to a file system. You&apos;ll use data preview to help configure the pipeline, and you&apos;ll create a data alert and run the pipeline...","datacollector/UserGuide/Tutorial/BeforeYouBegin.html@@@Before You Begin@@@Before you start this tutorial, you&apos;ll need to do a few things: Download sample data. You can download sample data from the following location...","datacollector/UserGuide/Tutorial/ExtendedTutorial.html@@@Extended Tutorial@@@The extended tutorial builds on the basic tutorial, using an additional set of stages to perform some data transformations and write to the Trash development destination. We&apos;ll also use data preview to test stage configuration...","datacollector/UserGuide/Tutorial/Overview.html@@@Tutorial Overview@@@This tutorial walks through creating and running a pipeline. You can download sample data so you can perform data preview, run the completed pipeline, and monitor the results...","datacollector/UserGuide/Tutorial/Tutorial-title.html@@@Tutorial@@@...","datacollector/UserGuide/Upgrade/CMUpgrade.html@@@Upgrade an Installation with Cloudera Manager@@@In Data Collector, stop all running pipelines...","datacollector/UserGuide/Upgrade/PostUpgrade.html@@@Post Upgrade Tasks@@@In some situations, you must complete tasks within Data Collector or your Control Hub on-premises installation after you upgrade...","datacollector/UserGuide/Upgrade/PreUpgrade.html@@@Pre Upgrade Tasks@@@In some situations, you must complete tasks before you upgrade...","datacollector/UserGuide/Upgrade/RPM.html@@@Upgrade an Installation from the RPM Package@@@When you upgrade an installation from the RPM package, the new version uses the default configuration, data, log, and resource directories. If the previous version used the default directories, the new version has access to the files created in the previous version...","datacollector/UserGuide/Upgrade/Tarball.html@@@Upgrade an Installation from the Tarball@@@Stop all pipelines and then shut down the previous version of Data Collector...","datacollector/UserGuide/Upgrade/Upgrade-ExternalSystems.html@@@Working with Upgraded External Systems@@@When an external system is upgraded to a new version, you can continue to use existing Data Collector pipelines that connected to the previous version of the external system. You simply configure the pipelines to work with the upgraded system...","datacollector/UserGuide/Upgrade/Upgrade.html@@@Upgrade@@@You can upgrade a previous version of Data Collector to a new version. You can upgrade an installation from the tarball, an installation from the RPM package, or an installation with Cloudera Manager...","datacollector/UserGuide/Upgrade/UpgradeTroubleshooting.html@@@Troubleshooting an Upgrade@@@Use the following tips for help with upgrades: After upgrading a Data Collector that is registered with StreamSets Control Hub , the Data Collector fails to start with the following error about a...","datacollector/UserGuide/Upgrade/Upgrade_title.html@@@Upgrade@@@...","datacollector/UserGuide/WhatsNew/WhatsNew_Title.html@@@What&apos;s New@@@Data Collector version 3.4.0 includes the following new features and enhancements: Origins New PostgreSQL CDC Client origin - Use the PostgreSQL CDC Client origin to process change data capture..."];
});