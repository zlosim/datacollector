fil = new Array();
fil["0"]= "Administration/Administration_title.html@@@Administration@@@The system command provides subcommands to register and unregister the Data Collector with DPM...";
fil["1"]= "Alerts/RulesAlerts_title.html@@@Rules and Alerts@@@You can define the email addresses to receive metric and data alerts. When an alert triggers an email, the Data Collector sends an email to every address in the list...";
fil["2"]= "Apx-DataFormats/DataFormat_Title.html@@@Data Formats by Stage@@@The following table lists the data formats supported by each destination...";
fil["3"]= "Apx-GrokPatterns/GrokPatterns_title.html@@@Grok Patterns@@@You can use the grok patterns in this appendix to define the structure of log data...";
fil["4"]= "Apx-RegEx/RegEx-Title.html@@@Regular Expressions@@@Though generally not required, you can use Java-based regular expressions at various locations within a pipeline to define, search for, or manipulate strings...";
fil["5"]= "Cluster_Mode/ClusterPipelines_title.html@@@Cluster Pipelines@@@Complete the following steps to configure a cluster pipeline to read from MapR in cluster streaming mode...";
fil["6"]= "Configuration/Authentication.html@@@User Authentication@@@If your organization does not use LDAP, configure Data Collector to use the default file-based authentication...";
fil["7"]= "Configuration/Config_title.html@@@Configuration@@@...";
fil["8"]= "Configuration/CredentialStores.html@@@Credential Stores@@@Use the credential:get() or credential:getWithOptions() function in pipeline stage properties to retrieve credential values from Vault...";
fil["9"]= "Configuration/CustomStageLibraries.html@@@Custom Stage Libraries@@@If you develop custom stages, store the stage libraries in a local directory external to the Data Collector installation directory. Use an external directory to enable use of the custom stage...";
fil["10"]= "Configuration/DCConfig.html@@@Data Collector Configuration@@@You can customize Data Collector by editing the Data Collector configuration file, sdc.properties...";
fil["11"]= "Configuration/DCEnvironmentConfig.html@@@Data Collector Environment Configuration@@@You can edit the Data Collector environment configuration file to configure the path to JAR files to be added to the Data Collector root classloader...";
fil["12"]= "Configuration/ExternalLibs.html@@@Install External Libraries@@@To manually install external libraries, use the required procedure for your installation type...";
fil["13"]= "Configuration/JMXMetrics-EnableExternalTools.html@@@Enabling External JMX Tools@@@You can view the Data Collector JMX metrics in external tools. The Data Collector JMX metric names all begin with &quot;sdc.pipeline.&quot...";
fil["14"]= "Configuration/PublishMetadata.html@@@Publishing Metadata to Cloudera Navigator (Beta)@@@If you use Cloudera Manager, you can configure Data Collector to publish metadata about running pipelines to Cloudera Navigator. You can then use Cloudera Navigator to explore the pipeline metadata, including viewing lineage diagrams of the metadata...";
fil["15"]= "Configuration/RolesandPermissions.html@@@Roles and Permissions@@@Data Collector allows you to assign roles and pipeline permissions to users and groups. Roles, such as Creator or Manager, enable users to perform different Data Collector tasks. Each user needs at...";
fil["16"]= "Configuration/Vault-Overview.html@@@Accessing Vault Secrets with Vault Functions (Deprecated)@@@Data Collector can use Vault functions to access information, a.k.a. secrets , stored in Hashicorp Vault. However, the Vault functions are now deprecated and will be removed in a future release. For...";
fil["17"]= "DPM/AggregatedStatistics.html@@@Pipeline Statistics@@@You can configure a pipeline to write statistics after the Data Collector has been registered with DPM...";
fil["18"]= "DPM/DPM.html@@@Meet Dataflow Performance Manager@@@As a DevOps or site reliability engineer, you can master your day-to-day operations by defining data SLAs (service level agreements) to ensure that incoming data meets business requirements for availability and accuracy...";
fil["19"]= "DPM/DPMConfiguration.html@@@DPM Configuration@@@You can customize how a Data Collector works with DPM by editing the DPM configuration file, dpm.properties...";
fil["20"]= "DPM/DPM_title.html@@@Dataflow Performance Manager@@@...";
fil["21"]= "DPM/OrgUserAccount.html@@@Request a DPM Organization and User Account@@@To register a Data Collector with DPM,\n        you must have a DPM user account within an organization...";
fil["22"]= "DPM/PipelineManagement.html@@@Pipeline Management with DPM@@@If you develop pipelines in a Data Collector that is not registered with DPM, you can export the pipelines and then import the pipelines into DPM. When a Data Collector is registered with DPM, it&apos;s simplest to publish the pipelines directly to DPM...";
fil["23"]= "DPM/RegisterSDCwithDPM.html@@@Register Data Collector with DPM@@@If you register a Data Collector that is installed on a cloud-computing platform such as Amazon Elastic Compute Cloud (EC2),\n        configure the Data Collector to use a publicly accessible URL...";
fil["24"]= "DPM/UnregisterSDCwithDPM.html@@@Unregister Data Collector from DPM@@@You can unregister a Data Collector from DPM using the Data Collector command line interface...";
fil["25"]= "DPM/WorkingWithDPM.html@@@Working with DPM@@@Before you can work with DPM, you must have a DPM organization and user account...";
fil["26"]= "Data_Formats/DataFormats-Overview.html@@@Data Formats Overview@@@Data formats - such as Avro, JSON, and log - are methods to encode data that adhere to generally accepted specifications. The way that stages process data can be similar based on the stage type and...";
fil["27"]= "Data_Formats/DataFormats-Title.html@@@Data Formats@@@...";
fil["28"]= "Data_Formats/DelimitedDataRootFieldTypes.html@@@Delimited Data Root Field Type@@@When reading delimited data, Data Collector can create records that use the list or list-map root field type. When Data Collector creates records for delimited data, it creates a single root field of...";
fil["29"]= "Data_Formats/NetFlow_Overview.html@@@NetFlow Data Processing@@@You can use Data Collector to process NetFlow 5 and NetFlow 9 data. When processing NetFlow 5 data, Data Collector processes flow records based on information in the packet header. Data Collector...";
fil["30"]= "Data_Formats/Protobuf-Prerequisites.html@@@Protobuf Data Format Prerequisites@@@Perform the following prerequisites before reading or writing protobuf data...";
fil["31"]= "Data_Formats/TextCDelim.html@@@Text Data Format with Custom Delimiters@@@By default, the text data format creates records based on line breaks, creating a record for each line of text. You can configure origins to create records based on custom delimiters. Use custom...";
fil["32"]= "Data_Formats/WholeFile.html@@@Whole File Data Format@@@You can use the whole file data format to move entire files from an origin system to a destination system. With the whole file data format, you can transfer any type of file. You cannot perform...";
fil["33"]= "Data_Formats/WritingXML.html@@@Writing XML Data@@@When writing XML data, destinations create a valid XML document for each record. The destination requires the record to have a single root field that contains the rest of the record data. When writing...";
fil["34"]= "Data_Formats/XMLDFormat.html@@@Reading and Processing XML Data@@@You can parse XML documents from an origin system with an origin enabled for the XML data format. You can also parse XML documents in a field in a Data Collector record with the XML Parser processor...";
fil["35"]= "Data_Preview/DataPreview_Title.html@@@Data Preview@@@In data preview, you can edit stage properties to see how the changes affect preview data. For example, you might edit the expression in an Expression Evaluator to see how the expression alters data...";
fil["36"]= "Destinations/AmazonS3.html@@@Amazon S3@@@When Data Collector writes data to an Amazon S3 destination, it must pass credentials to Amazon Web Services...";
fil["37"]= "Destinations/AzureEventHubProducer.html@@@Azure Event Hub Producer@@@The Azure Event Hub Producer writes data to Microsoft Azure Event Hub. To write to Microsoft Azure Data Lake Store, use the Azure Data Lake Store destination. To write to Microsoft Azure IoT Hub, use...";
fil["38"]= "Destinations/AzureIoTHub.html@@@Azure IoT Hub Producer@@@Data Collector functions as a simulated device that sends messages to Azure IoT Hub. Before you configure the Azure IoT Hub Producer destination, register Data Collector as a device in your IoT hub...";
fil["39"]= "Destinations/BigQuery.html@@@Google BigQuery@@@When configured to use the Google Cloud service account credentials file, the destination checks for the file defined in the destination properties...";
fil["40"]= "Destinations/Bigtable.html@@@Google Bigtable@@@The time basis determines the timestamp value added for each column written to Google Cloud Bigtable...";
fil["41"]= "Destinations/Cassandra.html@@@Cassandra@@@The Cassandra destination writes data to a Cassandra cluster...";
fil["42"]= "Destinations/CoAPClient.html@@@CoAP Client@@@Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Client destination writes data to a CoAP endpoint. Use the destination to send requests to a CoAP resource URL...";
fil["43"]= "Destinations/DataLakeStore.html@@@Azure Data Lake Store@@@The Azure Data Lake Store destination writes data to the Microsoft Azure Data Lake Store. You can use the Azure Data Lake Store destination in standalone and cluster batch pipelines. The destination...";
fil["44"]= "Destinations/Destinations-title.html@@@Destinations@@@...";
fil["45"]= "Destinations/Destinations_overview.html@@@Destinations@@@A destination stage represents the target for a pipeline. You can use one or more destinations in a pipeline...";
fil["46"]= "Destinations/Elasticsearch.html@@@Elasticsearch@@@When appropriate, you can specify an expression that defines the document ID. When you do not specify an expression, Elasticsearch generates IDs for each document...";
fil["47"]= "Destinations/Flume.html@@@Flume@@@The Flume destination writes data to a Flume source. When you write data to Flume, you pass data to a Flume client. The Flume client passes data to hosts based on client configuration properties...";
fil["48"]= "Destinations/GCS.html@@@Google Cloud Storage@@@The Google Cloud Storage destination writes data to objects in Google Cloud Storage. You can use other destinations to write to Google BigQuery , Google Bigtable , and Google Pub/Sub . The destination...";
fil["49"]= "Destinations/HBase.html@@@HBase@@@The time basis determines the timestamp value added for each column written to HBase...";
fil["50"]= "Destinations/HTTPClient.html@@@HTTP Client@@@To use OAuth 2 authorization to write to Google service accounts, configure HTTP Client to use no authentication and the JSON Web Tokens grant...";
fil["51"]= "Destinations/HadoopFS-destination.html@@@Hadoop FS@@@The Hadoop FS destination writes data to the Hadoop Distributed File System (HDFS). You can write the data to HDFS as flat files or Hadoop sequence files. You can also use the whole file data format...";
fil["52"]= "Destinations/Hive.html@@@Hive Streaming@@@The Hive Streaming destination writes data to Hive tables stored in the ORC (Optimized Row Columnar) file format...";
fil["53"]= "Destinations/HiveMetastore.html@@@Hive Metastore@@@You must configure Hive Metastore to use Hive and Hadoop configuration files and individual properties...";
fil["54"]= "Destinations/InfluxDB.html@@@InfluxDB@@@The InfluxDB destination writes data to an InfluxDB database...";
fil["55"]= "Destinations/JDBCProducer.html@@@JDBC Producer@@@Configure the JDBC Producer to use JDBC to write data to a database table...";
fil["56"]= "Destinations/JMSProducer.html@@@JMS Producer@@@Before you use the JMS Producer, install the JMS drivers for the implementation that you are using...";
fil["57"]= "Destinations/KProducer.html@@@Kafka Producer@@@The partition strategy determines how to write data to Kafka partitions. You can use a partition strategy to balance the work load or to write data semantically...";
fil["58"]= "Destinations/KinFirehose.html@@@Kinesis Firehose@@@The Kinesis Firehose destination writes data to an existing delivery stream in Amazon Kinesis Firehose. Before using the Kinesis Firehose destination, use the AWS Management Console to create a delivery stream to an Amazon S3 bucket or Amazon Redshift table...";
fil["59"]= "Destinations/KinProducer.html@@@Kinesis Producer@@@When Data Collector writes data to a Kinesis Producer destination, it must pass credentials to Amazon Web Services...";
fil["60"]= "Destinations/KineticaDB.html@@@KineticaDB@@@The KineticaDB destination writes data to a table in a Kinetica cluster using the Kinetica bulk inserter. When you configure the KineticaDB destination, you specify the URL for the Kinetica head node...";
fil["61"]= "Destinations/Kudu.html@@@Kudu@@@The Kudu destination writes data to a Kudu cluster...";
fil["62"]= "Destinations/LocalFS.html@@@Local FS@@@Use the Local FS destination to write records to files in a local file system. When you configure a Local FS destination, you can define a directory template and time basis to determine the output...";
fil["63"]= "Destinations/MQTTPublisher.html@@@MQTT Publisher@@@The MQTT Publisher destination writes messages to a single topic on the MQTT broker. Any MQTT client subscribed to that topic receives the messages. A topic is a string that the broker uses to filter messages for each connected client...";
fil["64"]= "Destinations/MapRDB.html@@@MapR DB@@@You can use Kerberos authentication to connect to MapR DB. When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to MapR DB. By default, Data Collector uses the user account who started it to connect...";
fil["65"]= "Destinations/MapRDBJSON.html@@@MapR DB JSON@@@You configure the MapR DB JSON destination to process row keys as string or binary data. If necessary, the MapR DB JSON destination converts the data type of the row key field and then writes the converted value to the _id field in the JSON document...";
fil["66"]= "Destinations/MapRFS.html@@@MapR FS@@@You can use Kerberos authentication to connect to MapR. When you use Kerberos authentication, the Data Collector uses the Kerberos principal and keytab to connect to MapR. By default, Data Collector uses the user account who started it to connect...";
fil["67"]= "Destinations/MapRStreamsProd.html@@@MapR Streams Producer@@@The MapR Streams Producer destination writes messages to MapR Streams...";
fil["68"]= "Destinations/MongoDB.html@@@MongoDB@@@The MongoDB destination writes data to MongoDB. To write to MongoDB, records must include a CRUD operation record header attribute. The CRUD operation header attribute indicates the operation to...";
fil["69"]= "Destinations/PubSubPublisher.html@@@Google Pub/Sub Publisher@@@When configured to use the Google Cloud service account credentials file, the destination checks for the file defined in the destination properties...";
fil["70"]= "Destinations/RabbitMQ.html@@@RabbitMQ Producer@@@RabbitMQ Producer writes AMQP messages to a single RabbitMQ queue...";
fil["71"]= "Destinations/Redis.html@@@Redis@@@The Redis destination writes data to Redis...";
fil["72"]= "Destinations/SDC_RPCdest.html@@@SDC RPC@@@The SDC RPC destination compresses data by default when passing data to an SDC RPC origin. When necessary, you can disable compression in the destination...";
fil["73"]= "Destinations/Salesforce.html@@@Salesforce@@@The Salesforce destination writes data to Salesforce objects...";
fil["74"]= "Destinations/Solr.html@@@Solr@@@You can use Kerberos authentication to connect to a Solr node or cluster. When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to Solr...";
fil["75"]= "Destinations/ToError.html@@@To Error@@@The To Error destination passes records to the pipeline for error handling. Use the To Error destination to send a stream of records to pipeline error handling...";
fil["76"]= "Destinations/Trash.html@@@Trash@@@The Trash destination discards records. Use the Trash destination as a visual representation of records discarded from the pipeline. Or, you might use the Trash destination during development as a temporary placeholder...";
fil["77"]= "Destinations/WaveAnalytics.html@@@Einstein Analytics@@@The Einstein Analytics destination typically creates multiple datasets, based on the configured dataset wait time. You can optionally configure the destination to use a Einstein Analytics dataflow to combine multiple datasets together...";
fil["78"]= "Destinations/WebSocketClient.html@@@WebSocket Client@@@The WebSocket Client destination writes data to a WebSocket endpoint. Use the destination to send data to a WebSocket resource URL...";
fil["79"]= "Edge_Mode/EdgePipelines_title.html@@@Edge Pipelines@@@When you first get started with SDC Edge, the easiest way to deploy pipelines to SDC Edge is to use Data Collector to download the Data Collector Edge executable along with your designed edge pipelines. After SDC Edge is installed, you&apos;ll need to export new and updated edge pipelines from Data Collector and move them to SDC Edge installed on the edge device...";
fil["80"]= "Event_Handling/EventFramework-Title.html@@@Dataflow Triggers (a.k.a. Event Framework)@@@Dataflow triggers are instructions for the event framework to kick off tasks in response to events that occur in the pipeline. For example, you can use dataflow triggers to start a MapReduce job after...";
fil["81"]= "Executors/AmazonS3.html@@@Amazon S3 Executor@@@When Data Collector uses the Amazon S3 executor, it must pass credentials to Amazon Web Services...";
fil["82"]= "Executors/Email.html@@@Email Executor@@@The Email executor sends the configured email to the specified recipients upon receiving an event. You can also configure the executor to send an email based on a condition, such as the arrival of a...";
fil["83"]= "Executors/Executors-overview.html@@@Executors@@@An executor stage triggers a task when it receives an event. Executors do not write or store events. Use executors as part of a dataflow trigger in an event stream to perform event-driven...";
fil["84"]= "Executors/Executors-title.html@@@Executors@@@...";
fil["85"]= "Executors/HDFSMetadata.html@@@HDFS File Metadata Executor@@@The HDFS File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in HDFS or a local file system each time it receives an event. You cannot perform multiple...";
fil["86"]= "Executors/HiveQuery.html@@@Hive Query Executor@@@The Hive Query executor connects to Hive or Impala and performs one or more user-defined Hive or Impala queries each time it receives an event record. Use the Hive Query executor as part of an event...";
fil["87"]= "Executors/JDBCQuery.html@@@JDBC Query Executor@@@The JDBC Query executor connects through JDBC to a database and performs a user-defined SQL query each time it receives an event record. Use the JDBC Query executor as part of an event stream in the...";
fil["88"]= "Executors/MapRFSFileMeta.html@@@MapR FS File Metadata Executor@@@The MapR FS File Metadata executor changes file metadata, creates an empty file, or removes a file or directory in MapR FS each time it receives an event. You cannot perform multiple tasks in the same...";
fil["89"]= "Executors/MapReduce.html@@@MapReduce Executor@@@The MapReduce executor starts a MapReduce job in HDFS or MapR FS each time it receives an event record. Use the MapReduce executor as part of an event stream. You can use the MapReduce executor to...";
fil["90"]= "Executors/PipelineFinisher.html@@@Pipeline Finisher Executor@@@When it receives an event, the Pipeline Finisher executor stops a pipeline and transitions it to a Finished state. This allows the pipeline to complete all expected processing before stopping. Use the...";
fil["91"]= "Executors/Shell.html@@@Shell Executor@@@The Shell executor executes a shell script every time it receives an event. Use the Shell executor as part of an event stream. When you configure the executor, you define the shell script that you...";
fil["92"]= "Executors/Spark.html@@@Spark Executor@@@The Spark executor starts a Spark application each time it receives an event. You can use the Spark executor with Spark on YARN or Spark on Databricks. The executor is not compatible with Spark on...";
fil["93"]= "Expression_Language/Constants.html@@@Constants@@@The expression language provides constants for use in expressions...";
fil["94"]= "Expression_Language/DateTimeVariables.html@@@Datetime Variables@@@The expression language provides datetime variables for use in expressions...";
fil["95"]= "Expression_Language/ExpressionLanguage_overview.html@@@Expression Language@@@The StreamSets expression language enables you to create expressions that evaluate or modify data. The StreamSets expression language is based on the JSP 2.0 expression language. Use the expression...";
fil["96"]= "Expression_Language/ExpressionLanguage_title.html@@@Expression Language@@@...";
fil["97"]= "Expression_Language/Functions.html@@@Functions@@@Use time functions to return the current time or to transform datetime data...";
fil["98"]= "Expression_Language/Literals.html@@@Literals@@@The expression language includes the following literals...";
fil["99"]= "Expression_Language/Operators.html@@@Operators@@@The precedence of operators highest to lowest, left to right is as follows...";
fil["100"]= "Expression_Language/ReservedWords.html@@@Reserved Words@@@The following words are reserved for the expression language and should not be used as identifiers...";
fil["101"]= "Getting_Started/GettingStarted_Title.html@@@Getting Started@@@Data Collector displays a list of all available pipelines and related information on the Home page. You can select a category of pipelines, such as Running Pipelines, to view a subset of all available pipelines...";
fil["102"]= "Glossary/Glossary_title.html@@@Glossary@@@batch A set of records that passes through a pipeline. Data Collector processes data in batches. CDC-enabled origin An origin that can process changed data and place CRUD operation information in the...";
fil["103"]= "Hive_Drift_Solution/HiveDriftSolution_title.html@@@Drift Synchronization Solution (a.k.a. Hive Drift Solution)@@@Now to process the metadata records - and to automatically create and update Parquet tables in Hive - you need the Hive Metastore destination...";
fil["104"]= "Installation/AddtionalStageLibs.html@@@Install Additional Stage Libraries@@@Install additional stage libraries to use stages that are not included in the core installation of Data Collector . This is an optional step, but generally you&apos;ll want to install additional stage...";
fil["105"]= "Installation/CMInstall-Overview.html@@@Installation with Cloudera Manager@@@When administering Data Collector with Cloudera Manager, configure all Data Collector configuration properties and environment variables through Cloudera Manager...";
fil["106"]= "Installation/CoreInstall_Overview.html@@@Core Installation@@@To install the core version of Data Collector,\n        download the core tarball. After you perform the core installation and launch, install individual stage libraries as needed...";
fil["107"]= "Installation/CreateAnotherDC.html@@@Creating Another Data Collector Instance@@@You can create another instance of a Data Collector tarball or RPM installation on the same machine with the create-dc command.\n        The additional Data Collector instance uses the same configuration as the original Data Collector instance. You can modify the configuration properties as needed...";
fil["108"]= "Installation/FullInstall_ServiceStart.html@@@Full Installation and Launch (Service Start)@@@You can install the Data Collector tarball and start it as a service for supported operating systems that use the systemd init system - including CentOS 7, Red Hat Enterprise Linux 7, or Ubuntu 16.04 LTS...";
fil["109"]= "Installation/Install_title.html@@@Installation@@@...";
fil["110"]= "Installation/InstallationAndConfig.html@@@Installation@@@Data Collector requires a large number of file descriptors to work correctly with all stages. Most operating systems provide a configuration to limit the number of files a process or a user can open. The default values are usually less than the Data Collector requirement of 32768 file descriptors...";
fil["111"]= "Installation/Installing_the_DC-Docker.html@@@Run Data Collector from Docker@@@You can run the Data Collector image from Docker Hub...";
fil["112"]= "Installation/Installing_the_DC.html@@@Full Installation and Launch (Manual Start)@@@You can install the full Data Collector tarball and start it manually on all supported operating systems...";
fil["113"]= "Installation/MapR-Prerequisites.html@@@MapR Prerequisites@@@To connect to a secure MapR cluster enabled for wire-level security and username/password authentication, you must configure Data Collector to run as the user account granted access to the cluster in the MapR user or service ticket...";
fil["114"]= "Installation/Uninstall.html@@@Uninstallation@@@You can uninstall a Data Collector instance that was installed from the tarball, from the RPM package, or from Cloudera Manager. To uninstall a Data Collector instance that was installed from the...";
fil["115"]= "Multithreaded_Pipelines/MultithreadedPipelines.html@@@Multithreaded Pipelines@@@A multithreaded pipeline is a pipeline with an origin that supports parallel execution, enabling one pipeline to run in multiple threads. Multithreaded pipelines enable processing high volumes of data...";
fil["116"]= "Origins/AmazonS3.html@@@Amazon S3@@@The Amazon S3 origin uses a buffer to read objects into memory to produce records. The size of the buffer determines the maximum size of the record that can be processed...";
fil["117"]= "Origins/AmazonSQS.html@@@Amazon SQS Consumer@@@When Data Collector reads data from an Amazon SQS Consumer origin, it must pass credentials to Amazon Simple Queue Services...";
fil["118"]= "Origins/AzureEventHub.html@@@Azure Event Hub Consumer@@@The Azure Event Hub Consumer origin reads data from Microsoft Azure Event Hub. The origin can use multiple threads to enable parallel processing of data from a single Azure event hub. Before you use...";
fil["119"]= "Origins/BigQuery.html@@@Google BigQuery@@@The Google BigQuery origin converts the Google BigQuery data types to Data Collector data types...";
fil["120"]= "Origins/CoAPServer.html@@@CoAP Server@@@The CoAP Server origin uses the default values for network configuration properties as implemented by Eclipse Californium. If needed, you can override the default values of these properties...";
fil["121"]= "Origins/Directory.html@@@Directory@@@Configure a Directory origin to read data from files in a directory...";
fil["122"]= "Origins/Elasticsearch.html@@@Elasticsearch@@@The Elasticsearch origin performs parallel processing and enables the creation of a multithreaded pipeline...";
fil["123"]= "Origins/FileTail.html@@@File Tail@@@When you use an origin to read log data, you define the format of the log files to be read...";
fil["124"]= "Origins/GCS.html@@@Google Cloud Storage@@@The Google Cloud Storage origin reads objects stored in Google Cloud Storage. The objects must be fully written and reside in a single bucket. The object names must share a prefix pattern. With the...";
fil["125"]= "Origins/HTTPClient.html@@@HTTP Client@@@The HTTP Client origin processes data differently based on the data format. The origin processes the following types of data...";
fil["126"]= "Origins/HTTPServer.html@@@HTTP Server@@@Configure the HTTP clients to include the HTTP Server application ID in each request...";
fil["127"]= "Origins/HTTPtoKafka.html@@@HTTP to Kafka@@@The HTTP to Kafka origin listens on an HTTP endpoint and writes the contents of all authorized HTTP POST requests directly to Kafka. Use the HTTP to Kafka origin to write large volumes of HTTP POST...";
fil["128"]= "Origins/HadoopFS-origin.html@@@Hadoop FS@@@The Hadoop FS origin reads data from the Hadoop Distributed File System (HDFS) or from other file systems using the Hadoop FileSystem interface. Use this origin only in pipelines configured for cluster batch execution mode...";
fil["129"]= "Origins/JDBCConsumer.html@@@JDBC Query Consumer@@@You can define any type of SQL query for full mode...";
fil["130"]= "Origins/JMS.html@@@JMS Consumer@@@Configure a JMS Consumer origin to read JMS messages...";
fil["131"]= "Origins/KConsumer.html@@@Kafka Consumer@@@Configure a Kafka Consumer to read data from a Kafka cluster...";
fil["132"]= "Origins/KafkaMultiConsumer.html@@@Kafka Multitopic Consumer@@@When you use an origin to read log data, you define the format of the log files to be read...";
fil["133"]= "Origins/KinConsumer.html@@@Kinesis Consumer@@@You can configure the read interval for the Kinesis Consumer. The read interval determines how long Kinesis Consumer waits before requesting additional data from Kinesis shards.\n  By default, the Kinesis Consumer waits one second between requests...";
fil["134"]= "Origins/MQTTSubscriber.html@@@MQTT Subscriber@@@The MQTT Subscriber origin subscribes to topics on an MQTT broker to read messages from the broker. The origin functions as an MQTT client that receives messages, generating a record for each message...";
fil["135"]= "Origins/MapRDBJSON.html@@@MapR DB JSON@@@When the origin converts a JSON document into a record, it includes the _id field of the JSON document in the record. If needed, you can use the Field Remover processor in the pipeline to remove the _id field...";
fil["136"]= "Origins/MapRFS.html@@@MapR FS@@@You can use Kerberos authentication to connect to MapR. When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to MapR.  By default, Data Collector uses the user account who started it to connect...";
fil["137"]= "Origins/MapRStreamsCons.html@@@MapR Streams Consumer@@@You can add custom configuration properties to MapR Streams Consumer. You can use any MapR or Kafka property supported by MapR Streams. For more information, see the MapR Streams documentation...";
fil["138"]= "Origins/MapRStreamsMultiConsumer.html@@@MapR Multitopic Streams Consumer@@@When you use an origin to read log data, you define the format of the log files to be read...";
fil["139"]= "Origins/MapRdbCDC.html@@@MapR DB CDC@@@The MapR DB CDC origin reads changed data from MapR DB that has been written to MapR Streams. The origin can use multiple threads to enable parallel processing of data. You might use this origin to...";
fil["140"]= "Origins/MongoDB.html@@@MongoDB@@@The MongoDB origin reads data from MongoDB. Data Collector generates a record for every MongoDB document. To read change data capture information from the MongoDB Oplog, use the MongoDB Oplog origin...";
fil["141"]= "Origins/MongoDBOplog.html@@@MongoDB Oplog@@@The MongoDB Oplog origin reads entries from MongoDB Oplog. MongoDB stores information about changes to the database in a local capped collection called an Oplog. The Oplog contains information about...";
fil["142"]= "Origins/MultiTableJDBCConsumer.html@@@JDBC Multitable Consumer@@@You can define the initial order that the origin uses to read the tables...";
fil["143"]= "Origins/MySQLBinaryLog.html@@@MySQL Binary Log@@@The binary log file captures all changes made to the MySQL database. If you want the MySQL Binary Log origin to capture changes from a subset of tables, you can configure the origin to include changes from specific tables or to ignore changes from specific tables...";
fil["144"]= "Origins/OPCUAClient.html@@@OPC UA Client@@@The OPC UA Client origin processes data from an OPC UA server. The OPC UA Client origin can poll the server at regular intervals, returning the latest data from all specified nodes. Or it can...";
fil["145"]= "Origins/Omniture.html@@@Omniture@@@The Omniture origin processes JSON website usage reports generated by the Omniture reporting APIs. Omniture is also known as the Adobe Marketing Cloud...";
fil["146"]= "Origins/OracleCDC.html@@@Oracle CDC Client@@@The Oracle CDC Client processes change data capture (CDC) information provided by Oracle LogMiner redo logs. Use Oracle CDC Client to process data from Oracle 11g or 12c. When needed, you can use a...";
fil["147"]= "Origins/Origins_overview.html@@@Origins@@@Some origins allow you to preview raw source data. Preview raw source data when reviewing the data might help with origin configuration...";
fil["148"]= "Origins/Origins_title.html@@@Origins@@@...";
fil["149"]= "Origins/PubSub.html@@@Google Pub/Sub Subscriber@@@The Google Pub/Sub Subscriber origin includes user-defined message attributes in record header attributes when they are available. When the origin processes Avro data, it includes the Avro schema in an avroSchema record header attribute...";
fil["150"]= "Origins/RabbitMQ.html@@@RabbitMQ Consumer@@@Configure a RabbitMQ Consumer to read messages from a RabbitMQ queue...";
fil["151"]= "Origins/Redis.html@@@Redis Consumer@@@When you use an origin to read log data, you define the format of the log files to be read...";
fil["152"]= "Origins/SDCRPCtoKafka.html@@@SDC RPC to Kafka@@@Configure the SDC RPC to Kafka maximum batch request size and Kafka message size properties in relationship to each other and to the maximum message size configured in Kafka...";
fil["153"]= "Origins/SDC_RPCorigin.html@@@SDC RPC@@@The SDC RPC origin enables connectivity between two SDC RPC pipelines. The SDC RPC origin reads data passed from an SDC RPC destination. Use the SDC RPC origin as part of an SDC RPC destination pipeline...";
fil["154"]= "Origins/SFTP.html@@@SFTP/FTP Client@@@If the remote server requires authentication, configure the authentication method that the origin must use to log in to the remote server...";
fil["155"]= "Origins/SQLServerCDC.html@@@SQL Server CDC Client@@@You can define the initial order that the origin uses to read the tables...";
fil["156"]= "Origins/SQLServerChange.html@@@SQL Server Change Tracking@@@You can define the initial order that the origin uses to read the tables...";
fil["157"]= "Origins/Salesforce.html@@@Salesforce@@@The Salesforce origin generates Salesforce field attributes that provide additional information about each field, such as the data type of the Salesforce field. The origin receives these details from Salesforce...";
fil["158"]= "Origins/TCPServer.html@@@TCP Server@@@The TCP Server origin listens at the specified port numbers, establishes TCP sessions with clients that initiate TCP connections, and then processes the incoming data. The origin can operate in...";
fil["159"]= "Origins/UDP.html@@@UDP Source@@@The UDP Source origin reads messages from one or more UDP ports. To use multiple threads for pipeline processing, use the UDP Mulithreaded Source . For a discussion about the differences between the...";
fil["160"]= "Origins/UDPMulti.html@@@UDP Multithreaded Source@@@The UDP Multithreaded Source origin reads messages from one or more UDP ports. The origin can create multiple worker threads to enable parallel processing in a multithreaded pipeline. UDP...";
fil["161"]= "Origins/UDPtoKafka.html@@@UDP to Kafka@@@When you use a UDP to Kafka origin in a pipeline, connect the origin to a Trash destination...";
fil["162"]= "Origins/WebSocketClient.html@@@WebSocket Client@@@The WebSocket Client origin reads data from a WebSocket server endpoint. Use the origin to read data from a WebSocket resource URL...";
fil["163"]= "Origins/WebSocketServer.html@@@WebSocket Server@@@The WebSocket Server origin performs parallel processing and enables the creation of a multithreaded pipeline...";
fil["164"]= "Origins/WindowsLog.html@@@Windows Event Log@@@The Windows Event Log origin reads data from a Microsoft Windows event log located on a Windows machine. The origin generates a record for each event in the log...";
fil["165"]= "Pipeline_Configuration/ConfiguringAPipeline.html@@@Configuring a Pipeline@@@Configure a pipeline to define the stream of data. After you configure the pipeline,\n        you can start the pipeline...";
fil["166"]= "Pipeline_Configuration/DataCollectorUI-Config.html@@@Data Collector UI - Edit Mode@@@The following image shows the Data Collector UI when you configure a pipeline: Area / Icon Name Description 1 Pipeline canvas Displays the pipeline. Use to configure the pipeline data flow. 2 Pipeline...";
fil["167"]= "Pipeline_Configuration/EventGeneration.html@@@Event Generation@@@The event framework generates events when the pipeline starts and stops. You can pass the event to an executor or to another pipeline for additional processing. By default, these events are discarded...";
fil["168"]= "Pipeline_Configuration/Expressions.html@@@Expression Configuration@@@When an expression requires, the expression language attempts implicit data type conversion - a.k.a. data type coercion. When coercion is not possible, Data Collector passes the error records to the stage for error handling...";
fil["169"]= "Pipeline_Configuration/Notifications.html@@@Notifications@@@You can configure a pipeline to send an email or webhook when the pipeline changes to specified states. For example, you might send an email when someone manually stops the pipeline, causing it to...";
fil["170"]= "Pipeline_Configuration/PipelineConfiguration_title.html@@@Pipeline Configuration@@@...";
fil["171"]= "Pipeline_Configuration/PipelineMemory.html@@@Pipeline Memory@@@When you enable the monitor.memory Data Collector configuration property, you can use the Max Pipeline Memory pipeline property to define the maximum amount of memory Data Collector uses when it runs...";
fil["172"]= "Pipeline_Configuration/PipelineRateLimit.html@@@Rate Limit@@@You can limit the rate at which a pipeline processes records by defining the maximum number of records that the pipeline can read in a second...";
fil["173"]= "Pipeline_Configuration/Retry.html@@@Retrying the Pipeline@@@By default, when Data Collector encounters a stage-level error that might cause a standalone pipeline to fail, it retries the pipeline. That is, it waits a period of time, and then tries again to run the pipeline...";
fil["174"]= "Pipeline_Configuration/RuntimeValues.html@@@Runtime Values@@@Use the runtime:loadResource function to call a runtime resource. You can use runtime resources to represent sensitive information in any stage or pipeline property that allows the use of the expression language...";
fil["175"]= "Pipeline_Configuration/SSL-TLS.html@@@SSL/TLS Configuration@@@Some stages allow you use SSL/TLS to connect to the external system. When you enable TLS, you can generally configure the following properties on the TLS tab of the stage: Keystore or Truststore...";
fil["176"]= "Pipeline_Configuration/SimpleBulkEdit.html@@@Simple and Bulk Edit Mode@@@Some pipeline and pipeline stage properties include an Add icon ( ) where you add additional configurations. You can add the configurations in simple or bulk edit mode. By default, each property uses...";
fil["177"]= "Pipeline_Configuration/Validation.html@@@Implicit and Explicit Validation@@@Data Collector performs two types of validation: Implicit validation Implicit validation occurs by default as the Data Collector UI saves your changes. Implicit validation lists missing or incomplete...";
fil["178"]= "Pipeline_Configuration/Webhooks.html@@@Webhooks@@@You can configure a pipeline use webhooks. A webhook is a user-defined HTTP callback - an HTTP request that the pipeline sends automatically when certain actions occur. You can use webhooks to...";
fil["179"]= "Pipeline_Design/CDC-Overview.html@@@Processing Changed Data@@@Certain stages enable you to easily process change capture data (CDC) or transactional data in a pipeline. CDC-enabled origins can read change capture data. Some exclusively read change capture data...";
fil["180"]= "Pipeline_Design/ControlCharacters.html@@@Control Character Removal@@@You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records...";
fil["181"]= "Pipeline_Design/DatainMotion.html@@@Data in Motion@@@When you configure a pipeline, you define how you want data to be treated: Do you want to prevent the loss of data or the duplication of data?...";
fil["182"]= "Pipeline_Design/DesigningDataFlow.html@@@Designing the Data Flow@@@You can merge streams of data in a pipeline by connecting two or more stages to the same downstream stage. When you merge streams of data, Data Collector channels the data from all streams to the same stage, but does not perform a join of records in the stream...";
fil["183"]= "Pipeline_Design/DevStages.html@@@Development Stages@@@You can use several development stages to help develop and test pipelines. Note: Do not use development stages in production pipelines. You can use the following stages when developing or testing...";
fil["184"]= "Pipeline_Design/DroppingUnwantedRecords.html@@@Dropping Unwanted Records@@@You can drop records from the pipeline at each stage by defining required fields or preconditions for a record to enter a stage...";
fil["185"]= "Pipeline_Design/ErrorHandling.html@@@Error Record Handling@@@You can configure error record handling at a stage level and at a pipeline level. You can also specify the version of the record to use as the basis for the error record. When an error occurs as a...";
fil["186"]= "Pipeline_Design/FieldAttributes.html@@@Field Attributes@@@Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed...";
fil["187"]= "Pipeline_Design/PipelineDesign_title.html@@@Pipeline Concepts and Design@@@...";
fil["188"]= "Pipeline_Design/RecordHeaderAttributes.html@@@Record Header Attributes@@@Record header attributes are attributes in record headers that you can use in pipeline logic, as needed. Some stages create record header attributes for a particular purpose. For example, CDC-enabled...";
fil["189"]= "Pipeline_Design/SDCRecordFormat.html@@@SDC Record Data Format@@@SDC Record is a proprietary data format that Data Collector uses to generate error records. Data Collector can also use the data format to read and write data...";
fil["190"]= "Pipeline_Design/What_isa_Pipeline.html@@@What is a Pipeline?@@@A pipeline describes the flow of data from the origin system to destination systems and defines how to transform the data along the way...";
fil["191"]= "Pipeline_Maintenance/PipelineMaintenance_title.html@@@Pipeline Maintenance@@@Duplicate a pipeline when you want to keep the existing version of a pipeline while continuing to configure a duplicate version. A duplicate is an exact copy of the original pipeline...";
fil["192"]= "Pipeline_Monitoring/PipelineMonitoring_title.html@@@Pipeline Monitoring@@@You can view a run summary for each run of the pipeline when you view the pipeline history...";
fil["193"]= "Processors/Base64Decoder.html@@@Base64 Field Decoder@@@The Base64 Field Decoder decodes Base64 encoded data to binary data. Use the processor to decode Base64 encoded data before evaluating data in the field...";
fil["194"]= "Processors/Base64Encoder.html@@@Base64 Field Encoder@@@The Base64 Field Encoder encodes binary data using Base64. Use the processor to encode binary data that must be sent over channels that expect ASCII data...";
fil["195"]= "Processors/DataParser.html@@@Data Parser@@@The Data Parser processor allows you to parse supported Data Collector data formats embedded in a field. You can parse NetFlow embedded in a byte array field or syslog messages embedded in a string...";
fil["196"]= "Processors/Delay.html@@@Delay@@@The Delay processor delays passing a batch to rest of the pipeline by the specified number of milliseconds. Use the Delay processor to delay pipeline processing. To limit the volume of data that can...";
fil["197"]= "Processors/Expression.html@@@Expression Evaluator@@@The Expression Evaluator performs calculations and writes the results to new or existing fields. You can also use the Expression Evaluator to add or modify record header attributes and field...";
fil["198"]= "Processors/FieldFlattener.html@@@Field Flattener@@@The Field Flattener flattens list and map fields. The processor can flatten the entire record to produce a record with no nested fields. Or it can flatten specific list or map fields. Use the Field...";
fil["199"]= "Processors/FieldHasher.html@@@Field Hasher@@@Field Hasher provides several methods to hash data. When you hash a field more than once, Field Hasher uses the existing hash when generating the next hash...";
fil["200"]= "Processors/FieldMasker.html@@@Field Masker@@@You can use the following mask types to mask data...";
fil["201"]= "Processors/FieldMerger.html@@@Field Merger@@@The Field Merger merges one or more fields in a record to a different location in the record. Use only for records with a list or map structure...";
fil["202"]= "Processors/FieldOrder.html@@@Field Order@@@When you configure the Field Order processor, you specify the list of fields to order.\n        An incoming record might be missing one of those fields or it might have some extra fields.\n        You configure how the processor handles any missing or extra fields...";
fil["203"]= "Processors/FieldRemover.html@@@Field Remover@@@The Field Remover removes fields from records. Use the Field Remover to discard field data that you do not need in the pipeline. You configure the Field Remover to complete one of the following...";
fil["204"]= "Processors/FieldRenamer.html@@@Field Renamer@@@You can use regular expressions, or regex, to rename sets of fields. You can use regex to define the set of source fields to rename. You can also use regex to define the target field names...";
fil["205"]= "Processors/FieldSplitter.html@@@Field Splitter@@@The Field Splitter splits string data based on a regular expression and passes the separated data to new fields. Use the Field Splitter to split complex string values into logical components...";
fil["206"]= "Processors/FieldTypeConverter.html@@@Field Type Converter@@@You can use the Field Type Converter to change the scale of decimal fields. For example,\n        you might have a decimal field with the value 12345.6789115, and you&apos;d like to decrease the scale to 4 so that the value is 12345.6789...";
fil["207"]= "Processors/FieldZip.html@@@Field Zip@@@The Field Zip processor merges list data from two fields into a single field. You can use the Field Zip processor to merge two List fields or List-Map fields. Use the processor to merge related lists...";
fil["208"]= "Processors/GeoIP.html@@@Geo IP@@@The Geo IP processor is a lookup processor that can return geolocation and IP intelligence information for a specified IP address. The Geo IP processor uses MaxMind GeoIP2 database files for the...";
fil["209"]= "Processors/Groovy.html@@@Groovy Evaluator@@@You can call external Java code from the Groovy Evaluator. Simply install the external Java library to make it available to the Groovy Evaluator. Then, call the external Java code from the Groovy script that you develop for the processor...";
fil["210"]= "Processors/HBaseLookup.html@@@HBase Lookup@@@Configure an HBase Lookup processor to perform key-value lookups in HBase...";
fil["211"]= "Processors/HTTPClient.html@@@HTTP Client@@@Configure an HTTP Client processor to perform requests against a resource URL...";
fil["212"]= "Processors/HiveMetadata.html@@@Hive Metadata@@@You must configure Hive Metadata to use Hive and Hadoop configuration files and individual properties...";
fil["213"]= "Processors/JDBCLookup.html@@@JDBC Lookup@@@When you monitor a pipeline that includes the JDBC Lookup processor, the Summary tab displays statistics about the queries that the JDBC Lookup processor performs. Use the statistics to help identify any performance bottlenecks encountered by the pipeline...";
fil["214"]= "Processors/JDBCTee.html@@@JDBC Tee@@@The JDBC Tee processor uses a JDBC connection to write data to a database table, and then pass generated database column values to fields. Use the JDBC Tee to write some or all record fields to a...";
fil["215"]= "Processors/JSONGenerator.html@@@JSON Generator@@@The JSON Generator serializes data in a field to a JSON-encoded string. You can serialize data from List, Map, or List-Map fields. When you configure the processor, you select the field that you want...";
fil["216"]= "Processors/JSONParser.html@@@JSON Parser@@@Configure a JSON Parser to parse a JSON object in a String field...";
fil["217"]= "Processors/JavaScript.html@@@JavaScript Evaluator@@@You can call external Java code from the JavaScript Evaluator. Simply install the external Java library to make it available to the JavaScript Evaluator. Then, call the external Java code from the script that you develop for the processor...";
fil["218"]= "Processors/Jython.html@@@Jython Evaluator@@@You can call external Java code from the Jython Evaluator. Simply install the external Java library to make it available to the Jython Evaluator. Then, call the external Java code from the Jython script that you develop for the processor...";
fil["219"]= "Processors/KuduLookup.html@@@Kudu Lookup@@@The Kudu Lookup processor performs lookups in a Kudu table and passes the lookup values to fields. Use the Kudu Lookup to enrich records with additional data...";
fil["220"]= "Processors/ListPivoter.html@@@Field Pivoter@@@Configure a Field Pivoter to pivot data in a list, map, or list-map field and generate a record for each item in the field...";
fil["221"]= "Processors/LogParser.html@@@Log Parser@@@When you use Log Parser to parse log data, you define the format of the log files to be read...";
fil["222"]= "Processors/Processors_overview.html@@@Processors@@@A processor stage represents a type of data processing that you want to perform. You can use as many processors in a pipeline as you need. You can use different processors based on the execution mode...";
fil["223"]= "Processors/Processors_title.html@@@Processors@@@...";
fil["224"]= "Processors/RDeduplicator.html@@@Record Deduplicator@@@The Record Deduplicator caches record information for comparison until it reaches a specified number of records. Then, it discards the information in the cache and starts over...";
fil["225"]= "Processors/RedisLookup.html@@@Redis Lookup@@@Configure a Redis Lookup processor to perform key-value lookups in Redis...";
fil["226"]= "Processors/SalesforceLookup.html@@@Salesforce Lookup@@@The Salesforce Lookup processor generates Salesforce field attributes that provide additional information about each field. The origin receives these details from Salesforce...";
fil["227"]= "Processors/SchemaGenerator.html@@@Schema Generator@@@The Schema Generator processor generates a schema based on the structure of a record and writes the schema into a record header attribute. The Schema Generator generates Avro schemas at this time. You...";
fil["228"]= "Processors/Spark.html@@@Spark Evaluator@@@Install the Spark application JAR file as an external library for Data Collector. If your custom Spark application depends on external libraries other than the streamsets-datacollector-api,\n                        streamsets-datacollector-spark-api, and spark-core libraries, install those libraries in the same location as well...";
fil["229"]= "Processors/StaticLookup.html@@@Static Lookup@@@Configure a Static Lookup processor to perform key-value lookups in memory...";
fil["230"]= "Processors/StreamSelector.html@@@Stream Selector@@@A condition defines the data that passes to the associated stream. All records that meet the condition pass to the stream. Use the expression language to define conditions...";
fil["231"]= "Processors/ValueReplacer.html@@@Value Replacer@@@The Value Replacer replaces values in fields. You can use the Value Replacer to perform the following tasks: Replace field values with nulls, optionally based on a condition Replace null values in...";
fil["232"]= "Processors/XMLFlattener.html@@@XML Flattener@@@Configure an XML Flattener to flatten XML data embedded in a string field...";
fil["233"]= "Processors/XMLParser.html@@@XML Parser@@@Configure an XML Parser to parse XML data in a string field...";
fil["234"]= "RPC_Pipelines/SDC_RPCpipelines_title.html@@@SDC RPC Pipelines@@@You can enable SDC RPC pipelines to transfer data securely using SSL/TLS. To use SSL/TLS, enable TLS in both the SDC RPC destination and the SDC RPC origin...";
fil["235"]= "Troubleshooting/Troubleshooting_title.html@@@Troubleshooting@@@Informational and error messages display in different locations based on the type of information: Pipeline configuration issues The Data Collector UI provides guidance and error details as follows...";
fil["236"]= "Tutorial/BasicTutorial.html@@@Basic Tutorial@@@Now that the basic pipeline is complete, you can start it by clicking the Start icon...";
fil["237"]= "Tutorial/BeforeYouBegin.html@@@Before You Begin@@@Before you start this tutorial, you&apos;ll need to do a few things: Download sample data. You can download sample data from the following location...";
fil["238"]= "Tutorial/ExtendedTutorial.html@@@Extended Tutorial@@@Now that the extended pipeline is complete, let&apos;s reset the origin and run the pipeline again...";
fil["239"]= "Tutorial/Overview.html@@@Tutorial Overview@@@This tutorial walks through creating and running a pipeline. You can download sample data so you can perform data preview, run the completed pipeline, and monitor the results...";
fil["240"]= "Tutorial/Tutorial-title.html@@@Tutorial@@@...";
fil["241"]= "Upgrade/CMUpgrade.html@@@Upgrade an Installation with Cloudera Manager@@@After you add the StreamSets repository to Cloudera Manager, you can download and distribute the new StreamSets parcel across the cluster. Stop the StreamSets service and deactivate the previous parcel before you activate the new parcel...";
fil["242"]= "Upgrade/PostUpgrade.html@@@Post Upgrade Tasks@@@Data Collector version 2.3.0.0 includes an enhanced Elasticsearch destination that uses the Elasticsearch HTTP API. To upgrade pipelines that use the Elasticsearch destination from Data Collector versions earlier than 2.3.0.0, you must review the value of the Default Operation property...";
fil["243"]= "Upgrade/PreUpgrade.html@@@Pre Upgrade Tasks@@@If you use cluster pipelines that run in cluster streaming mode and you are upgrading from a version earlier than 2.3.0.0, you must upgrade to Data Collector version 2.3.0.0 before upgrading to the latest version...";
fil["244"]= "Upgrade/RPM.html@@@Upgrade an Installation from the RPM Package@@@Uninstall all stage libraries used by the previous Data Collector version...";
fil["245"]= "Upgrade/Tarball.html@@@Upgrade an Installation from the Tarball@@@Start the new version of Data Collector...";
fil["246"]= "Upgrade/Upgrade-ExternalSystems.html@@@Working with Upgraded External Systems@@@If you upgrade MapR, you must complete additional steps to continue using existing pipelines that connected to the previous MapR version...";
fil["247"]= "Upgrade/Upgrade.html@@@Upgrade@@@You can upgrade a previous version of Data Collector to a new version. You can upgrade an installation from the tarball, an installation from the RPM package, or an installation with Cloudera Manager...";
fil["248"]= "Upgrade/UpgradeTroubleshooting.html@@@Troubleshooting an Upgrade@@@Use the following tips for help with upgrades: After upgrading a Data Collector that is registered with DPM , the Data Collector fails to start with the following error about a component ID: Failed...";
fil["249"]= "Upgrade/Upgrade_title.html@@@Upgrade@@@...";
fil["250"]= "WhatsNew/WhatsNew_Title.html@@@What&apos;s New@@@Data Collector version 3.0.0.0 includes the following new features: Installation Java requirement - Data Collector now supports both Oracle Java 8 and OpenJDK 8. RPM packages - StreamSets now provides...";
