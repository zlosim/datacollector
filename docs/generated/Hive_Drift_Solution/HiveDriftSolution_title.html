
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><meta name="description" content="The Drift Synchronization Solution for Hive detects drift in incoming data and updates corresponding Hive tables. Previously known as the Hive Drift Solution, the Drift Synchronization Solution for ..." /><meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Drift Synchronization Solution (a.k.a. Hive Drift Solution)" /><meta name="DC.Relation" scheme="URI" content="../Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx" /><meta name="DC.Relation" scheme="URI" content="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_fjj_zcf_2w" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Drift Synchronization Solution (a.k.a. Hive Drift Solution)</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx" title="Dataflow Triggers (a.k.a. Event Framework)"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Dataflow Triggers (a.k.a. Event Framework)</span></a></span>  
<span class="navnext"><a class="link" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py" title="Multithreaded Pipelines"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Multithreaded Pipelines</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_fjj_zcf_2w">
 <h1 class="title topictitle1">Drift Synchronization Solution (a.k.a. Hive Drift Solution)</h1>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_phk_bdf_2w">
    <h2 class="title topictitle2">Drift Synchronization Solution for Hive</h2>

    <div class="body conbody">
        <p class="p">The <span class="ph">Drift Synchronization Solution for Hive</span>
            detects drift in incoming data and updates corresponding Hive tables. Previously known
            as the Hive Drift Solution, the <span class="ph">Drift Synchronization Solution for Hive</span>
            enables creating and updating Hive tables based on record requirements and writing data
            to HDFS or MapR FS based on record header attributes. You can use the full functionality
            of the solution or individual pieces, as needed. </p>

        <p class="p">The <span class="ph">Drift Synchronization Solution for Hive</span> supports processing Avro and Parquet data. When processing Parquet data, the solution
            generates temporary Avro files and uses the MapReduce executor to convert the Avro files
            to Parquet. </p>

        <p class="p">The solution is compatible with Impala, but requires additional steps to refresh the
            Impala metadata cache.</p>

        <p class="p"> </p>

    </div>

<div class="topic concept nested2" id="concept_qtt_bzw_vz">
 <h3 class="title topictitle3">General Processing</h3>

 <div class="body conbody">
        <p class="p">The <span class="ph">Drift Synchronization Solution for Hive</span>
            incorporates the Hive Metadata processor, Hive Metastore destination, and the Hadoop FS
            or MapR FS destination as follows:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Drift detection</dt>

                    <dd class="dd">When processing records, the Hive Metadata processor detects columnar drift
                        and the need for new tables and partitions. It generates metadata records
                        that describe the necessary changes and passes it to the Hive Metastore
                        destination.</dd>

                    <dd class="dd">When the Hive Metastore destination receives a metadata record, it compares
                        the proposed changes with the latest Hive metadata, and creates and updates
                        Hive tables as needed.</dd>

                    <dd class="dd"><span class="ph">The destination can
                        create tables and partitions. It can add columns to tables and ignore
                        existing columns. It does not drop existing columns from tables.</span></dd>

                
                
                    <dt class="dt dlterm">Record-based writes</dt>

                    <dd class="dd">The Hive Metadata processor also adds information to the header of each
                        record and passes the records to the Hadoop FS destination or the MapR FS
                        destination. The destinations can perform record-based writes to their
                        destination systems based on the following details: <ul class="ul" id="concept_qtt_bzw_vz__ul_sn1_bjg_2w">
                            <li class="li">Target directory - Based on user-defined expressions, the Hive
                                Metadata processor assembles the path where each record should be
                                stored. It writes the generated path to a
                                    <dfn class="term">targetDirectory</dfn> attribute in each record
                                    header.<p class="p">To write the record to the generated path, configure
                                    the destination to use the targetDirectory header attribute.
                                </p>
</li>

                            <li class="li">Avro schema - The processor writes the Avro schema to the
                                    <dfn class="term">avroSchema</dfn> attribute in each record header. It
                                generates new Avro schemas when necessary based on the record
                                structure. Used for both Avro and Parquet data. <p class="p">To use the
                                    generated Avro schema, configure the destination to use the
                                    avroSchema header attribute.</p>
</li>

                            <li class="li">Roll files - When a schema change occurs, the processor generates a
                                roll indicator - the <dfn class="term">roll</dfn> header attribute. This allows
                                the data with the changed schema to be written to an updated Hive
                                    table.<p class="p">To roll files based on schema changes, configure the
                                    destination use the roll header attribute.</p>
</li>

                        </ul>
</dd>

                
            </dl>

        </div>

        <p class="p">For example, say you use this solution to write sales data to MapR FS. A partial upgrade
            of the sales system adds several new fields to a subset of the incoming data. </p>

        <p class="p">With the <span class="ph">Drift Synchronization Solution for Hive</span>,
            the Hive Metadata processor notes the new fields in a metadata record and passes it to
            the Hive Metastore destination. The Hive Metastore destination adds the new columns to
            the Hive target table. The MapR FS destination then writes the data to the updated
            table. When writing data without the new fields to the updated table, the destination
            inserts null values for the missing fields.</p>

 </div>

</div>
<div class="topic concept nested2" id="concept_ndg_3zw_vz">
 <h3 class="title topictitle3">Parquet Processing</h3>

 <div class="body conbody">
        <p class="p">Here are some differences in how <span class="ph">Drift Synchronization Solution for Hive</span>
            works when processing Parquet data:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Uses events to trigger Avro to Parquet MapReduce jobs</dt>

                    <dd class="dd">When you build the pipeline, you must configure the data-processing
                        destination to generate events. The destination then generates events each
                        time it closes an output file. </dd>

                    <dd class="dd">Then, you use a MapReduce executor to kick off the Convert Avro to Parquet
                        MapReduce job each time it receives an event. </dd>

                
            </dl>

        </div>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Creates and updates Parquet tables</dt>

                    <dd class="dd">The Hive Metastore destination creates and updates Parquet tables as needed.
                            <span class="ph">The destination uses the Stored as Parquet clause when
                        generating the table so it does not need to generate a new schema for each
                        change.</span></dd>

                
            </dl>

        </div>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Uses temporary directories for Avro output files</dt>

                    <dd class="dd"><p class="p">When processing Parquet data, the Hive Metadata processor <span class="ph" id="concept_ndg_3zw_vz__d37993e2634">adds .avro to the target directory that it
                        generates for each record. This allows the data-processing destination to
                        write the Avro files to a directory that Hive ignores as a temporary
                        directory.</span></p>
<p class="p">As a result, the destination writes files to the following
                  directories: <samp class="ph codeph" id="concept_ndg_3zw_vz__d37993e2639">&lt;generated
                  directory&gt;/.avro</samp>.</p>
<span class="ph">You can configure
                              the MapReduce executor to write the Parquet files to the parent
                              generated directory and to delete the Avro files after processing
                              them. You can also delete the temporary directories after the files
                              are processed, as needed.</span></dd>

                
            </dl>

        </div>

    </div>

</div>
<div class="topic concept nested2" id="concept_u2t_fgy_1x">
 <h3 class="title topictitle3">Impala Support</h3>

 <div class="body conbody">
  <p class="p">Data written by the <span class="ph">Drift Synchronization Solution for Hive</span>
            is compatible with Impala.</p>

        <p class="p">Impala requires using the Invalidate Metadata command to refresh the Impala metadata
            cache each time changes occur in the Hive metastore. </p>

        <p class="p">When processing Avro data, you can use the Hive Query executor to automatically refresh
            the Impala metadata cache. For details, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_szz_xwm_lx">Case Study: Impala Metadata Updates for DDS for Hive</a>. </p>

        <p class="p">When processing Parquet data, you need to run the Impala Invalidate Metadata command
            manually after the Hive Metastore destination makes changes to the Hive Metastore and
            after the MapReduce executor converts a file to Parquet. </p>

        <p class="p">You can set up an alert to notify you when the Hive Metastore destination makes a change.
            Simply add a <a class="xref" href="../Alerts/RulesAlerts_title.html#concept_tpm_rsk_zq" title="Data rules define the information that you want to see about the data that passes between stages. You can create data rules based on any link in the pipeline. You can also enable metrics and create alerts for data rules.">data rule
                alert</a> on the link to the Hive Metastore destination and have the alert send
            an email or webhook when metadata records are passed to the Hive Metastore. </p>

        <p class="p">Use external tools to determine when the Convert Avro to Parquet MapReduce jobs complete. </p>

 </div>

</div>
<div class="topic concept nested2" id="concept_s3v_21p_hx">
 <h3 class="title topictitle3">Flatten Records</h3>

 <div class="body conbody">
  <p class="p">At this time, the <span class="ph">Drift Synchronization Solution for Hive</span>
            does not process records with nested fields. If necessary, you can use the Field
            Flattener processor to flatten records with nested fields before passing them to the
            Hive Metadata processor. </p>

 </div>

    <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br />
<div class="related_link"><a class="navheader_parent_path" href="../Processors/FieldFlattener.html#concept_njn_3kk_fx" title="Field Flattener">Field Flattener</a></div>
</div>
</div>
</div>
</div>
<div class="topic concept nested1" id="concept_zzs_fkg_2w">
    <h2 class="title topictitle2">Basic Avro Implementation</h2>

    <div class="body conbody">
        <p class="p">You can use the Hive Metadata processor,
            Hive Metastore destination for metadata processing, and Hadoop FS or MapR FS destination
            for data processing in any pipeline where the logic is appropriate.</p>

        <p class="p">A basic implementation of the <span class="ph">Drift Synchronization Solution for Hive</span>
            to process Avro data includes <span class="ph">the origin of your choice, the Hive Metadata processor
                        connected to the Hive Metastore destination to perform metadata updates, and
                        to either the Hadoop FS or MapR FS destination to process data</span>, as follows:</p>

        <p class="p"><img class="image" id="concept_zzs_fkg_2w__image_ys3_ztg_2w" src="../Graphics/HiveMeta-Pipeline.png" height="173" width="407" /></p>

        <p class="p">The <span class="ph">Hive Metadata processor passes
                        records through the first output stream - the data stream. Connect the data
                        stream to the Hadoop FS or MapR FS destination to write data to the
                        destination system using record header attributes.</span>
        </p>

        <p class="p">The Hive Metadata processor passes the metadata record
                  through the second output stream - the metadata output stream. Connect the Hive
                  Metastore destination to the metadata output stream to enable the destination to
                  create and update tables in Hive. The metadata output stream contains no record
                  data. </p>

        <p class="p">If your data contains nested fields, you would add a Field
                  Flattener to flatten records as follows: </p>

        <p class="p"><img class="image" id="concept_zzs_fkg_2w__image_m1s_bzn_zz" src="../Graphics/HiveDrift-Flatten.png" height="175" width="511" /></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_fkm_mzw_vz">
 <h2 class="title topictitle2">Basic Parquet Implementation</h2>

 <div class="body conbody">
        <p class="p">A basic implementation of the <span class="ph">Drift Synchronization Solution for Hive</span>
            to process Parquet data adds a MapReduce executor to the Avro implementation.</p>

        <p class="p">You use <span class="ph">the origin of your choice, the Hive Metadata processor
                        connected to the Hive Metastore destination to perform metadata updates, and
                        to either the Hadoop FS or MapR FS destination to process data</span>. You configure the data-processing destination to generate events, and use a
            MapReduce executor to convert the closed Avro files to Parquet. </p>

        <p class="p">The basic Parquet implementation looks like this: </p>

        <p class="p"><img class="image" id="concept_fkm_mzw_vz__image_opc_rq2_wz" src="../Graphics/HiveDrift-Parquet.png" height="217" width="501" /></p>

        <p class="p">As with Avro data, the <span class="ph">Hive Metadata processor passes
                        records through the first output stream - the data stream. Connect the data
                        stream to the Hadoop FS or MapR FS destination to write data to the
                        destination system using record header attributes.</span> Each time the destination closes an output file, it creates a file-closure event that
            triggers the MapReduce executor to start an Avro to Parquet MapReduce job.</p>

        <p class="p">The Hive Metadata processor passes the metadata record
                  through the second output stream - the metadata output stream. Connect the Hive
                  Metastore destination to the metadata output stream to enable the destination to
                  create and update tables in Hive. The metadata output stream contains no record
                  data. </p>

        <p class="p">If your data contains nested fields, you would add a Field
                  Flattener to flatten records as follows: </p>

        <p class="p"><img class="image" id="concept_fkm_mzw_vz__image_f4h_h14_zz" src="../Graphics/Parquet-Flatten.png" height="177" width="605" /></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_y5w_dj3_fw">
 <h2 class="title topictitle2">Implementation Steps</h2>

 <div class="body conbody">
  <div class="p">To implement the <span class="ph">Drift Synchronization Solution for Hive</span>,
            perform the following steps:<ol class="ol" id="concept_y5w_dj3_fw__ol_zr1_3j3_fw">
                <li class="li"> Configure the origin and any additional processors that you want to use. <ul class="ul" id="concept_y5w_dj3_fw__ul_qjn_kp4_hx">
                        <li class="li">If using the JDBC Query Consumer as the origin, enable the creation of
                            JDBC header attributes. For more information, see <a class="xref" href="../Origins/JDBCConsumer.html#concept_tvf_tgp_fx">Header Attributes with the Drift Synchronization Solution</a>. </li>

                        <li class="li">If data includes records with nested fields, add a Field Flattener to
                            flatten records before passing them to the Hive Metadata processor.</li>

                    </ul>
</li>

                <li class="li">To capture columnar drift and to enable record-based writes, configure the Hive
                    Metadata processor:<ul class="ul" id="concept_y5w_dj3_fw__ul_qsk_5x3_fw">
                        <li class="li">Configure the Hive connection information.</li>

                        <li class="li">Configure the database, table, and partition expressions.<p class="p">You can
                                enter a single name or use an expression that evaluates to the names
                                to use. If necessary, you can use an Expression Evaluator earlier in
                                the pipeline to write the information to a record field or record
                                header attribute.</p>
</li>

                        <li class="li">Configure the decimal field precision and scale expressions.<p class="p">You can
                                use constants or expressions that evaluate to the same precision and
                                scale for all decimal fields. Or, you can create more complex
                                expressions that evaluate to different values for different fields.
                                </p>
<p class="p">When processing data from the JDBC Query Consumer or the JDBC
                                Multitable Consumer with JDBC header attributes, use the default
                                expressions. </p>
</li>

                        <li class="li">Specify the data format to use, Avro or Parquet.</li>

                        <li class="li">Optionally configure advanced options, such as the maximum cache size,
                            time basis, and data time zone.</li>

                    </ul>
<p class="p">For more information about the Hive Metadata processor, see <a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv" title="The Hive Metadata processor works with the Hive Metastore destination, and the Hadoop FS or MapR FS destinations as part of the Drift Synchronization Solution for Hive.">Hive Metadata</a>.</p>
</li>

                <li class="li">To process metadata records generated by the processor and alter tables as
                    needed, connect the metadata output of the Hive Metadata processor to the Hive
                    Metastore destination.<div class="p">
                        <div class="note note"><span class="notetitle">Note:</span> While you might filter or route some records away from the Hive
                            Metastore destination, the destination must receive metadata records to
                            update Hive tables.</div>

                    </div>
</li>

                <li class="li">Configure the Hive Metastore destination:<ul class="ul" id="concept_y5w_dj3_fw__ul_k12_ty3_fw">
                        <li class="li">Configure the Hive connection information.</li>

                        <li class="li">Optionally configure cache information and how tables are updated.</li>

                    </ul>
<p class="p">For more information about the Hive Metastore destination, see <a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive Metastore</a>.</p>
</li>

                <li class="li">Connect the data output of the Hive Metadata processor to the Hadoop FS or MapR
                    FS destination to write records to the destination system using record header
                    attributes.</li>

                <li class="li">Configure the Hadoop FS or MapR FS destination:<ol class="ol" type="a" id="concept_y5w_dj3_fw__ul_n4p_dz3_fw">
                        <li class="li">To write records using the targetDirectory header attribute, on the
                            Output Files tab, select Directory in Header.</li>

                        <li class="li">To roll records based on a roll header attribute, on the Output Files
                            tab, select Use Roll Attribute, and for Roll Attribute Name, enter
                            “roll”.</li>

                        <li class="li">To write records using the avroSchema header attribute, on the Data
                            Format tab, select the Avro data format, and then for the Avro Schema
                            Location property, select In Record Header.</li>

                    </ol>
<p class="p">For more information about using record header attributes, see <a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_lmn_gdc_1w">Record Header Attributes for Record-Based Writes</a>.</p>
<div class="p">
                        <div class="note note"><span class="notetitle">Note:</span> To compress Avro data, use the Avro compression option on the Data
                            Formats tab, rather than the compression codec property on the Output
                            Files tab.</div>

                    </div>
</li>

                <li class="li">When processing Parquet data, perform the following additional steps: <ol class="ol" type="a" id="concept_y5w_dj3_fw__ul_bsd_v52_wz">
                        <li class="li">On the General tab of the data-processing destination, select Produce
                            Events.</li>

                        <li class="li">Connect a MapReduce executor to the resulting event stream and configure
                            the necessary connection information for the stage.</li>

                        <li class="li">On the Jobs tab of the MapReduce executor, select the Convert Avro to
                            Parquet job type and add any additional job parameters that are
                            required.</li>

                        <li class="li">On the Avro to Parquet tab, use the default Input Avro File
                            configuration, specify the Output Directory to use and optionally
                            configure the additional job properties. <ul class="ul" id="concept_y5w_dj3_fw__ul_ykw_gfc_11b">
                                <li class="li">Use the default Input Avro File expression - This allows the
                                    executor to process the file that the data processing
                                    destination just closed.</li>

                                <li class="li">Specify the Output Directory to use - To write the Parquet files
                                    to the parent directory of the .avro temporary directory, use
                                    the following
                                    expression:<pre class="pre codeblock">${file:parentPath(file:parentPath(record:value('/filepath')))}</pre>
</li>

                                <li class="li">Optionally configure the additional job properties.</li>

                            </ul>
</li>

                    </ol>
</li>

            </ol>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_a1w_kkn_fw">
 <h2 class="title topictitle2">Avro Case Study</h2>

 <div class="body conbody">
  <p class="p">Let's say you have a <span class="ph">Data
                  Collector</span>
            pipeline that writes Avro log data to Kafka. The File Tail origin in the pipeline
            processes data from several different web services, tagging each record with a "tag"
            header attribute that identifies the service that generated the data. </p>

        <p class="p">Now you want a new pipeline to pass the data to HDFS where it can be stored and reviewed,
            and you'd like the data written to tables based on the web service that generated the
            data. Note that you could also write the data to MapR FS -- the steps are almost
            identical to this case study, you'd just use a different destination.</p>

        <p class="p">To do this, add and configure a Kafka Consumer to read the data into the pipeline, then
            connect it to a Hive Metadata processor. The processor assesses the record structure and
            generates a metadata record that describes any required Hive metadata changes. Using the
            tag header attribute and other user-defined expressions, a Hive Metadata processor can
            determine the database, table, and partition to use for the target directory and write
            that information along with the Avro schema to the record header, including file roll
            indicator when necessary.</p>

        <p class="p">You connect the Hive Metadata processor metadata output stream to a Hive Metastore
            destination. The destination, upon receiving the metadata record from the Hive Metadata
            processor, creates or updates Hive tables as needed. </p>

        <p class="p">You connect the Hive Metadata processor data output stream to a Hadoop FS destination and
            configure it to use the information in record headers. The destination then writes each
            record where it wants to go using the target directory and Avro schema in the record
            header, and rolling files when needed. </p>

        <p class="p">Now let's take a closer look... </p>

 </div>

<div class="topic concept nested2" id="concept_fzk_mmn_fw">
 <h3 class="title topictitle3">The Hive Metadata Processor</h3>

 <div class="body conbody">
  <div class="p">You set up the Kafka Consumer and connect it to the Hive Metadata processor. When you configure
            the processor, you have a few things to consider in addition to the basic connection
            details: <ol class="ol" id="concept_fzk_mmn_fw__ol_fzm_bmv_fw">
                <li class="li">Which database should the records be written to? <p class="p">Hadoop FS will do the
                        writing, but the processor needs to know where the records should
                        go.</p>
<p class="p">Let's write to the Hive default database. To do that, you can
                        leave the database property empty.</p>
</li>

                <li class="li">What tables should the records be written to?<div class="p">The pipeline supplying the data
                        to Kafka uses the "tag" header attribute to indicate the originating web
                        service. To use the tag attribute to write to tables, you use the following
                        expression for the table name:
                        <pre class="pre codeblock">${record:attribute('tag')}</pre>
</div>
</li>

                <li class="li">What partitions, if any, do you want to use? <div class="p">Let's create daily partitions
                        using datetime variables for the partition value expression as
                        follows:<pre class="pre codeblock">${YYYY()}-${MM()}-${DD()}</pre>
</div>
</li>

                <li class="li">How do you want to configure the precision and scale for decimal fields?
                        <p class="p">Though the data from the web services contains no decimal data that you
                        are aware of, to prevent new decimal data from generating error records,
                        configure the decimal field expressions. </p>
<p class="p">The default expressions are
                        for data generated by the JDBC Query Consumer or the JDBC Multitable
                        Consumer. You can replace them with other expressions or with constants.
                    </p>
</li>

                <li class="li">What type of data is being processed?<p class="p">On the Data Format tab, select the Avro
                        data format.</p>
</li>

            </ol>
</div>

        <p class="p"> At this point, your pipeline would look like this: </p>

        <p class="p"><img class="image" id="concept_fzk_mmn_fw__image_g5b_34n_fw" src="../Graphics/HiveMeta-Ex-Processor.png" height="292" width="579" /></p>

        <p class="p">With this configuration, the Hadoop FS destination will write every record to the Hive
            table listed in the tag attribute and to the daily partition based on the time of
            processing.</p>

 </div>

</div>
<div class="topic concept nested2" id="concept_vh3_s4n_fw">
 <h3 class="title topictitle3">The Hive Metastore Destination</h3>

 
 <div class="body conbody"><p class="shortdesc">Now to process the metadata records - and to automatically create and update tables in
        Hive - you need the Hive Metastore destination.</p>

  <p class="p">Connect the destination to the second output stream of the processor and configure the
            destination. Configuration of this destination is a breeze - just configure the Hive
            connection information and optionally configure some advanced options. </p>

        <p class="p">The destination connects to Hive the same way the processor does so you can reuse that
            connection information:</p>

        <p class="p"><img class="image" id="concept_vh3_s4n_fw__image_imx_5pn_fw" src="../Graphics/HiveMeta-Ex-Dest.png" height="301" width="490" /></p>

        <p class="p">When the Drift Synchronization Solution for Hive processes Avro data, the destination
            includes the <a class="xref" href="../Destinations/HiveMetastore.html#concept_wyr_5jv_hw">Stored As Avro clause in table creation queries</a>, by default. You can change
            that and configure other advanced properties on the Advanced tab. You can generally use
            the defaults for the advanced properties, so let's do that. </p>

        <p class="p">Now, the beauty of the Hive Metastore destination is this: when the destination gets a
            metadata record that says you need a new table for a new web service, it creates the
            table with all the necessary columns so you can write the record (that triggered that
            metadata record) to the table.</p>

        <p class="p">
            <span class="ph">And if the structure of the record going
                        to a table changes, like adding a couple new fields, the destination updates
                        the table so the record can be written to it.</span></p>

        <p class="p">That covers the metadata, but what about the data?  </p>

 </div>

</div>
<div class="topic concept nested2" id="concept_jzr_ypn_fw">
 <h3 class="title topictitle3">The Data-Processing Destination</h3>

 <div class="body conbody">
        <p class="p">To write data to Hive using record header attributes, you can use
                  the Hadoop FS or MapR FS destinations. We'll use Hadoop FS destination. </p>

        <p class="p">To write data to HDFS, you connect the Hadoop FS destination to the data output stream of
            the Hive Metadata processor. </p>

        <p class="p">When you configure the destination, instead of configuring
                  a directory template, you configure the destination to use the directory in the
                  record header. Configure the destination to roll files when it sees a "roll"
                  attribute in the record header, and when configuring the Avro properties, indicate
                  that the schema is in the record header. </p>

        <p class="p">The Output Files tab of the destination might look something like this:</p>

        <p class="p"><img class="image" id="concept_jzr_ypn_fw__image_sbv_xrn_fw" src="../Graphics/HiveMeta-Ex-HDFS.png" height="458" width="491" /></p>

        <p class="p">And the Data Format tab looks like this:</p>

        <p class="p"><img class="image" id="concept_jzr_ypn_fw__image_wzp_1sn_fw" src="../Graphics/HiveMeta-Ex-HDFS-Avro.png" height="327" width="478" /></p>

        <p class="p"><span class="ph" id="concept_jzr_ypn_fw__d37993e3467">With this
                        configuration, the destination uses the information in record header
                        attributes to write data to HDFS. It writes each record to the directory in
                        the targetDirectory header attribute, using the Avro schema in the
                        avroSchema header attribute.</span> And it rolls a file when it spots the roll
                  attribute in a record header. </p>

        <p class="p">Note that the destination can also use Max Records in File, Max Files Size, and Idle
            Timeout to determine when to roll files.</p>

        <p class="p">Also, if you want to compress the Avro files, use the Avro Compression Codec property on
            the Data Formats tab, instead of the general compression option on the Output Files
            tab.</p>

 </div>

</div>
<div class="topic concept nested2" id="concept_jlr_zlk_gw">
 <h3 class="title topictitle3">Processing Avro Data</h3>

    
    <div class="body conbody"><p class="shortdesc">Now what happens when you start the pipeline?</p>

        <p class="p">This pipeline is set up to write data to different tables based on the table name in the
            "tag" attribute that was added to the record headers in the earlier pipeline. </p>

        <div class="p">Say the table names are "weblog" and "service". For each record with "weblog" as the tag
            attribute, the Hive Metadata processor evaluates the fields in the record as follows:
                <ul class="ul" id="concept_jlr_zlk_gw__ul_iml_1mk_gw">
                <li class="li">If the fields match the existing Hive table, it just writes the necessary
                    information into the targetDirectory and avroSchema stage attributes, and Hadoop
                    FS writes the record to the weblog table.</li>

                <li class="li">If a record includes a new field, the processor generates a metadata record that
                    the Hive Metastore destination uses to update the weblog table to include the
                    new column. It also writes information to stage attributes so Hadoop FS can
                    write the record to the updated weblog table.</li>

                <li class="li">If a record has missing fields, the processor just writes information to stage
                    attributes, and Hadoop FS writes the record to HDFS with null values for the
                    missing fields.</li>

                <li class="li">If a field has been renamed, the processor treats the field as a new field,
                    generating a metadata record that the Hive Metastore destination uses to update
                    the weblog table. When Hadoop FS writes the record, data is written to the new
                    field and a null value to the old field.</li>

                <li class="li">If a data type changes for an existing field, the processor treats the record as
                    an error record.</li>

            </ul>
</div>

        <p class="p">For each record with a "service" tag, the processor performs the same actions.</p>

        <div class="note note"><span class="notetitle">Note:</span> If a record includes a new tag value, the Hive Metadata processor generates a metadata
            record that the Hive Metastore destination uses to create a new table. And Hadoop FS
            writes the record to the new table. So if you spin up a new web service, you don't need
            to touch this pipeline to have it handle the new data set. </div>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_vl3_v2f_zz">
 <h2 class="title topictitle2">Parquet Case Study</h2>

 <div class="body conbody">
        <p class="p">Let's say you have database data that you want to
            write to Parquet tables in Hive. You want to write the data to different Parquet tables
            based on the country of origin. You don't expect a lot of schema changes, but would like
            it handled automatically when it occurs. </p>

        <p class="p">To do this, you'd start off with the JDBC Query Consumer to read data into the pipeline.
            You connect the origin to the Hive Metadata processor and configure expressions that
            define the corresponding database, table, and partition where each record should be
            written in the Parquet table. The Hive Metadata processor uses this information to
            assess records and generate the record header attributes that the data-processing
            destination uses to write the data. It also uses the information to generate metadata
            records that the Hive Metastore destination uses to create and update tables as
            needed.</p>

        <p class="p">You connect the Hive Metadata processor data output stream to a Hadoop FS destination and
            configure it to use the information in record headers. The destination then writes each
            record using the target directory and schema information in the record header, and rolls
            files upon schema changes. And you configure the destination to generate events so it
            generates events each time it closes a file. </p>

        <p class="p">You connect the Hive Metadata processor metadata output stream to a Hive Metastore
            destination. The destination, upon receiving the metadata record from the Hive Metadata
            processor, creates or updates Parquet tables as needed. </p>

        <p class="p">And finally, you connect a MapReduce executor to the event stream of the Hadoop FS
            destination and configure the executor to use the Convert Avro to Parquet job available
            in the stage. So each time the executor receives an event from the Hadoop FS
            destination, it processes the closed Avro file and converts it to Parquet, writing it to
            the updated Parquet tables.</p>

        <p class="p">Now let's take a closer look... </p>

    </div>

<div class="topic concept nested2" id="concept_kp3_nmf_zz">
 <h3 class="title topictitle3">JDBC Query Consumer</h3>

 <div class="body conbody">
  <p class="p">When you configure the origin, you configure it as you would for any normal pipeline. Specify
            the connection string to use, the query and offset column to use, and the query
            interval. If you want all the existing data, omit the initial offset. Use the default
            incremental mode to avoid requerying the entire table when the origin runs the next
            query.</p>

        <p class="p">When using the origin to process decimal data, ensure that the origin <a class="xref" href="../Origins/JDBCConsumer.html#concept_egw_d4c_kw">creates JDBC
                record header attributes</a>. When creating record header attributes, the origin
            includes the precision and scale of each decimal field in record header attributes. This
            allows the Hive Metadata processor to easily determine the original precision and scale
            of decimal data. </p>

        <p class="p">You can alternatively enter constants in the Hive Metadata processor for the precision
            and scale to be used for all decimal fields in the record, but use JDBC record header
            attributes to use field-specific values. The origin creates header attributes by
            default. </p>

        <p class="p">Here are the JDBC record header attribute properties in the origin: </p>

        <p class="p"><img class="image" id="concept_kp3_nmf_zz__image_xbr_5rf_zz" src="../Graphics/Parquet-Origin.png" height="337" width="412" /></p>

 </div>

</div>
<div class="topic concept nested2" id="concept_kt2_zrf_zz">
 <h3 class="title topictitle3">The Hive Metadata Processor</h3>

 <div class="body conbody">
        <p class="p">Connect the JDBC Query Consumer origin to the Hive Metadata processor. When you configure
            the processor, you have a few things to consider in addition to the basic connection
            details: </p>

        <ol class="ol" id="concept_kt2_zrf_zz__ol_kx4_fsf_zz">
            <li class="li">Which database should the records be written to?<p class="p">Hadoop FS will do the writing,
                    but the processor needs to know where the records should go. Let's write to the
                    Hive default database. To do that, you can leave the database property empty.
                </p>
</li>

            <li class="li">What tables should the records be written to? <div class="p">You can write all data to a single
                    table by hardcoding the Table Name property. But since you want to write the
                    data to different tables based on the country of origin, let's use an expression
                    to pull the table name from the Country field, as
                    follows:<pre class="pre codeblock">${record:value('/Country')}</pre>
</div>
</li>

            <li class="li">What partition do you want to use?<div class="p">Let's create a dt partition column for daily
                    partitions using datetime variables in the expression as follows:
                    <pre class="pre codeblock">${YYYY()}-${MM()}-${DD()}</pre>
</div>
</li>

            <li class="li">How do you want to configure the precision and scale expressions for decimal fields?
                    <div class="p">Since you have the JDBC Query Consumer generating record header attributes,
                    you can use the default expressions in the processor:
                    <pre class="pre codeblock">${record:attribute(str:concat(str:concat('jdbc.', field:field()), '.scale'))}
${record:attribute(str:concat(str:concat('jdbc.', field:field()), '.precision'))}</pre>
</div>
<p class="p">With
                    these expressions, the processor uses the precision and scale that is written to
                    record header attributes by the JDBC Query Consumer for each decimal field in
                    the record.</p>
</li>

            <li class="li">What type of data is being processed?<p class="p">On the Data Format tab, select the Parquet
                    data format.</p>
</li>

        </ol>

        <p class="p">At this point, the pipeline looks like this:</p>

        <p class="p"><img class="image" id="concept_kt2_zrf_zz__image_dr3_mvg_zz" src="../Graphics/Parquet-HiveMetadata.png" height="303" width="548" /></p>

        <p class="p">When processing records, the Hive Metadata processor uses the configuration details to
            assess records. It generates a targetDirectory header attribute for each record using
            the country listed in the record for the table and the time the record was processed for
            the partition. </p>

        <p class="p"><span class="ph">When a record includes a schema change, the
                        processor writes the new schema to the avroSchema header attribute and adds
                        the roll header attribute to the record. It also generates a metadata record
                        for the Hive Metastore destination. The combination of these actions enables
                        the Hive Metastore destination to update Parquet tables as needed and for
                        the Hadoop FS destination to write the file with schema drift to the updated
                        table.</span></p>

        <p class="p">Remember that for Parquet data, the processor <span class="ph">adds .avro to the target directory that it
                        generates for each record. This allows the data-processing destination to
                        write the Avro files to a directory that Hive ignores as a temporary
                        directory.</span></p>

        <p class="p">As a result, the destination writes files to the following
                  directories: <samp class="ph codeph" id="concept_kt2_zrf_zz__d37993e2639">&lt;generated
                  directory&gt;/.avro</samp>.</p>

 </div>

</div>
<div class="topic concept nested2" id="concept_mr2_y1g_zz">
 <h3 class="title topictitle3">The Hive Metastore Destination</h3>

 
 <div class="body conbody"><p class="shortdesc">Now to process the metadata records - and to automatically create and update Parquet
        tables in Hive - you need the Hive Metastore destination.</p>

        <p class="p">Connect the destination to the second output stream
                  of the processor and configure the destination. Configuration of this destination
                  is a breeze - just configure the Hive connection information and optionally
                  configure some advanced options. </p>

        <p class="p">The destination connects to Hive the same way the processor does, so you can reuse that
            connection information. The Advanced tab includes some properties that only apply to
            Avro data and a Max Cache Size property to limit the size of the cache that the Hive
            Metastore uses. By default, the cache size is unlimited, so let's leave it that way. </p>

        <p class="p">Now, the beauty of the Hive Metastore destination is this: when the destination gets a
            metadata record that says you need a new table for a new country, it creates a new
            Parquet table with all the necessary columns so you can write the record (that triggered
            that metadata record) to the table. </p>

        
     <p class="p"><span class="ph">And if the structure of the record going
                        to a table changes, like adding a couple new fields, the destination updates
                        the table so the record can be written to it.</span>
            <span class="ph">The destination uses the Stored as Parquet clause when
                        generating the table so it does not need to generate a new schema for each
                        change.</span></p>

        <p class="p">This is how the pipeline looks at this point: </p>

        <p class="p"><img class="image" id="concept_mr2_y1g_zz__image_ckn_swn_zz" src="../Graphics/Parquet-HMetastore.png" height="114" width="363" /></p>

        <p class="p">That covers the metadata, but what about the data? </p>

 </div>

</div>
<div class="topic concept nested2" id="concept_oyq_qbh_zz">
 <h3 class="title topictitle3">The Data-Processing Destination</h3>

 <div class="body conbody">
        <p class="p">To write data to Hive using record header attributes, you can use
                  the Hadoop FS or MapR FS destinations. We'll use Hadoop FS destination. </p>

        <p class="p" id="concept_oyq_qbh_zz__HD-CStudy-ConnectHDFS">To write Avro files to HDFS, you connect the Hadoop FS
            destination to the data output stream of the Hive Metadata processor. </p>

        <p class="p">First, on the General tab, enable the destination to generate events, as follows:</p>

        <p class="p"><img class="image" id="concept_oyq_qbh_zz__image_zc3_hjh_zz" src="../Graphics/Parquet-HDFS-Events.png" height="365" width="368" /></p>

        <p class="p">Now, the destination generates an event each time the destination closes an output file.
            As described in the <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_dmx_1ln_qx">Event Record
                section</a> of the Hadoop FS documentation, the event record includes the
            filepath and file name of the closed file. The MapReduce executor will use this
            information to convert the Avro files to Parquet.</p>

        <p class="p">When you configure the destination, instead of configuring
                  a directory template, you configure the destination to use the directory in the
                  record header. Configure the destination to roll files when it sees a "roll"
                  attribute in the record header, and when configuring the Avro properties, indicate
                  that the schema is in the record header. </p>

        <p class="p">The Output Files tab of the destination might look something like this:</p>

        <p class="p"><img class="image" id="concept_oyq_qbh_zz__image_ckx_kkh_zz" src="../Graphics/Parquet-HDFS-Output.png" height="421" width="338" /></p>

        <p class="p"><span class="ph">With this
                        configuration, the destination uses the information in record header
                        attributes to write data to HDFS. It writes each record to the directory in
                        the targetDirectory header attribute, using the Avro schema in the
                        avroSchema header attribute.</span> It closes files when it spots the roll attribute in a record header or upon reaching
            other file closure limits configured in the destination. And it generates an event each
            time it closes a file. </p>

        <div class="p">
            <div class="note tip"><span class="tiptitle">Tip:</span> Data does not become available to Hive until the Avro files are
                converted to Parquet. If you want to convert data quickly, configure one or more of
                the file closure properties to ensure files roll regularly: Max Records in File, Max
                File Size, or Idle Timeout.</div>

        </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_gmh_b4h_zz">
 <h3 class="title topictitle3">The MapReduce Executor</h3>

 <div class="body conbody">
  <p class="p">To convert the Avro files generated by the Hadoop FS destination, use the Convert Avro to
            Parquet job in the MapReduce executor. Like all executors, the MapReduce executor
            performs tasks when triggered by an event. In this case, it will be the file-closure
            events generated by the Hadoop FS destination. </p>

        <div class="p">Connect the Hadoop FS event output stream to the MapReduce executor. In addition to the
            required configuration details, select the Convert Avro to Parquet job type, then
            configure the following key job details:<ul class="ul" id="concept_gmh_b4h_zz__ul_fdd_wth_zz">
                <li class="li">Input Avro File - Use the default expression for this property. With the
                    default, the executor uses the directory and file name specified in the filepath
                    field of the event record. Files will be in the .avro directory, but this
                    information will be correctly noted in the event record.</li>

                <li class="li">Keep Avro Input File - Select this if you want to keep the original Avro file.
                    By default, the executor deletes the original file after successfully converting
                    it to Parquet.</li>

                <li class="li">Output Directory - To write the Parquet files to the original directory where
                    the data was expected - rather than the .avro directory - use the following
                        expression:<pre class="pre codeblock">${file:parentPath(file:parentPath(record:value('/filepath')))}</pre>
<p class="p">The
                            <samp class="ph codeph">file:parentPath</samp> function returns a file path without
                        the final separator. So this expression removes /.avro/&lt;filename&gt; from
                        the filepath.</p>
<p class="p">For example, if the original filepath is:
                        /sales/countries/nz/.avro/sdc-file, then <samp class="ph codeph">file:parentPath</samp>
                        returns the following output path: /sales/countries/nz.</p>
</li>

            </ul>
</div>

        <p class="p">Here's the pipeline and MapReduce executor configuration: </p>

        <p class="p"><img class="image" id="concept_gmh_b4h_zz__image_qhp_j4n_zz" src="../Graphics/Parquet-MapReduce.png" height="434" width="581" /></p>

        <p class="p"><span class="ph">For more information about dataflow
                        triggers and the event framework, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Dataflow Triggers Overview</a>.</span></p>

       
 </div>

</div>
<div class="topic concept nested2" id="concept_enc_2wh_zz">
    <h3 class="title topictitle3">Processing Parquet Data</h3>

    <div class="body conbody">
        <div class="p">When the pipeline runs, the following actions occur:<ul class="ul" id="concept_enc_2wh_zz__ul_jqb_jfn_zz">
                <li class="li">
                    <p class="p">Hive Metadata processor assesses each record, using the country in the record
                        to create the output directory for the targetDirectory header attribute.</p>

                </li>

                <li class="li">
                    <p class="p"><span class="ph">When a record includes a schema change, the
                        processor writes the new schema to the avroSchema header attribute and adds
                        the roll header attribute to the record. It also generates a metadata record
                        for the Hive Metastore destination. The combination of these actions enables
                        the Hive Metastore destination to update Parquet tables as needed and for
                        the Hadoop FS destination to write the file with schema drift to the updated
                        table.</span></p>

                </li>

                <li class="li">When the Hive Metastore destination receives a metadata record, it updates the
                    Hive metastore accordingly, creating or updating a Parquet table. </li>

                <li class="li">The Hadoop FS destination writes records to files based on the directory in the
                    targetDirectory header, closing files based on the roll header attribute and any
                    other file closure properties configured in the stage. </li>

                <li class="li">When the Hadoop FS destination closes a file, it sends an event record to the
                    MapReduce executor, triggering the executor to kick off the Convert Avro to
                    Parquet job. The MapReduce executor does not monitor the job.</li>

                <li class="li">After the job completes, the Parquet data becomes available to Hive.</li>

            </ul>
</div>

    </div>

</div>
</div>
<div class="topic concept nested1" id="concept_ry2_qkm_hw">
 <h2 class="title topictitle2">Hive Data Types</h2>

 <div class="body conbody">
        <p class="p">The following table lists the <span class="ph">Data
                  Collector</span>
            data types and the corresponding Hive data types. The Hive Metadata processor uses these
            conversions when generating metadata records. The Hive Metadata destination uses these
            conversions when generating Hive CREATE TABLE and ALTER TABLE statements.</p>

  <div class="p">
            
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="concept_ry2_qkm_hw__table_bcm_jlm_hw" class="table" frame="border" border="1" rules="all">
                    
                    
                    <thead class="thead" align="left">
                        <tr>
                            <th class="entry" valign="top" width="50%" id="d205688e1159"><span class="ph">Data
                  Collector</span> Data Type</th>

                            <th class="entry" valign="top" width="50%" id="d205688e1164">Hive Data Type</th>

                        </tr>

                    </thead>

                    <tbody class="tbody">
                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Boolean</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Boolean</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Byte</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Char</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">String</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Date</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Date </td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Datetime</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">String</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Decimal</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Decimal</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Double</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Double</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Float</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Float</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Integer</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Int</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Long</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Bigint</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">List</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">List-Map</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Map</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Not supported</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Short</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">Int</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">String</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">String</td>

                        </tr>

                        <tr>
                            <td class="entry" valign="top" width="50%" headers="d205688e1159 ">Time</td>

                            <td class="entry" valign="top" width="50%" headers="d205688e1164 ">String</td>

                        </tr>

                    </tbody>

                </table>
</div>

        </div>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navprev"><a class="link" href="../Event_Handling/EventFramework-Title.html#concept_xxd_f5r_kx" title="Dataflow Triggers (a.k.a. Event Framework)"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Dataflow Triggers (a.k.a. Event Framework)</span></a></span>  
<span class="navnext"><a class="link" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wwq_gxc_py" title="Multithreaded Pipelines"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Multithreaded Pipelines</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

--><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>