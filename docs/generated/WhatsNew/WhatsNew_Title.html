
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="What's New" /><meta name="abstract" content="" /><meta name="description" content="Data Collector version 3.0.0.0 includes the following new features: Installation Java requirement - Data Collector now supports both Oracle Java 8 and OpenJDK 8. RPM packages - StreamSets now provides ..." /><meta name="DC.Relation" scheme="URI" content="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" /><meta name="DC.Relation" scheme="URI" content="../Installation/Install_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_hz3_5fk_fy" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>What's New</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Getting Started</span></a></span>  
<span class="navnext"><a class="link" href="../Installation/Install_title.html" title="Installation"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Installation</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_hz3_5fk_fy">
 <h1 class="title topictitle1">What's New</h1>

 
 <div class="body conbody"><p class="shortdesc"></p>

  <p class="p"></p>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_cjx_y4k_wbb">
 <h2 class="title topictitle2">What's New in 3.0.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 3.0.0.0 includes the following new features:<dl class="dl">
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_dxx_z5k_wbb">
                            <li class="li"><a class="xref" href="../Installation/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Java requirement</a> - Data Collector now supports both
                                Oracle Java 8 and OpenJDK 8.</li>

                            <li class="li"><a class="xref" href="../Installation/FullInstall_ServiceStart.html#task_th5_1yj_dx" title="You can install the Data Collector RPM package and start it as a service on CentOS or Red Hat Enterprise Linux.">RPM packages</a> - StreamSets now provides the following
                                Data Collector RPM packages:<ul class="ul" id="concept_cjx_y4k_wbb__ul_k13_cvk_wbb">
                                    <li class="li">EL6 - Use to install Data Collector on CentOS 6, Red Hat
                                        Enterprise Linux 6, or Ubuntu 14.04 LTS.</li>

                                    <li class="li">EL7 - Use to install Data Collector on CentOS 7, Red Hat
                                        Enterprise Linux 7, or Ubuntu 16.04 LTS.</li>

                                </ul>
<p class="p">Previously, StreamSets provided a single RPM package used to
                                    install Data Collector on any of these operating
                                systems.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Edge Pipelines</dt>

                    <dd class="dd">You can now design and run <a class="xref" href="../Edge_Mode/EdgePipelines_title.html#concept_d4h_kkq_4bb" title="An edge pipeline is a pipeline that runs on an edge device with limited resources. Use edge pipelines to read data from the edge device or to receive data from another pipeline and then act on that data to control the edge device.">edge
                            pipelines</a> to read data from or send data to an edge device. Edge
                        pipelines are bidirectional. They can send edge data to other Data Collector
                        pipelines for further processing. Or, they can receive data from other
                        pipelines and then act on that data to control the edge device. </dd>

                    <dd class="dd">Edge pipelines run in edge execution mode on <span class="ph">StreamSets</span> Data Collector Edge (<span class="ph">SDC Edge</span>). <span class="ph">SDC Edge</span> is a lightweight agent without a UI that runs pipelines on edge devices.
                        Install <span class="ph">SDC Edge</span> on each edge device where you want to run edge pipelines. </dd>

                    <dd class="dd">You design edge pipelines in Data Collector, export the edge pipelines, and
                        then use commands to run the edge pipelines on an <span class="ph">SDC Edge</span> installed on an edge device. </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_xdj_qvk_wbb">
                            <li class="li"><a class="xref" href="../Origins/AmazonSQS.html#concept_xsh_knm_5bb" title="When Data Collector reads data from an Amazon SQS Consumer origin, it must pass credentials to Amazon Simple Queue Services.">New
                                    Amazon SQS Consumer origin</a> - An origin that reads
                                messages from Amazon Simple Queue Service (SQS). Can create multiple
                                threads to enable parallel processing in a multithreaded
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/GCS.html#concept_iyd_wql_nbb">New Google
                                    Cloud Storage origin</a> - An origin that reads fully written
                                objects in Google Cloud Storage. </li>

                            <li class="li"><a class="xref" href="../Origins/MapRdbCDC.html#concept_qwj_5vm_pbb">New MapR
                                    DB CDC origin</a> - An origin that reads changed MapR DB data
                                that has been written to MapR Streams. Can create multiple threads
                                to enable parallel processing in a multithreaded pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/MapRStreamsMultiConsumer.html#concept_hvd_hww_lbb" title="You can add custom configuration properties to the MapR Multitopic Streams Consumer. You can use any MapR or Kafka property supported by MapR Streams. For more information, see the MapR Streams documentation. When you use an origin to read log data, you define the format of the log files to be read.">New MapR Multitopic Streams Consumer origin</a> - An origin
                                that reads messages from multiple MapR Streams topics. It can create
                                multiple threads to enable parallel processing in a multithreaded
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/UDPMulti.html#concept_wng_g5f_5bb">New UDP
                                    Multithreaded Source origin</a> - The origin listens for UDP
                                messages on one or more ports and queues incoming packets on an
                                intermediate queue for processing. It can create multiple threads to
                                enable parallel processing in a multithreaded pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/WebSocketClient.html#concept_unk_nzk_fbb" title="The WebSocket Client origin reads data from a WebSocket server endpoint. Use the origin to read data from a WebSocket resource URL.">New
                                    WebSocket Client origin</a> - An origin that reads data from
                                a WebSocket server endpoint.</li>

                            <li class="li"><a class="xref" href="../Origins/WindowsLog.html#concept_agf_5jv_sbb" title="The Windows Event Log origin reads data from a Microsoft Windows event log located on a Windows machine. The origin generates a record for each event in the log.">New
                                    Windows Event Log origin</a> - An origin that reads data from
                                Microsoft Windows event logs. You can use this origin only in
                                pipelines configured for edge execution mode.</li>

                            <li class="li">Amazon S3 origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_gxg_dbl_wbb">
                                    <li class="li">The origin now produces <a class="xref" href="../Origins/AmazonS3.html#concept_vtn_ty4_jbb">no-more-data events</a> and includes a new socket
                                        timeout property.</li>

                                    <li class="li">You can now specify the number of times the origin retries a
                                        query. The default is three.</li>

                                </ul>
</li>

                            <li class="li">Directory origin enhancement - The origin can now <a class="xref" href="../Origins/Directory.html#concept_pcl_nwn_qbb">use multiple threads</a> to perform parallel processing of
                                files.</li>

                            <li class="li">JDBC Multitable Consumer origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_ij2_lbl_wbb">
                                    <li class="li">The origin can now use <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_xwr_bhm_nbb">non-incremental processing</a> for tables with no
                                        primary key or offset column. </li>

                                    <li class="li">You can now specify a query to be executed after
                                        establishing a connection to the database, before performing
                                        other tasks. This can be used, for example, to modify
                                        session attributes. </li>

                                    <li class="li">A new <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#task_kst_m4w_4y">Queries Per Second property</a> determines how many
                                        queries can be run every second. <p class="p">This property replaces
                                            the Query Interval property. For information about
                                            possible upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_hky_ljl_wbb">JDBC Multitable Consumer Query Interval
                                                Change</a>.</p>
</li>

                                </ul>
</li>

                            <li class="li">JDBC Query Consumer origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_hzy_tbl_wbb">
                                    <li class="li">You can now specify a query to be executed after
                                        establishing a connection to the database, before performing
                                        other tasks. This can be used, for example, to modify
                                        session attributes.</li>

                                    <li class="li">The Microsoft SQL Server CDC functionality in the JDBC Query
                                        Consumer origin is now deprecated and will be removed from
                                        the origin in a future release. For upgrade information, see
                                            <a class="xref" href="../Upgrade/PostUpgrade.html#concept_ys3_bjl_wbb">Update JDBC Query Consumer Pipelines used for SQL
                                            Server CDC Data</a>.</li>

                                </ul>
</li>

                            <li class="li">Kafka Multitopic Consumer origin enhancement - The origin is now
                                available in the following stage libraries, in addition to the
                                Apache Kafka 0.10 stage library:<ul class="ul" id="concept_cjx_y4k_wbb__ul_emj_ccl_wbb">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">Apache Kafka 0.9</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p">CDH Kafka 2.0 (0.9.0) and 2.1 (0.9.0)</p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p class="p">HDP 2.5 and 2.6</p>

                                    </li>

                                </ul>
</li>

                            <li class="li">Kinesis Consumer origin enhancement - You can now specify the number
                                of times the origin retries a query. The default is three.</li>

                            <li class="li">Oracle CDC Client origin enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_wbj_fcl_wbb">
                                    <li class="li">When using <a class="xref" href="../Origins/OracleCDC.html#concept_zrc_pyj_dx">SCNs for the initial change</a>, the origin now
                                        treats the specified SCN as a starting point rather than
                                        looking for an exact match.</li>

                                    <li class="li">The origin now passes <a class="xref" href="../Origins/OracleCDC.html#concept_gwp_d4n_n1b">raw data</a> to the pipeline as a byte array.</li>

                                    <li class="li">The origin can now include unparsed strings from the parsed
                                        SQL query for <a class="xref" href="../Origins/OracleCDC.html#concept_gwp_d4n_n1b">unsupported data types</a> in records.</li>

                                    <li class="li">The origin now uses <a class="xref" href="../Origins/OracleCDC.html#concept_yqk_3hn_n1b">local buffering</a> instead of Oracle LogMiner
                                        buffering by default. Upgraded pipelines require no changes.
                                    </li>

                                </ul>
</li>

                            <li class="li">SQL Server CDC Client origin enhancements - You can now perform the
                                following tasks with the SQL Server CDC Client origin:<ul class="ul" id="concept_cjx_y4k_wbb__ul_et1_pcl_wbb">
                                    <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_nxm_1lp_qbb">Process CDC tables</a> that appear after the
                                        pipeline starts.</li>

                                    <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_avq_s2q_qbb">Check for schema changes and generate events</a>
                                        when they are found.</li>

                                    <li class="li">
                                        <p dir="ltr" class="p">In addition, a new Capture Instance Name
                                            property replaces the Schema and Table Name Pattern
                                            properties from earlier releases.</p>

                                        <p dir="ltr" class="p">You can simply use the schema name and table
                                            name pattern for the capture instance name. Or, you can
                                            specify the schema name and a capture instance name
                                            pattern, which allows you to specify specific CDC tables
                                            to process when you have multiple CDC tables for a
                                            single data table.</p>

                                        <p dir="ltr" class="p">Upgraded pipelines require no changes.</p>

                                    </li>

                                </ul>
</li>

                            <li class="li">UDP Source origin enhancement - The Enable Multithreading property
                                that enabled using multiple epoll receiver threads is now named Use
                                Native Transports (epoll).</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_qfc_3fl_wbb">
                            <li class="li">New Aggregator processor - A processor that aggregates data that
                                arrives within a window of time.</li>

                            <li class="li"><a class="xref" href="../Processors/Delay.html#concept_ez5_pvf_wbb">New Delay
                                    processor</a> - A processor that can delay processing a batch
                                of records for a specified amount of time.</li>

                            <li class="li">Field Converter processor enhancement - You can now convert strings
                                to the Zoned Datetime data type, and vice versa. You can also
                                specify the format to use. </li>

                            <li class="li">Hive Metadata processor enhancement - You can now configure
                                additional JDBC configuration properties to pass to the JDBC
                                driver.</li>

                            <li class="li">HTTP Client processor enhancement - The Rate Limit now defines the
                                maximum number of requests to make per millisecond. Previously, it
                                defined the maximum number of requests per second.</li>

                            <li class="li">JDBC Lookup and JDBC Tee processor enhancements - You can now
                                specify a query to be executed after establishing a connection to
                                the database, before performing other tasks. This can be used, for
                                example, to modify session attributes. </li>

                            <li class="li">Kudu Lookup processor enhancement - The Cache Kudu Table property is
                                now named Enable Table Caching. The Maximum Entries to Cache Table
                                Objects property is now named Maximum Table Entries to Cache. </li>

                            <li class="li">Salesforce Lookup processor enhancement - You can use a new <a class="xref" href="../Processors/SalesforceLookup.html#concept_ow1_lj3_xbb">Retrieve lookup mode</a> to look up data for a set of
                                records instead of record-by-record. The mode provided in previous
                                releases is now named SOQL Query. Upgraded pipelines require no
                                changes. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_rv3_nfl_wbb">
                            <li class="li"><a class="xref" href="../Destinations/GCS.html#concept_p4n_jrl_nbb">New Google
                                    Cloud Storage destination</a> - A new destination that writes
                                data to objects in Google Cloud Storage. The destination can
                                generate events for use as dataflow triggers.</li>

                            <li class="li"><a class="xref" href="../Destinations/KineticaDB.html#concept_hxh_5xg_qbb">New
                                    KineticaDB destination</a> - A new destination that writes
                                data to a Kinetica table. </li>

                            <li class="li">Hive Metastore destination enhancement - You can now configure
                                additional JDBC configuration properties to pass to the JDBC
                                driver.</li>

                            <li class="li">HTTP Client destination enhancement - You can now use the HTTP
                                Client destination to write Avro, Delimited, and Protobuf data in
                                addition to the previous data formats. </li>

                            <li class="li">JDBC Producer destination enhancement - You can now specify a query
                                to be executed after establishing a connection to the database,
                                before performing other tasks. This can be used, for example, to
                                modify session attributes. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#task_c4x_tmh_4v">Kudu destination enhancement</a> - If the destination
                                receives a change data capture log from the following source
                                systems, you now must specify the source system in the Change Log
                                Format property so that the destination can determine the format of
                                the log: Microsoft SQL Server, Oracle CDC Client, MySQL Binary Log,
                                or MongoDB Oplog. </li>

                            <li class="li">MapR DB JSON destination enhancement - The destination now supports
                                writing to MapR DB based on the <a class="xref" href="../Destinations/MapRDBJSON.html#concept_ab4_gbb_xbb">CRUD operation in record header attributes</a>.</li>

                            <li class="li">MongoDB destination enhancements - With this release, the Upsert
                                operation is no longer supported by the destination. Instead, the
                                destination includes the following enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_xfc_xfl_wbb">
                                    <li class="li">Support for the <a class="xref" href="../Destinations/MongoDB.html#concept_bkc_m24_4v">Replace and Update operations</a>. </li>

                                    <li class="li">Support for an <a class="xref" href="../Destinations/MongoDB.html#concept_syh_s1l_tbb">Upsert flag</a> that, when enabled, is used with
                                        both the Replace and Update operations. </li>

                                </ul>
<p class="p">For information about upgrading existing upsert pipelines,
                                    see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_ncs_5jl_wbb">Update MongoDB Destination Upsert Pipelines</a>.
                                </p>
</li>

                            <li class="li">Redis destination enhancement - The destination now supports writing
                                data using <a class="xref" href="../Destinations/Redis.html#concept_dz2_4xh_xbb">CRUD operations stored in record header attributes</a>. </li>

                            <li class="li">Salesforce destination enhancement - When using the Salesforce Bulk
                                API to update, insert, or upsert data, you can now use a colon (:)
                                or period (.) as a field separator when defining the Salesforce
                                field to map the Data Collector field to. For example,
                                    <samp class="ph codeph">Parent__r:External_Id__c</samp> or
                                    <samp class="ph codeph">Parent__r.External_Id__c</samp> are both valid
                                Salesforce fields. </li>

                            <li class="li">Wave Analytics destination rename - With this release, the Wave
                                Analytics destination is now named the <a class="xref" href="../Destinations/WaveAnalytics.html#concept_hlx_r53_rx" title="The Einstein Analytics destination writes data to Salesforce Einstein Analytics. The destination connects to Einstein Analytics to create a dataset with external data.">Einstein Analytics destination</a>, following the recent
                                Salesforce rebranding. All of the properties and functionality of
                                the destination remain the same. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_if5_jgl_wbb">
                            <li class="li">Hive Query executor enhancement - You can now configure additional
                                JDBC configuration properties to pass to the JDBC driver. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Cloudera Navigator </dt>

                    <dd class="dd">Cloudera Navigator integration is now released as part of the StreamSets
                        Commercial Subscription. The beta version included in earlier releases is no
                        longer available with Data Collector. For information about the StreamSets
                        Commercial Subscription, <a class="xref" href="https://streamsets.com/contact-us/" target="_blank">contact us</a>.<p dir="ltr" class="p">For
                            information about upgrading a version of Data Collector with Cloudera
                            Navigator integration enabled, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnp_scs_wbb">Disable Cloudera Navigator Integration</a>.</p>
</dd>

                
                
                    <dt class="dt dlterm">Credential Stores</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_o1b_mgl_wbb">
                            <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_v21_nvd_fbb" title="By default, a full Data Collector installation includes the CyberArk credential store stage library. The core installation does not include the library.Use the credential:get() or credential:getWithOptions() function in pipeline stage properties to retrieve credential values from CyberArk.">CyberArk</a> - Data Collector now provides a credential
                                store implementation for CyberArk Application Identity Manager. You
                                can define the credentials required by external systems - user names
                                or passwords - in CyberArk. Then you use credential expression
                                language functions in JDBC stage properties to retrieve those
                                values, instead of directly entering credential values in stage
                                properties. </li>

                            <li class="li"><a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">Supported stages</a> - You can now use the credential
                                functions in all stages that require you to enter sensitive
                                information. Previously, you could only use the credential functions
                                in JDBC stages. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Collector Configuration</dt>

                    <dd class="dd">By default when Data Collector restarts, it automatically restarts all
                        pipelines that were running before Data Collector shut down. You can now
                        disable the automatic restart of pipelines by enabling the
                            <samp class="ph codeph">runner.boot.pipeline.restart</samp> property in the
                            <samp class="ph codeph">$SDC_CONF/sdc.properties</samp> file. </dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_p2b_4hl_wbb">
                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_qh5_v5t_mbb" title="When you write statistics to MapR Streams, Data Collector effectively adds a MapR Streams Producer destination to the pipeline that you are configuring. DPM automatically generates and runs a system pipeline for the job. The system pipeline reads the statistics from MapR Streams, and then aggregates and sends the statistics to DPM.">Aggregated statistics</a> - When working with DPM, you can
                                now configure a pipeline to write aggregated statistics to MapR
                                Streams. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_sp4_rhl_wbb">
                            <li class="li"><a class="xref" href="../Data_Formats/NetFlow_Overview.html#concept_thl_nnr_hbb">New NetFlow 9 support</a> - Data Collector now supports
                                processing NetFlow 9 template-based messages. Stages that previously
                                processed NetFlow 5 data can now process NetFlow 9 data as well. </li>

                            <li class="li">Datagram data format enhancement - The Datagram Data Format property
                                is now named the Datagram Packet Format. </li>

                            <li class="li">Delimited data format enhancement - Data Collector can now process
                                data using the Postgres CSV and Postgres Text delimited format
                                types. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_ppq_whl_wbb">
                            <li class="li">New field-path syntax - You can use a powerful new field-path syntax
                                in certain stages to specify the fields that you want to use in an
                                expression. </li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New string functions</a> - The release includes the
                                following new functions:<ul class="ul" id="concept_cjx_y4k_wbb__ul_uss_whl_wbb">
                                        <li class="li">str:isNullOrEmpty() - Returns true or false based on
                                            whether a string is null or is the empty string.</li>

                                        <li class="li">str:splitKV() - Splits key-value pairs in a string into
                                            a map of string values.</li>

                                    
                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_qhq_c3l_wbb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release includes the
                                following new stage libraries:<ul class="ul" id="concept_cjx_y4k_wbb__ul_rcr_23l_wbb">
                                    <li class="li">Apache Kafka 1.0 </li>

                                    <li class="li">Apache Kafka 0.11 </li>

                                    <li class="li">Apache Kudu 1.5 </li>

                                    <li class="li">Cloudera CDH 5.13 </li>

                                    <li class="li">Cloudera Kafka 3.0.0 (0.11.0) </li>

                                    <li class="li">Hortonworks 2.6.1, including Hive 1.2 </li>

                                    <li class="li">Hortonworks 2.6.2, including Hive 1.2 and 2.0 </li>

                                    <li class="li">MapR version 6.0 (MEP 4)</li>

                                    <li class="li">MapR Spark 2.1 (MEP 3) </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_fw3_zt3_tbb">Legacy stage libraries</a> - Stage libraries that are more
                                than two years old are no longer included with Data Collector.
                                Though not recommended, you can still download and install the older
                                stage libraries as custom stage libraries. <p class="p">If you have pipelines
                                    that use these legacy stage libraries, you will need to update
                                    the pipelines to use a more current stage library or install the
                                    legacy stage library manually, For moe information see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_wnl_zk4_5bb">Update Pipelines using Legacy Stage
                                Libraries</a>.</p>
</li>

                            <li class="li"><a class="xref" href="../Installation/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space.">Statistics stage library enhancement</a> - The statistics
                                stage library is now included in the core Data Collector
                                installation. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Miscellaneous</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_cjx_y4k_wbb__ul_fw1_s3l_wbb">
                            <li class="li">New data type - Data Collector now supports the Zoned Datetime data
                                type.</li>

                            <li class="li">New Data Collector metrics - JVM metrics have been renamed Data
                                Collector Metrics and now include general Data Collector metrics in
                                addition to JVM metrics. The JVM Metrics menu item has also been
                                renamed SDC Metrics.</li>

                            <li class="li">Pipeline error records - You can now write error records to Google
                                Pub/Sub, Google Cloud Storage, or an MQTT broker.</li>

                            <li class="li">Snapshot enhancements:<ul class="ul" id="concept_cjx_y4k_wbb__ul_cv5_53l_wbb">
                                    <li class="li">You can now configure the pipeline to take a snapshot of
                                        data if the pipeline fails due to a data-related
                                        exception.</li>

                                    <li class="li">You can now download snapshots through the UI and the REST
                                        API.</li>

                                </ul>
</li>

                            <li class="li">Time zone enhancement - Time zones have been organized and updated
                                to use JDK 8 names. This should make it easier to select time zones
                                in stage properties. In the rare case that your pipeline uses a
                                format not supported by JDK 8, edit the pipeline to select a
                                compatible time zone.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_rrq_v3k_kbb">
 <h2 class="title topictitle2">What's New in 2.7.2.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.7.2.0 includes the following new features:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rrq_v3k_kbb__ul_ibg_djk_kbb">
                            <li class="li"><a class="xref" href="../Origins/KafkaMultiConsumer.html#concept_ccs_fn4_x1b" title="You can add custom Kafka configuration properties to the Kafka Multitopic Consumer.When you use an origin to read log data, you define the format of the log files to be read.">New Kafka Multitopic Consumer origin</a> - A new origin that
                                reads messages from multiple Kafka topics. Creates multiple threads
                                to enable parallel processing in a multithreaded pipeline. </li>

                            <li class="li"><a class="xref" href="../Origins/KinConsumer.html#task_p4b_vv4_yr">Kinesis Consumer origin enhancement</a> - You can now
                                configure the origin to start reading messages from a specified
                                timestamp.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rrq_v3k_kbb__ul_crw_frk_kbb">
                            <li class="li"><a class="xref" href="../Destinations/BigQuery.html#concept_hj4_brk_dbb" title="The Google BigQuery destination maps fields from records to BigQuery columns in existing tables based on matching names and compatible data types. If needed, the destination converts Data Collector data types to BigQuery data types.When configured to use the Google Cloud service account credentials file, the destination checks for the file defined in the destination properties.">New
                                    Google BigQuery destination</a> - A new destination that
                                streams data into Google BigQuery.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_rr2_mbz_w1b">
    <h2 class="title topictitle2">What's New in 2.7.1.1</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.7.1.1 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rr2_mbz_w1b__ul_gr5_hvr_1bb">
                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin enhancement</a> - You can now specify a
                                Connection Timeout advanced property. </li>

                            <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_m43_zzm_dbb" title="The JDBC Multitable Consumer origin can read from views in addition to tables.">JDBC Multitable Consumer origin enhancement</a> - You can
                                now use the origin to read from views in addition to tables.</li>

                            <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#task_bqt_mx3_h1b">OPC UA Client origin enhancement</a> - You can now configure
                                channel properties, such as the maximum chunk or message size.</li>

                            <li class="li"><a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client origin enhancement </a>- You can now
                                configure a JDBC Fetch Size property to determine the minimum number
                                of records that the origin waits for before passing data to the
                                pipeline. When writing to the destination is slow, use the default
                                of 1 record to improve performance. Previously, the origin used the
                                Oracle JDBC driver default of 10 records.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executor</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_rr2_mbz_w1b__ul_zzr_ztn_2bb">
                            <li class="li"><a class="xref" href="../Executors/MapRFSFileMeta.html#concept_ohx_r5h_z1b">New MapR FS File Metadata executor</a> - The new executor
                                can change file metadata, create an empty file, or remove a file or
                                directory in MapR each time it receives an event. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="unique_1257448973">
    <h2 class="title topictitle2">What's New in 2.7.1.0</h2>

    <div class="body conbody">
        <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.7.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">Data Collector includes the following upgraded stage library:<ul class="ul" id="unique_1257448973__ul_s1w_cvr_1bb">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">Jython 2.7.1</a></li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_1257448973__ul_gr5_hvr_1bb">
                            <li class="li"><a class="xref" href="../Origins/AzureEventHub.html#concept_c1z_15q_1bb">New
                                    Azure Event Hub Consumer origin</a> - A multithreaded origin
                                that reads data from Microsoft Azure Event Hub.</li>

                            <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#concept_p25_wm2_dbb">OPC UA Client origin enhancement</a> - You can now specify
                                node information in a file. Or have the origin browse for nodes to
                                use based on a specified root node.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_1257448973__ul_rh5_3vr_1bb">
                            <li class="li"><a class="xref" href="../Processors/SchemaGenerator.html#concept_rfz_ks3_x1b">New Schema Generator processor</a> - A processor that
                                generates a schema for each record and writes the schema to a record
                                header attribute. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="unique_1257448973__ul_s22_lvr_1bb">
                            <li class="li"><a class="xref" href="../Destinations/AzureEventHubProducer.html#concept_xq5_d5q_1bb">New Azure Event Hub Producer destination</a> - A destination
                                that writes data to Microsoft Azure Event Hub.</li>

                            <li class="li"><a class="xref" href="../Destinations/AzureIoTHub.html#concept_pnd_jkq_1bb" title="Data Collector functions as a simulated device that sends messages to Azure IoT Hub. Before you configure the Azure IoT Hub Producer destination, register Data Collector as a device in your IoT hub.">New Azure IoT Hub Producer destination</a> - A destination
                                that writes data to Microsoft Azure IoT Hub.</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="unique_2019246562">
 <h2 class="title topictitle2">What's New in 2.7.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data
                  Collector</span>
            version 2.7.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Credential Stores</dt>

                <dd class="dd"><span class="ph">Data
                  Collector</span> now has a <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential
                        store</a> API that integrates with the following credential store
                        systems:<ul class="ul" id="unique_2019246562__ul_vgr_3cz_w1b">
                        <li class="li">Java keystore</li>

                        <li class="li">Hashicorp Vault</li>

                    </ul>
</dd>

                <dd class="dd">
                    <p class="p">You define the credentials required by external systems - user names,
                        passwords, or access keys - in a Java keystore file or in Vault. Then you
                        use <a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential expression language functions</a> in JDBC stage
                        properties to retrieve those values, instead of directly entering credential
                        values in stage properties. </p>

                </dd>

                <dd class="dd">
                    <div class="p">The following JDBC stages can use the new credential functions:<ul class="ul" id="unique_2019246562__ul_ezm_4cz_w1b">
                            <li class="li">JDBC Multitable Consumer origin</li>

                            <li class="li">JDBC Query Consumer origin</li>

                            <li class="li">Oracle CDC Client origin</li>

                            <li class="li">SQL Server CDC Client origin</li>

                            <li class="li">SQL Server Change Tracking origin</li>

                            <li class="li">JDBC Lookup processor</li>

                            <li class="li">JDBC Tee processor</li>

                            <li class="li">JDBC Producer destination</li>

                            <li class="li">JDBC Query executor</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Publish Pipeline Metadata to Cloudera Navigator (Beta)</dt>

                <dd class="dd">
                    <p class="p"><span class="ph">Data
                  Collector</span> now provides beta support for <a class="xref" href="../Configuration/PublishMetadata.html#concept_q4y_4mt_p1b" title="If you use Cloudera Manager, you can configure Data Collector to publish metadata about running pipelines to Cloudera Navigator. You can then use Cloudera Navigator to explore the pipeline metadata, including viewing lineage diagrams of the metadata.">publishing metadata</a> about running pipelines to Cloudera
                        Navigator. You can then use Cloudera Navigator to explore the pipeline
                        metadata, including viewing lineage diagrams of the metadata.</p>

                </dd>

                <dd class="dd">Feel free to try out this feature in a development or test <span class="ph">Data
                  Collector</span>, and send us your feedback. We are continuing to refine metadata publishing
                    as we gather input from the community and work with Cloudera.</dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <div class="p">Data Collector includes the following new stage libraries:<ul class="ul" id="unique_2019246562__ul_cxp_d2z_w1b">
                            <li class="li">Apache Kudu version 1.4.0</li>

                            <li class="li">Cloudera CDH version 5.12 distribution of Hadoop</li>

                            <li class="li">Cloudera version 5.12 distribution of Apache Kafka 2.1</li>

                            <li class="li">Google Cloud - Includes the Google BigQuery origin, Google Pub/Sub
                                Subscriber origin, and Google Pub/Sub Publisher destination.</li>

                            <li class="li">Java keystore credential store - For use with <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential stores</a>.</li>

                            <li class="li">Vault credential store - For use with <a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential stores</a>.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Data Collector Configuration</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_hc5_q2z_w1b">
                        <li class="li"><a class="xref" href="../Configuration/Vault-Overview.html#concept_bmq_gl1_mw">Access  Hashicorp Vault secrets</a> - The Data Collector Vault
                            integration now relies on Vault's App Role authentication backend.
                            Previously, Data Collector relied on Vault's App ID authentication
                            backend. Hashicorp has deprecated the App ID authentication
                            backend.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#concept_dcq_mpk_f1b">New Hadoop user impersonation property</a> - When you enable
                            Data Collector to impersonate the current Data Collector user when
                            writing to Hadoop, you can now also configure Data Collector to make the
                            username lowercase. This can be helpful with case-sensitive
                            implementations of LDAP.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New Java security properties</a> - The Data Collector
                            configuration file now includes properties with a "java.security."
                            prefix, which you can use to configure Java security properties.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New property to define the amount of time to cache DNS
                                lookups</a> - By default, the
                            java.security.networkaddress.cache.ttl property is set to 0 so that the
                            JVM uses the Domain Name Service (DNS) time to live value, instead of
                            caching the lookups for the lifetime of the JVM.</li>

                        <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_cy3_r44_z1b">SDC_HEAPDUMP_PATH enhancement</a> - The new default file name,
                                <samp class="ph codeph">$SDC_LOG/sdc_heapdump_${timestamp}.hprof</samp>, includes
                            a timestamp so you can write multiple heap dump files to the specified
                            directory.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Dataflow Triggers</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_fg2_gfz_w1b">
                        <li class="li"><a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_amg_2qr_t1b">Pipeline events</a> - The event framework now generates pipeline
                            lifecycle events when the pipeline stops and starts. You can pass each
                            pipeline event to an executor or to another pipeline for more complex
                            processing. Use pipeline events to trigger tasks before pipeline
                            processing begins or after it stops.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_tpp_sfz_w1b">
                        <li class="li"><a class="xref" href="../Origins/BigQuery.html#concept_cg3_y3v_q1b" title="The Google BigQuery origin executes a query job and reads the result from Google BigQuery.">New Google
                                BigQuery origin</a> - An origin that executes a query job and
                            reads the result from Google BigQuery.</li>

                        <li class="li"><a class="xref" href="../Origins/PubSub.html#concept_pjw_qtl_r1b" title="The Google Pub/Sub Subscriber origin consumes messages from a Google Pub/Sub subscription.">New Google
                                Pub/Sub Subscriber origin</a> - A multithreaded origin that
                            consumes messages from a Google Pub/Sub subscription.</li>

                        <li class="li"><a class="xref" href="../Origins/OPCUAClient.html#concept_nmf_1ly_f1b">New OPC UA
                                Client origin</a> - An origin that processes data from an OPC UA
                            server.</li>

                        <li class="li"><a class="xref" href="../Origins/SQLServerCDC.html#concept_ut3_ywc_v1b" title="You can define the initial order that the origin uses to read the tables.">New SQL
                                Server CDC Client origin</a> - A multithreaded origin that reads
                            data from Microsoft SQL Server CDC tables. </li>

                        <li class="li"><a class="xref" href="../Origins/SQLServerChange.html#concept_ewq_b2s_r1b" title="You can define the initial order that the origin uses to read the tables.">New SQL
                                Server Change Tracking origin</a> - A multithreaded origin that
                            reads data from Microsoft SQL Server change tracking tables and
                            generates the latest version of each record.</li>

                        <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory origin event enhancements</a> - The Directory origin
                            can now generate no-more-data events when it completes processing all
                            available files and the batch wait time has elapsed without the arrival
                            of new files. Also, the File Finished event now includes the number of
                            records and files processed. </li>

                        <li class="li"><a class="xref" href="../Origins/HadoopFS-origin.html#concept_ogc_xzd_f1b">Hadoop
                                FS origin enhancement</a> - The Hadoop FS origin now allows you
                            to read data from other file systems using the Hadoop FileSystem
                            interface. Use the Hadoop FS origin in cluster batch pipelines.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#task_akl_rkz_5r">HTTP
                                Client origin enhancement</a> - The HTTP Client origin now allows
                            time functions and datetime variables in the request body. It also
                            allows you to specify the time zone to use when evaluating the request
                            body. </li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_anf_ss4_qy">HTTP Server origin enhancement</a> - The HTTP Server origin can
                            now process Avro files.</li>

                        <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Query Consumer origin enhancement</a> - You can now
                            configure the behavior for the origin when it encounters data of an
                            unknown data type.</li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y" title="You define the group of tables that the JDBC Multitable Consumer origin reads by defining a table name pattern for the table configuration. The origin reads all tables whose names match the pattern. The JDBC Multitable Consumer origin uses an offset column and initial offset value to determine where to start reading data within tables and partitions.The JDBC Multitable Consumer origin can read from views in addition to tables. You can define the initial order that the origin uses to read the tables.">JDBC Multitable Consumer origin enhancements</a>:<ul class="ul" id="unique_2019246562__ul_gnl_rhz_w1b">
                                <li class="li">You can now use the origin to perform multithreaded processing
                                    of partitions within a table. Use partition processing to handle
                                    even larger volumes of data. This enhancement also includes new
                                    JDBC header attributes.<p class="p">By default, all new pipelines use
                                        partition processing when possible. Upgraded pipelines use
                                        multithreaded table processing to preserve previous
                                        behavior.</p>
</li>

                                <li class="li">You  can now configure the behavior for the origin when it
                                    encounters data of an unknown data type.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle CDC
                                Client origin enhancements</a>:<ul class="ul" id="unique_2019246562__ul_ych_yhz_w1b">
                                <li class="li">The origin can now buffer data locally rather than utilizing
                                    Oracle LogMiner buffers.</li>

                                <li class="li">You can now specify the behavior when the origin encounters an
                                    unsupported field type - send to the pipeline, send to error, or
                                    discard.</li>

                                <li class="li">You can configure the origin to include null values passed from
                                    the LogMiner full supplemental logging. By default, the origin
                                    ignores null values.</li>

                                <li class="li">You now must select the target server time zone for the origin. </li>

                                <li class="li">You can now configure a query timeout for the origin.</li>

                                <li class="li">The origin now includes the row ID in the oracle.cdc.rowId
                                    record header attribute and can include the LogMiner redo query
                                    in the oracle.cdc.query record header attribute.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Origins/RabbitMQ.html#concept_rg5_yts_y1b">RabbitMQ Consumer origin enhancement</a> - When available, the
                            origin now provides attributes generated by RabbitMQ, such as
                            contentType, contentEncoding, and deliveryMode, as record header
                            attributes.</li>

                        <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_bqt_tl4_sz">TCP
                                Server origin enhancement</a> - The origin can now process
                            character-based data that includes a length prefix. </li>

                        <li class="li"><a class="xref" href="../Origins/UDP.html#concept_jhh_ryx_r1b">UDP Source
                                origin enhancement</a> - The origin can now process binary and
                            character-based raw data.</li>

                        <li class="li">New last-modified time record header attribute - <a class="xref" href="../Origins/Directory.html#concept_tlj_3g1_2z">Directory</a>, <a class="xref" href="../Origins/FileTail.html#concept_tlj_3g1_2z" title="A tag is an optional record header attribute that you can define for sets of files. In the pipeline, you can use a function to return the value of the tag attribute.">File Tail</a>, and <a class="xref" href="../Origins/SFTP.html#concept_tlj_3g1_2z">SFTP/FTP Client</a> origins now include the last modified time
                            for the originating file for a record in an mtime record header
                            attribute.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_py4_vmz_w1b">
                        <li class="li"><a class="xref" href="../Processors/DataParser.html#concept_xw3_4xk_r1b">New Data
                                Parser processor</a> - Use the new processor to extract NetFlow
                            or syslog messages as well as other supported data formats that are
                            embedded in a field. </li>

                        <li class="li"><a class="xref" href="../Processors/JSONParser.html#concept_bs1_4t3_yq" title="Configure a JSON Parser to parse a JSON object in a String field.">New JSON
                                Generator processor</a> - Use the new processor to serialize data
                            from a record field to a JSON-encoded string.</li>

                        <li class="li"><a class="xref" href="../Processors/KuduLookup.html#concept_a1x_3wl_p1b" title="The Kudu Lookup processor performs lookups in a Kudu table and passes the lookup values to fields. Use the Kudu Lookup to enrich records with additional data.">New Kudu
                                Lookup processor</a> - Use the new processor to perform lookups
                            in Kudu to enrich records with additional data.</li>

                        <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_jv2_jjn_l1b">Hive Metadata processor enhancement</a> - You can now configure
                            custom record header attributes for metadata records.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_wdl_lnz_w1b">
                        <li class="li"><a class="xref" href="../Destinations/PubSubPublisher.html#concept_qsj_hk1_v1b" title="When configured to use the Google Cloud service account credentials file, the destination checks for the file defined in the destination properties.">New Google Pub/Sub Publisher destination</a> - A destination
                            that publishes messages to Google Pub/Sub.</li>

                        <li class="li"><a class="xref" href="../Destinations/JMSProducer.html#concept_sfz_ww5_n1b" title="Before you use the JMS Producer, install the JMS drivers for the implementation that you are using.">New
                                JMS Producer destination</a> - A destination that writes data to
                            JMS.</li>

                        <li class="li">Amazon S3 destination enhancements:<ul class="ul" id="unique_2019246562__ul_ls3_d4z_w1b">
                                <li class="li">You can now use expressions in the <a class="xref" href="../Destinations/AmazonS3.html#concept_bnp_gwp_f1b">Bucket property</a> for the Amazon S3 destination. This
                                    enables you to write records dynamically based expression
                                    evaluation.</li>

                                <li class="li">The Amazon S3 object written <a class="xref" href="../Destinations/AmazonS3.html#concept_nly_sw2_px">event record</a> now includes the number of records
                                    written to the object.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#task_jfl_nf4_zx">Azure Data Lake Store destination enhancement</a> - The Client
                            ID and Client Key properties have been renamed Application ID and
                            Application Key to align with the updated property names in the new
                            Azure portal.</li>

                        <li class="li"><a class="xref" href="../Destinations/Cassandra.html#concept_ajh_vhp_x1b">Cassandra destination enhancement</a> - The destination now
                            supports Kerberos authentication if you have installed the DataStax
                            Enterprise Java driver. </li>

                        <li class="li"><a class="xref" href="../Destinations/Elasticsearch.html#task_uns_gtv_4r">Elasticsearch destination enhancement</a> - The destination can
                            now create parent-child relationships between documents in the same
                            index.</li>

                        <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive
                                Metastore destination</a> - You can now configure the destination
                            to create custom record header attributes.</li>

                        <li class="li"><a class="xref" href="../Destinations/KProducer.html#concept_lww_3b3_kr">Kafka Producer destination enhancement</a> - The destination can
                            now write XML documents.</li>

                        <li class="li"><a class="xref" href="../Destinations/Solr.html#task_ld1_phr_wr">Solr
                                destination enhancement</a> - You can now configure the
                            destination to skip connection validation when the Solr configuration
                            file, <samp class="ph codeph">solrconfig.xml</samp>, does not define the default
                            search field (“df”) parameter.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_gqx_fpz_w1b">
                        <li class="li"><a class="xref" href="../Executors/AmazonS3.html#concept_mvh_bnm_f1b" title="When Data Collector uses the Amazon S3 executor, it must pass credentials to Amazon Web Services.">New Amazon
                                S3 executor</a> - Use the Amazon S3 executor to create new Amazon
                            S3 objects for the specified content or add tags to existing objects
                            each time it receives an event.</li>

                        <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_yf2_hc4_x1b">HDFS File Metadata executor enhancement</a> - The executor can
                            now remove a file or directory when it receives an event.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Dataflow Performance Manager</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_dwx_lpz_w1b">
                        <li class="li"><a class="xref" href="../DPM/PipelineManagement.html#task_c4x_vff_p1b">Revert changes to published pipelines</a> - If you update a
                            published pipeline but decide not to publish the updates to <span class="ph">DPM</span> as a new version, you can revert the changes made to the pipeline
                            configuration.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_ngb_qpz_w1b">
                        <li class="li"><a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_pm4_txm_vq">Pipeline error handling enhancements</a>: <ul class="ul" id="unique_2019246562__ul_rq1_tpz_w1b">
                                <li class="li">Use the new Error Record Policy to specify the version of the
                                    record to include in error records. </li>

                                <li class="li">You can now write error records to Amazon Kinesis Streams.</li>

                            </ul>
</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/RecordHeaderAttributes.html#concept_itf_55z_dz">Error records enhancement</a> - Error records now include the
                            user-defined stage label in the errorStageLabel header attribute.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#concept_s4p_ns5_nz">Pipeline state enhancements</a> - Pipelines can now display the
                            following new states: STARTING_ERROR, STOPPING_ERROR, and
                            STOP_ERROR.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Data Formats</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_btq_yqz_w1b">
                        <li class="li"><a class="xref" href="../Data_Formats/WritingXML.html#concept_t2m_hhx_41b">Writing
                                XML</a> - You can now use the Google Pub/Sub Publisher, JMS
                            Producer, and Kafka Producer destinations to write XML documents to
                            destination systems. Note the record structure requirement before you
                            use this data format. </li>

                        <li class="li">Avro:<ul class="ul" id="unique_2019246562__ul_k4b_yrz_w1b">
                                <li class="li">Origins now write the Avro schema to an avroSchema record header
                                    attribute.</li>

                                <li class="li">Origins now include precision and scale <a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">field attributes</a> for every Decimal field.</li>

                                <li class="li">Data Collector now supports the time-based logical types added
                                    to Avro in version 1.8.</li>

                            </ul>
</li>

                        <li class="li">Delimited - Data Collector can now continue processing records with
                            delimited data when a row has more fields than the header. Previously,
                            rows with more fields than the header were sent to error. </li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Cluster Pipelines</dt>

                <dd class="dd">
                    <div class="p">This release includes the following <a class="xref" href="../Cluster_Mode/ClusterPipelines_title.html#concept_rjc_4m5_lx" title="Data Collector can run a cluster pipeline using the cluster batch or the cluster streaming execution mode.">Cluster Yarn Streaming enhancements</a>:<ul class="ul" id="unique_2019246562__ul_b14_nsz_w1b">
                            <li class="li">Use a new Worker Count property to limit the number of worker nodes
                                used in Cluster Yarn Streaming pipelines. By default, a Data
                                Collector worker is spawned for each partition in the topic. </li>

                            <li class="li">You can now define Spark configuration properties to pass to the
                                spark-submit script.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Expression Language</dt>

                <dd class="dd">
                    <div class="p">This release includes the following new functions:<ul class="ul" id="unique_2019246562__ul_abf_qsz_w1b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential:get()</a> - Returns credential values from a
                                credential store.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_yvc_3qs_r1b">credential:getWithOptions()</a> - Returns credential values
                                from a credential store using additional options to communicate with
                                the credential store.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ndj_43v_1r" title="Error record functions provide information about error records. Use error functions to process error records.">record:errorStageLabel()</a> - Returns the user-defined name
                                of the stage that generated the error record.</li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">list:join()</a>  - Merges elements in a List field into a
                                String field, using the specified separator between elements. </li>

                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">list:joinSkipNulls()</a> - Merges elements in a List field
                                into a String field, using the specified separator between elements
                                and skipping null values.</li>

                        </ul>
</div>

                </dd>

                <dd class="dd">
                    <div class="p">
                        <ul class="ul" id="unique_2019246562__ul_k5c_rg4_z1b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">str:indexOf()</a> - Returns the index within a string of the
                                first occurrence of the specified subset of characters.</li>

                        </ul>

                    </div>

                </dd>

            
            
                <dt class="dt dlterm">Miscellaneous</dt>

                <dd class="dd">
                    <ul class="ul" id="unique_2019246562__ul_y2k_htz_w1b">
                        <li class="li"><a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">Global bulk edit mode</a> - In any property where you would
                            previously click an Add icon to add additional configurations, you can
                            now switch to bulk edit mode to enter a list of configurations in JSON
                            format.</li>

                        <li class="li">Snapshot enhancement - Snapshots no longer produce empty batches when
                            waiting for data.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/Webhooks.html#concept_rby_1rl_rz">Webhooks enhancement</a> - You can use several new pipeline
                            state notification parameters in webhooks.</li>

                    </ul>

                </dd>

            
        </dl>

 </div>

</div>
<div class="topic concept nested1" id="concept_avm_x1y_h1b">
 <h2 class="title topictitle2">What's New in 2.6.0.1</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.6.0.1 includes the following enhancement:<ul class="ul" id="concept_avm_x1y_h1b__ul_x5t_gby_h1b">
                <li class="li"><a class="xref" href="../Origins/KinConsumer.html#concept_dt1_4mq_h1b">Kinesis
                        Consumer origin</a> - You can now reset the origin for Kinesis Consumer
                    pipelines. Resetting the origin for Kinesis Consumer differs from other origins,
                    so please note the requirement and guidelines.</li>

            </ul>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_bsw_cky_11b">
 <h2 class="title topictitle2">What's New in 2.6.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.6.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_s2s_lky_11b">
                            <li class="li"><a class="xref" href="../Installation/MapR-Prerequisites.html#concept_jgs_qpg_2v" title="Due to licensing restrictions, StreamSets cannot distribute MapR libraries with Data Collector. As a result, you must perform additional steps to enable the Data Collector machine to connect to MapR. Data Collector does not display MapR origins and destinations in the stage library nor the MapR Streams statistics aggregator in the pipeline properties until you perform these prerequisites.">MapR prerequisites</a> - You can now run the
                                    <samp class="ph codeph">setup-mapr</samp> command in interactive or
                                non-interactive mode. In interactive mode, the command prompts you
                                for the MapR version and home directory. In non-interactive mode,
                                you define the MapR version and home directory in environment
                                variables before running the command.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Stage Libraries</dt>

                    <dd class="dd">Data Collector now supports the following <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage libraries</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_zcv_4ky_11b">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">Hortonworks version 2.6 distribution of Apache
                                    Hadoop</p>

                            </li>

                            <li class="li">Cloudera distribution of Spark 2.1</li>

                            <li class="li">MapR distribution of Spark 2.1</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Data Collector Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_jys_cpm_d1b">
                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New buffer size configuration</a> - You can now use a new
                                parser.limit configuration property to increase the Data Collector
                                parser buffer size. The parser buffer is used by the origin to
                                process many data formats, including Delimited, JSON, and XML. The
                                parser buffer size limits the size of the records that origins can
                                process. The Data Collector parser buffer size is 1048576 bytes by
                                default.</li>

                        </ul>

                    </dd>

                
            </dl>
<dl class="dl">
                
                    <dt class="dt dlterm">Drift Synchronization Solution for Hive</dt>

                    <dd class="dd"><ul class="ul" id="concept_bsw_cky_11b__ul_x5p_zky_11b">
                            <li class="li"><a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_ndg_3zw_vz">Parquet support</a> - You can now use the <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_phk_bdf_2w">Drift Synchronization Solution for Hive</a> to generate
                                Parquet files. Previously, the Data Synchronization Solution
                                supported only Avro data. This enhancement includes the following
                                    updates:<ul class="ul" id="concept_bsw_cky_11b__ul_dt1_sky_11b">
                                    <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv" title="The Hive Metadata processor works with the Hive Metastore destination, and the Hadoop FS or MapR FS destinations as part of the Drift Synchronization Solution for Hive.">Hive Metadata processor</a> data format property -
                                        Use the new data format property to indicate the data format
                                        to use.</li>

                                    <li class="li">Parquet support in the <a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive Metastore destination</a> - The destination can
                                        now create and update Parquet tables in Hive. The
                                        destination no longer includes a data format property since
                                        that information is now configured in the Hive Metadata
                                        processor. </li>

                                </ul>
</li>

                        </ul>
See the documentation for implementation details and a <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_vl3_v2f_zz" title="Now to process the metadata records - and to automatically create and update Parquet tables in Hive - you need the Hive Metastore destination.">Parquet case study</a>. </dd>

                
                
                    <dt class="dt dlterm">Multithreaded Pipelines</dt>

                    <dd class="dd">The <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">multithreaded framework</a> includes the following enhancements:<ul class="ul" id="concept_bsw_cky_11b__ul_a2d_4ry_11b">
                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wcz_tpd_py">Origins for multithreaded pipelines</a> - You can now use
                                the following origins to create multithreaded pipelines:<ul class="ul" id="concept_bsw_cky_11b__ul_efl_4ry_11b">
                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/CoAPServer.html#concept_wfy_ghn_sz" title="Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Server origin is a multithreaded origin that listens on a CoAP endpoint and processes the contents of all authorized CoAP requests.">CoAP Server origin</a></p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p dir="ltr" class="p"><a class="xref" href="../Origins/TCPServer.html#concept_ppm_xb1_4z">TCP Server origin</a></p>

                                    </li>

                                </ul>
</li>

                            <li class="li">Multithreaded origin icons - The icons for multithreaded origins now
                                include the following multithreaded indicator: <img class="image" id="concept_bsw_cky_11b__image_al4_tpm_d1b" src="../Graphics/icon_Multithreaded.png" height="16" width="14" /><p class="p">For example, here’s the updated Elasticsearch
                                    origin icon:</p>
<p class="p"><img class="image" id="concept_bsw_cky_11b__image_iml_ypm_d1b" src="../Graphics/Multithreaded-icon.png" height="40" width="63" /></p>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Dataflow Triggers / Event Framework</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_ot4_ssy_11b">
                            <li class="li"><a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_rxg_shn_lx">New executors</a> - You can now use the following executors
                                to perform tasks upon receiving an event:<ul class="ul" id="concept_bsw_cky_11b__ul_pd2_tsy_11b">
                                    <li class="li"><a class="xref" href="../Executors/Email.html#concept_sjs_sfp_qz">Email executor</a></li>

                                    <li class="li"><a class="xref" href="../Executors/Shell.html#concept_jsr_zpw_tz">Shell executor</a>
                                    </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager (DPM)</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_pht_hld_d1b">
                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_h2q_mb5_xw" title="A DPM job defines the pipeline to run and the Data Collectors that run the pipeline. When you start a job, DPM remotely runs the pipeline on the group of Data Collectors. To monitor the job statistics and metrics within DPM, you must configure the pipeline to write statistics to DPM or to another system.">Pipeline statistics</a> - You can now configure a pipeline
                                to <a class="xref" href="../DPM/AggregatedStatistics.html#concept_abc_1w1_c1b" title="When you write statistics directly to DPM, DPM does not generate a system pipeline for the job. Instead, the Data Collector directly sends the statistics to DPM.">write statistics directly to DPM</a>. Write statistics
                                directly to DPM when you run a job for the pipeline on a single Data
                                Collector. <p class="p">When you run a job on multiple Data Collectors, a
                                    remote pipeline instance runs on each of the Data Collectors. To
                                    view aggregated statistics for the job within DPM, you must
                                    configure the pipeline to write the statistics to a Kafka
                                    cluster, Amazon Kinesis Streams, or SDC RPC.</p>
</li>

                            <li class="li"><a class="xref" href="../DPM/PipelineManagement.html#task_rxy_xqc_fx">Update
                                    published pipelines</a> - When you update a published
                                pipeline, Data Collector now displays a red asterisk next to the
                                pipeline name to indicate that the pipeline has been updated since
                                it was last published.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_xb4_bty_11b">
                            <li class="li"><a class="xref" href="../Origins/CoAPServer.html#concept_wfy_ghn_sz" title="Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Server origin is a multithreaded origin that listens on a CoAP endpoint and processes the contents of all authorized CoAP requests.">New CoAP
                                    Server origin</a> - An origin that listens on a CoAP endpoint
                                and processes the contents of all authorized CoAP requests. The
                                origin performs parallel processing and can generate multithreaded
                                pipelines. </li>

                            <li class="li"><a class="xref" href="../Origins/TCPServer.html#concept_ppm_xb1_4z">New TCP
                                    Server origin</a> - An origin that listens at the specified
                                ports, establishes TCP sessions with clients that initiate TCP
                                connections, and then processes the incoming data. The origin can
                                process NetFlow, syslog, and most Data Collector data formats as
                                separated records. You can configure custom acknowledgement messages
                                and use a new batchSize variable, as well as other expressions, in
                                the messages.</li>

                            <li class="li"><a class="xref" href="../Origins/SFTP.html#concept_g5p_5ks_b1b">SFTP/FTP Client origin enhancement</a> - You can now specify
                                the first file to process. This enables you to skip processing files
                                with earlier timestamps. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_dvb_jty_11b">
                            <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_ldh_sct_gv" title="You can choose the processing mode that the Groovy Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode.In scripts that process list-map data, treat the data as maps. You can call external Java code from the Groovy Evaluator. Simply install the external Java library to make it available to the Groovy Evaluator. Then, call the external Java code from the Groovy script that you develop for the processor.">Groovy</a>, <a class="xref" href="../Processors/JavaScript.html#concept_n2p_jgf_lr" title="You can choose the processing mode that the JavaScript Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode.In scripts that process list-map data, treat the data as maps. You can call external Java code from the JavaScript Evaluator. Simply install the external Java library to make it available to the JavaScript Evaluator. Then, call the external Java code from the script that you develop for the processor.">JavaScript</a>, and <a class="xref" href="../Processors/Jython.html#concept_a1h_lkf_lr" title="You can choose the processing mode that the Jython Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode.In scripts that process list-map data, treat the data as maps. You can call external Java code from the Jython Evaluator. Simply install the external Java library to make it available to the Jython Evaluator. Then, call the external Java code from the Jython script that you develop for the processor.">Jython
                                    Evaluator</a> processor enhancements:<ul class="ul" id="concept_bsw_cky_11b__ul_e4s_jty_11b">
                                    <li class="li">You can now include some methods of the sdcFunctions
                                        scripting object in the initialization and destroy scripts
                                        for the processors.</li>

                                    <li class="li">You can now use runtime parameters in the code developed for
                                        a Groovy Evaluator processor.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv" title="The Hive Metadata processor works with the Hive Metastore destination, and the Hadoop FS or MapR FS destinations as part of the Drift Synchronization Solution for Hive.">Hive
                                    Metadata processor enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_v3b_lty_11b">
                                    <li class="li">The Hive Metadata processor can now <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_ndg_3zw_vz">process Parquet data as part of the Drift
                                            Synchronization Solution for Hive</a>. </li>

                                    <li class="li">You can now specify the data format to use: Avro or
                                        Parquet.</li>

                                    <li class="li">You can now configure an expression that defines comments
                                        for generated columns. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#concept_ysc_ccy_hw" title="The JDBC Lookup processor uses a JDBC connection to perform lookups in a database table and pass the lookup values to fields. Use the JDBC Lookup to enrich records with additional data.">JDBC
                                    Lookup processor enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_vls_4ty_11b">
                                    <li class="li">The JDBC Lookup processor can now return multiple values.
                                        You can now configure the lookup to return the first value
                                        or to return all matches as separate records. </li>

                                    <li class="li">When you <a class="xref" href="../Processors/JDBCLookup.html#concept_k3l_hrd_wz" title="When you monitor a pipeline that includes the JDBC Lookup processor, the Summary tab displays statistics about the queries that the JDBC Lookup processor performs. Use the statistics to help identify any performance bottlenecks encountered by the pipeline.">monitor a pipeline that includes the JDBC Lookup
                                            processor</a>, you can now view stage statistics
                                        about the number of queries the processor makes and the
                                        average time of the queries.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Processors/Spark.html#concept_nbj_1jb_c1b">Spark Evaluator enhancement</a> - The Spark Evaluator now
                                supports Spark 2.x.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_p5x_qty_11b">
                            <li class="li"><a class="xref" href="../Destinations/CoAPClient.html#concept_hw5_s3n_sz" title="Constrained Application Protocol (CoAP) is a web transfer protocol designed for machine-to-machine devices. The CoAP Client destination writes data to a CoAP endpoint. Use the destination to send requests to a CoAP resource URL.">New
                                    CoAP Client destination</a> - A destination that writes to a
                                CoAP endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Drift Synchronization Solution for Hive.">Hive Metastore destination enhancements</a>:<ul class="ul" id="concept_bsw_cky_11b__ul_y5w_sty_11b">
                                    <li class="li">The destination can now <a class="xref" href="../Destinations/HiveMetastore.html#concept_wyr_5jv_hw">create and update Parquet tables in Hive</a>. </li>

                                    <li class="li">Also, the data format property has been removed. You now
                                        specify the data format in the <a class="xref" href="../Processors/HiveMetadata.html#concept_rz5_nft_zv" title="The Hive Metadata processor works with the Hive Metastore destination, and the Hadoop FS or MapR FS destinations as part of the Drift Synchronization Solution for Hive.">Hive Metadata processor.</a>
                                        <p class="p">Since the Hive Metastore previously supported only Avro
                                            data, there is no upgrade impact. </p>
</li>

                                </ul>
</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/Kudu.html#concept_chy_xxg_4v" title="The Kudu destination writes data to a Kudu cluster.">Kudu
                                        destination enhancement</a> - You can use the new
                                    Mutation Buffer Space property to set the buffer size that the
                                    Kudu client uses to write each batch.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_xl4_b5y_11b">
                            <li class="li"><a class="xref" href="../Executors/Email.html#concept_sjs_sfp_qz">New Email
                                    executor</a> - Use to send custom emails upon receiving an
                                event. For a case study, see <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_t2t_lp5_xz">Case Study: Sending Email</a>.<ul class="ul" id="concept_bsw_cky_11b__ul_ekw_b5y_11b">
                                    <li dir="ltr" class="li">
                                        <p class="p"><a class="xref" href="../Executors/Shell.html#concept_jsr_zpw_tz">New Shell executor</a> - Use to execute shell
                                            scripts upon receiving an event. </p>

                                    </li>

                                    <li dir="ltr" class="li">
                                        <p class="p"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor enhancement</a> - A new
                                            Batch Commit property allows the executor to commit to
                                            the database after each batch. Previously, the executor
                                            did not call commits by default.</p>

                                        <p class="p">For new pipelines, the property is enabled by default.
                                            For upgraded pipelines, the property is disabled to
                                            prevent changes in pipeline behavior. </p>

                                    </li>

                                    <li dir="ltr" class="li"><a class="xref" href="../Executors/Spark.html#concept_vbm_ywb_c1b">Spark executor enhancement</a> - The executor now
                                        supports Spark 2.x. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">REST API / Command Line Interface</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_ktv_k5y_11b">
                            <li class="li">Offset management - Both the REST API and <a class="xref" href="../Administration/Administration_title.html#concept_ywx_d5x_pt" title="Data Collector provides a command line interface that includes a basic cli command. Use the command to perform some of the same actions that you can complete from the Data Collector UI. Data Collector must be running before you can use the cli command.">command line interface</a> can now retrieve the last-saved
                                offset for a pipeline and set the offset for a pipeline when it is
                                not running. Use these commands to implement pipeline failover using
                                an external storage system. Otherwise, pipeline offsets are managed
                                by Data Collector and there is no need to update the offsets.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_fm5_m5y_11b">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">vault:read enhancement</a> - The vault:read function now
                                supports returning the value for a key nested in a map.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">General</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_bsw_cky_11b__ul_rlz_55y_11b">
                            <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_szj_3mw_xz" title="When you generate a support bundle, you choose the information to include in the bundle. Only users with the admin role can generate support bundles.By default, the generators redact all passwords entered in pipelines, configuration files, or resource files. You can customize the generators to redact other sensitive information, such as machine names or usernames.">Support bundles</a> - You can now use Data Collector to
                                generate a support bundle. A support bundle is a ZIP file that
                                includes Data Collector logs, environment and configuration
                                information, pipeline JSON files, resource files, and pipeline
                                snapshots. <p class="p">You upload the generated file to the StreamSets
                                    support team so that we can use the information to troubleshoot
                                    your support tickets.</p>
</li>

                            <li class="li">
                                <div class="p"><a class="xref" href="../Pipeline_Configuration/SSL-TLS.html#concept_dd1_n3f_5z">TLS property enhancements</a> - Stages that support
                                    SSL/TLS now provide the following enhanced set of properties
                                    that enable more specific configuration:<ul class="ul" id="concept_bsw_cky_11b__ul_v5g_w5y_11b">
                                        <li class="li">Keystore and truststore type - You can now choose
                                            between Java Keystore (JKS) and PKCS-12 (p-12).
                                            Previously, Data Collector only supported JKS.</li>

                                        <li class="li">Transport protocols - You can now specify the transport
                                            protocols that you want to allow. By default, Data
                                            Collector allows only TLSv1.2. </li>

                                        <li class="li">Cipher suites - You can now specify the cipher suites to
                                            allow. Data Collector provides a modern set of default
                                            cipher suites. Previously, Data Collector always allowed
                                            the default cipher suites for the JRE.</li>

                                    </ul>
To avoid upgrade impact, all SSL/TLS/HTTPS properties in
                                    existing pipelines are preserved during upgrade. </div>

                            </li>

                            <li class="li">
                                <p class="p"><a class="xref" href="../Cluster_Mode/ClusterPipelines_title.html#concept_hmh_kfn_1s" title="A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode.">Cluster mode enhancement</a> - Cluster streaming mode
                                    now supports Spark 2.x. For information about using Spark 2.x
                                    stages with cluster mode, see <a class="xref" href="../Cluster_Mode/ClusterPipelines_title.html#concept_pdf_r5y_fz">Stage Limitations</a>.</p>

                            </li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_msl_yd4_fs">Precondition enhancement</a> - Stages with user-defined
                                preconditions now process all preconditions before passing a record
                                to error handling. This allows error records to include all
                                precondition failures in the error message.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_fdf_hrr_5q">Pipeline import</a>/<a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#task_dtz_4tr_5q">export enhancement</a> - When you export multiple pipelines,
                                Data Collector now includes all pipelines in a single zip file. You
                                can also import multiple pipelines from a single zip file. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_afr_hly_tz">
 <h2 class="title topictitle2">What's New in 2.5.1.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.5.1.0 includes the following enhancement:<ul class="ul" id="concept_afr_hly_tz__ul_uwf_lly_tz">
                <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New
                        stage library</a> - <span class="ph">Data
                  Collector</span> now supports the Cloudera CDH version 5.11 distribution of Hadoop and the
                    Cloudera version 5.11 distribution of Apache Kafka 2.1. <p class="p">Upgrading to this
                        version can require updating existing pipelines. For details, see <a class="xref" href="../Upgrade/Upgrade-ExternalSystems.html#concept_spf_2gq_vz">Working with Cloudera CDH 5.11 or Later</a>.</p>

                </li>

            </ul>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_ddx_bpm_pz">
 <h2 class="title topictitle2">What's New in 2.5.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data
                  Collector</span>
            version 2.5.0.0 includes the following new features and enhancements:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">Multithreaded Pipelines</dt>

                <dd class="dd">
                    <div class="p">The multithreaded framework includes the following enhancements:<ul class="ul" id="concept_ddx_bpm_pz__ul_gs3_3pm_pz">
                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_wcz_tpd_py">Origins for multithreaded pipelines</a> - You can now use
                                the following origins to create multithreaded pipelines:<ul class="ul" id="concept_ddx_bpm_pz__ul_hx3_mpm_pz">
                                    <li class="li">Elasticsearch origin</li>

                                    <li class="li">JDBC Multitable Consumer origin</li>

                                    <li class="li">Kinesis Consumer origin</li>

                                    <li class="li">WebSocket Server origin</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_fmg_pjd_mz">Maximum pipeline runners</a> - You can now configure a
                                maximum number of pipeline runners to use in a pipeline. Previously,
                                    <span class="ph">Data
                  Collector</span> generated pipeline runners based on the number of threads created
                                by the origin. This allows you to tune performance and resource
                                usage. By default, <span class="ph">Data
                  Collector</span> still generates runners based on the number of threads that the
                                origin uses. </li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_np1_pkz_ry">Record Deduplicator processor enhancement</a> - The
                                processor can now deduplicate records across all pipeline runners in
                                a multithreaded pipeline.</li>

                            <li class="li">Pipeline validation enhancement - The pipeline now displays
                                duplicate errors generated by using multiple threads as one error
                                message.</li>

                            <li class="li">Log enhancement - Multithreaded pipelines now include the runner ID
                                in log information. </li>

                            <li class="li"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_tdn_vwy_ry">Monitoring</a> - Monitoring now displays a histogram of
                                available pipeline runners, replacing the information previously
                                included in the Runtime Statistics list.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Pipelines</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_asz_cqm_pz">
                        <li class="li"><span class="ph">Data
                  Collector</span>
                            <a class="xref" href="../Configuration/RolesandPermissions.html#concept_i1p_hzd_yy">pipeline permissions</a> change - With this release, pipeline
                            permissions are no longer enabled by default. To enable pipeline
                            permissions, edit the <samp class="ph codeph">pipeline.access.control.enabled</samp>
                            <span class="ph">Data
                  Collector</span> configuration property.</li>

                        <li class="li"><a class="xref" href="../Executors/PipelineFinisher.html#concept_qzm_l4r_kz">Stop
                                pipeline execution</a> - You can configure pipelines to transfer
                            data and automatically stop execution based on an event such as reaching
                            the end of a table. The JDBC and Salesforce origins can generate events
                            when they reach the end of available data that the Pipeline Finisher
                            uses to stop the pipeline. Click <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_kff_ykv_lz">here</a> for a case study. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_rjh_ntz_qr" title="Runtime parameters are parameters that you define in a pipeline and then call from within that same pipeline. When you start the pipeline, you specify the parameter values to use. Use runtime parameters to specify values for pipeline properties when you start the pipeline.">Pipeline runtime parameters</a> - You can now define runtime
                            parameters when you configure a pipeline, and then call the parameters
                            from within that pipeline. When you start the pipeline from the user
                            interface, the command line, or the REST API, you specify the values to
                            use for those parameters. Use pipeline parameters to represent any stage
                            or pipeline property with a value that must change for each pipeline run
                            - such as batch sizes and timeouts, directories, or URI.<p class="p">In previous
                                versions, pipeline runtime parameters were named pipeline constants.
                                You defined the constant values in the pipeline, and could not pass
                                different values when you started the pipeline.</p>
</li>

                        <li class="li"><a class="xref" href="../Pipeline_Configuration/ConfiguringAPipeline.html#task_xlv_jdw_kq" title="Configure a pipeline to define the stream of data. After you configure the pipeline, you can start the pipeline.">Pipeline ID enhancement</a> - <span class="ph">Data
                  Collector</span> now prefixes the pipeline ID with the alphanumeric characters entered
                            for the pipeline title. For example, if you enter “Oracle To HDFS” as
                            the pipeline title, then the pipeline ID has the following value:
                            OracleToHDFStad9f592-5f02-4695-bb10-127b2e41561c.</li>

                        <li class="li">Webhooks for pipeline state changes and alerts - You can now configure
                            pipeline state changes and metric and data alerts to call webhooks in
                            addition to sending email. For example, you can configure an incoming
                            webhook in Slack so that an alert can be posted to a Slack channel. Or,
                            you can configure a webhook to start another pipeline when the pipeline
                            state is changed to Finished or Stopped.</li>

                        <li class="li"><a class="xref" href="../Administration/Administration_title.html#concept_msh_k2q_yt" title="The manager command provides subcommands to start and stop a pipeline, view the status of all pipelines, and reset the origin for a pipeline. It can also be used to get the last-saved offset and to update the last-saved offset for a pipeline.">Force
                                a pipeline to stop from the command line</a> - If a pipeline
                            remains in a Stopping state, you can now use the command line to force
                            stop the pipeline immediately.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Stage Libraries</dt>

                <dd class="dd">
                    <p class="p"><span class="ph">Data
                  Collector</span> now supports the Apache Kudu version 1.3.x. <a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">stage library</a>.</p>

                </dd>

            
            
                <dt class="dt dlterm">Salesforce Stages</dt>

                <dd class="dd">
                    <div class="p">The following Salesforce stages include several enhancements:<ul class="ul" id="concept_ddx_bpm_pz__ul_gsv_prm_pz">
                            <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx" title="The Salesforce origin reads data from Salesforce.">Salesforce origin</a> and <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor</a><ul class="ul" id="concept_ddx_bpm_pz__ul_k1d_5rm_pz">
                                    <li class="li">The origin and processor can use a proxy to connect to
                                        Salesforce.</li>

                                    <li class="li">You can now specify <samp class="ph codeph">SELECT * FROM
                                            &lt;object&gt;</samp> in a SOQL query. The origin or
                                        processor expands * to all fields in the Salesforce object
                                        that are accessible to the configured user. </li>

                                    <li class="li">The origin and processor generate Salesforce field
                                        attributes that provide additional information about each
                                        field, such as the data type of the Salesforce field.</li>

                                    <li class="li">The origin and processor can now additionally retrieve
                                        deleted records from the Salesforce recycle bin. </li>

                                    <li class="li">The origin can now generate events when it completes
                                        processing all available data.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">Salesforce destination</a> - The destination can now use a
                                CRUD operation record header attribute to indicate the operation to
                                perform for each record. You can also configure the destination to
                                use a proxy to connect to Salesforce. </li>

                            <li class="li"><a class="xref" href="../Destinations/WaveAnalytics.html#concept_hlx_r53_rx" title="The Einstein Analytics destination writes data to Salesforce Einstein Analytics. The destination connects to Einstein Analytics to create a dataset with external data.">Wave Analytics destination</a> - You can now configure the
                                authentication endpoint and the API version that the destination
                                uses to connect to Salesforce Wave Analytics. You can also configure
                                the destination to use a proxy to connect to Salesforce.</li>

                        </ul>
</div>

                </dd>

            
            
                <dt class="dt dlterm">Origins</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_z25_bsm_pz">
                        <li class="li"><a class="xref" href="../Origins/Elasticsearch.html#concept_f1q_vpm_2z" title="The Elasticsearch origin is a multithreaded origin that reads data from an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters). The origin generates a record for each Elasticsearch document.">New
                                Elasticsearch origin</a> - An origin that reads data from an
                            Elasticsearch cluster. The origin uses the Elasticsearch scroll API to
                            read documents using a user-defined Elasticsearch query. The origin
                            performs parallel processing and can generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Origins/MQTTSubscriber.html#concept_ukz_3vt_lz" title="The MQTT Subscriber origin subscribes to topics on an MQTT broker to read messages from the broker. The origin functions as an MQTT client that receives messages, generating a record for each message.">New MQTT
                                Subscriber origin</a> - An origin that subscribes to a topic on
                            an MQTT broker to read messages from the broker.</li>

                        <li class="li"><a class="xref" href="../Origins/WebSocketServer.html#concept_u2r_gpc_3z" title="The WebSocket Server origin is a multithreaded origin that listens on a WebSocket endpoint and processes the contents of all authorized WebSocket client requests. Use the WebSocket Server origin to read high volumes of WebSocket client requests using multiple threads.">New
                                WebSocket Server origin</a> - An origin that listens on a
                            WebSocket endpoint and processes the contents of all authorized
                            WebSocket requests. The origin performs parallel processing and can
                            generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev
                                Data Generator origin enhancement</a> - When you configure the
                            origin to generate events to test event handling functionality, you can
                            now specify the event type to use.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPClient.html#concept_wk4_bjz_5r" title="You can configure the HTTP Client origin to use the OAuth 2 protocol to connect to an HTTP service that uses basic, digest, or universal authentication, OAuth 2 client credentials, OAuth 2 username and password, or OAuth 2 JSON Web Tokens (JWT).To use OAuth 2 authorization to read from Twitter, configure HTTP Client to use basic authentication and the client credentials grant.To use OAuth 2 authorization to read from Microsoft Azure AD, configure HTTP Client to use no authentication and the client credentials grant.To use OAuth 2 authorization to read from Google service accounts, configure HTTP Client to use no authentication and the JSON Web Tokens grant.The HTTP Client origin processes data differently based on the data format. The origin processes the following types of data:">HTTP Client
                                origin enhancements</a> - When using pagination, the origin can
                            include all response fields in the resulting record in addition to the
                            fields in the specified result field path. The origin can now also
                            process the following new data formats: Binary, Delimited, Log, and SDC
                            Record.</li>

                        <li class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_thw_wtd_kz" title="Configure the HTTP clients to include the HTTP Server application ID in each request.">HTTP Server origin enhancement</a> - The origin requires that
                            HTTP clients include the application ID in all requests. You can now
                            configure HTTP clients to send data to a URL that includes the
                            application ID in a query parameter, rather than including the
                            application ID in request headers. </li>

                        <li class="li"><a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y" title="You define the group of tables that the JDBC Multitable Consumer origin reads by defining a table name pattern for the table configuration. The origin reads all tables whose names match the pattern. The JDBC Multitable Consumer origin uses an offset column and initial offset value to determine where to start reading data within tables and partitions.The JDBC Multitable Consumer origin can read from views in addition to tables. You can define the initial order that the origin uses to read the tables.">JDBC Multitable Consumer origin enhancements</a> - The origin
                            now performs parallel processing and can generate multithreaded
                            pipelines. The origin can generate events when it completes processing
                            all available data. <p class="p">You can also configure the quote character to use
                                around table, schema, and column names in the query. And you can
                                configure the number of times a thread tries to read a batch of data
                                after receiving an SQL error.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_qhf_hjr_bs" title="JDBC Query Consumer uses an offset column and initial offset value to determine where to start reading data within a table. Include both the offset column and the offset value in the WHERE clause of the SQL query. JDBC Query Consumer supports recovery after a deliberate or unexpected stop when it performs incremental queries. Recovery is not supported for full queries.When you define the SQL query for incremental mode, JDBC Query Consumer requires a WHERE and ORDER BY clause in the query. You can define any type of SQL query for full mode.">JDBC Query
                                Consumer origin enhancements</a> - The origin can now generate
                            events when it completes processing all available data, and when it
                            successfully completes or fails to complete a query. <p class="p">To handle
                                transient connection or network errors, you can now specify how many
                                times the origin should retry a query before stopping the
                                pipeline.</p>
</li>

                        <li class="li"><a class="xref" href="../Origins/KConsumer.html#concept_msz_wnr_5q" title="You can add custom Kafka configuration properties to the Kafka Consumer.When you use an origin to read log data, you define the format of the log files to be read. Configure a Kafka Consumer to read data from a Kafka cluster.">Kinesis
                                Consumer origin enhancement</a> - The origin now performs
                            parallel processing and can generate multithreaded pipelines. </li>

                        <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_gzz_kdr_tz">MongoDB origin</a> and <a class="xref" href="../Origins/MongoDBOplog.html#concept_ovt_vpt_tz">MongoDB Oplog origin</a> enhancements - The origins can now use
                            LDAP authentication in addition to username/password authentication to
                            connect to MongoDB. You can also now include credentials in the MongoDB
                            connection string.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Processors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_apy_wsm_pz">
                        <li class="li"><a class="xref" href="../Processors/FieldOrder.html#concept_krp_5fv_vy" title="The Field Order processor orders fields in a map or list-map field and outputs the fields into a list-map or list root field.">New Field
                                Order processor</a> - A processor that orders fields in a map or
                            list-map field and outputs the fields into a list-map or list root
                            field.</li>

                        <li class="li">Field Flattener enhancement - You can now flatten a field in place to
                            raise it to the parent level.</li>

                        <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_ldh_sct_gv" title="You can choose the processing mode that the Groovy Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode.In scripts that process list-map data, treat the data as maps. You can call external Java code from the Groovy Evaluator. Simply install the external Java library to make it available to the Groovy Evaluator. Then, call the external Java code from the Groovy script that you develop for the processor.">Groovy</a>,
                                <a class="xref" href="../Processors/JavaScript.html#concept_n2p_jgf_lr" title="You can choose the processing mode that the JavaScript Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode.In scripts that process list-map data, treat the data as maps. You can call external Java code from the JavaScript Evaluator. Simply install the external Java library to make it available to the JavaScript Evaluator. Then, call the external Java code from the script that you develop for the processor.">JavaScript</a>, and <a class="xref" href="../Processors/Jython.html#concept_a1h_lkf_lr" title="You can choose the processing mode that the Jython Evaluator uses to process the main script. You can use the same script in each processing mode. However, you should include error handling in the main script before you run in batch mode.In scripts that process list-map data, treat the data as maps. You can call external Java code from the Jython Evaluator. Simply install the external Java library to make it available to the Jython Evaluator. Then, call the external Java code from the Jython script that you develop for the processor.">Jython
                                Evaluator</a> processor enhancement - You can now develop an
                            initialization script that the processor runs once when the pipeline
                            starts. Use an initialization script to set up connections or resources
                            required by the processor.<p class="p">You can also develop a destroy script that
                                the processor runs once when the pipeline stops. Use a destroy
                                script to close any connections or resources opened by the
                                processor.</p>
</li>

                        <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup enhancement</a> - Default value date formats. When
                            the default value data type is Date, use the following format:
                            yyyy/MM/dd . When the default value data type is Datetime, use the
                            following format: yyyy/MM/dd HH:mm:ss.</li>

                        <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Record
                                Deduplicator processor enhancement</a> - The processor can now
                            deduplicate records across all pipeline runners in a multithreaded
                            pipeline.</li>

                        <li class="li"><a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx" title="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop.">Spark Evaluator
                                processor enhancements</a> - The processor is now included in the
                            MapR 5.2 stage library. <p class="p">The processor also now provides beta support
                                of cluster mode pipelines. In a development or test environment, you
                                can use the processor in pipelines that process data from a Kafka or
                                MapR cluster in cluster streaming mode. Do not use the Spark
                                Evaluator processor in cluster mode pipelines in a production
                                environment.</p>
</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Destinations</dt>

                <dd class="dd">
                        <ul class="ul" id="concept_ddx_bpm_pz__ul_x13_lym_pz">
                            <li class="li"><a class="xref" href="../Destinations/HTTPClient.html#concept_khl_sg5_lz" title="The HTTP Client destination writes data to an HTTP endpoint. The destination sends requests to an HTTP resource URL. Use the HTTP Client destination to perform a range of standard requests or use an expression to determine the request for each record.">New
                                    HTTP Client destination</a> - A destination that writes to an
                                HTTP endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/MQTTPublisher.html#concept_odz_txt_lz" title="The MQTT Publisher destination publishes messages to a topic on an MQTT broker. The destination functions as an MQTT client that publishes messages, writing each record as a message.">New MQTT Publisher destination</a> - A destination that
                                publishes messages to a topic on an MQTT broker.</li>

                            <li class="li"><a class="xref" href="../Destinations/WebSocketClient.html#concept_l4d_mjn_lz" title="The WebSocket Client destination writes data to a WebSocket endpoint. Use the destination to send data to a WebSocket resource URL.">New WebSocket Client destination</a> - A destination that
                                writes to a WebSocket endpoint.</li>

                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_c2p_wzh_4z">Azure Data Lake Store destination enhancement</a> - You can
                                now configure an idle timeout for output files. </li>

                            <li class="li">Cassandra destination enhancements - The destination now supports
                                the Cassandra uuid and timeuuid data types. And you can now specify
                                the Cassandra batch type to use: Logged or Unlogged. Previously, the
                                destination used the Logged batch type.</li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht" title="The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log.">JDBC Producer enhancements</a> - The origin now includes a
                                Schema Name property for entering the schema name. For information
                                about possible upgrade impact, see <a class="xref" href="../Upgrade/PostUpgrade.html#concept_cmh_ryd_pz">Configure JDBC Producer Schema Names</a>.<p class="p">You can also use
                                    the Enclose Object Name property to enclose the database/schema,
                                    table, and column names in quotation marks when writing to the
                                    database. </p>
</li>

                            <li class="li"><a class="xref" href="../Destinations/MapRDBJSON.html#task_wq3_wkj_dy">MapR DB JSON destination enhancement</a> - You can now enter an
                            expression that evaluates to the name of the MapR DB JSON table to write
                            to.</li>

                        <li class="li"><a class="xref" href="../Destinations/MongoDB.html#concept_ppl_3qt_tz">MongoDB destination enhancements</a> - The destination can now
                            use LDAP authentication in addition to username/password authentication
                            to connect to MongoDB. You can also now include credentials in the
                            MongoDB connection string.</li>

                        <li class="li"><a class="xref" href="../Destinations/SDC_RPCdest.html#task_nbl_r2x_dt">SDC RPC destination enhancements</a> - The Back Off Period value
                            that you enter now increases exponentially after each retry, until it
                            reaches the maximum wait time of 5 minutes. Previously, there was no
                            limit to the maximum wait time. The maximum value for the Retries per
                            Batch property is now unlimited - previously it was 10 retries.</li>

                        <li class="li"><a class="xref" href="../Destinations/Solr.html#concept_z2g_q1r_wr" title="The Solr destination writes data to a Solr node or cluster.">Solr
                                destination enhancement</a> - You can now configure the action
                            that the destination takes when it encounters missing fields in the
                            record. The destination can discard the fields, send the record to
                            error, or stop the pipeline.</li>

                        </ul>

                </dd>

            
            
                <dt class="dt dlterm">Executors</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ojc_3zm_pz">
                        <li class="li"><a class="xref" href="../Executors/Spark.html#concept_cvy_vxb_1z">New Spark
                                executor</a> - The executor starts a Spark application on a YARN
                            or Databricks cluster each time it receives an event.</li>

                        <li class="li"><a class="xref" href="../Executors/PipelineFinisher.html#concept_qzm_l4r_kz">New
                                Pipeline Finisher executor</a> - The executor stops the pipeline
                            and transitions it to a Finished state when it receives an event. Can be
                            used with the JDBC Query Consumer, JDBC Multitable Consumer, and
                            Salesforce origins to perform batch processing of available data.</li>

                        <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File
                                Metadata executor enhancement</a> - The executor can now create
                            an empty file upon receiving an event. The executor can also generate a
                            file-created event when generating events. </li>

                        <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce
                                executor enhancement</a> - When starting the provided Avro to
                            Parquet job, the executor can now overwrite any temporary files created
                            from a previous run of the job.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">Functions</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ayq_pzm_pz">
                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New escape XML functions</a> - Three new string functions enable
                            you to escape and unescape XML.</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">New pipeline user function</a> - A new pipeline user function
                            enables you to determine the user who started the pipeline. </li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New function to generate UUIDs</a> - A new function that enables
                            you generate UUIDs.</li>

                        <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New function returns the number of available processors</a> -
                            The runtime:availableProcessors() function returns the number of
                            processors available to the Java virtual machine.</li>

                    </ul>

                </dd>

            
            
                <dt class="dt dlterm">General Enhancements</dt>

                <dd class="dd">
                    <ul class="ul" id="concept_ddx_bpm_pz__ul_ksn_tzm_pz">
                        <li class="li"><a class="xref" href="../Configuration/DCConfig.html#concept_pmr_sy5_nz">Data Collector Hadoop impersonation enhancement</a> - You can
                            use the
                                <samp class="ph codeph">stage.conf_hadoop.always.impersonate.current.user</samp>
                            <span class="ph">Data
                  Collector</span> configuration property to ensure that <span class="ph">Data
                  Collector</span> uses the current <span class="ph">Data
                  Collector</span> user to read from or write to Hadoop systems. <div class="p">When enabled, you
                                cannot configure alternate users in the following Hadoop-related
                                    stages:<ul class="ul" id="concept_ddx_bpm_pz__ul_ztn_xzm_pz">
                                    <li class="li">Hadoop FS origin and destination</li>

                                    <li class="li">MapR FS origin and destination</li>

                                    <li class="li">HBase lookup and destination</li>

                                    <li class="li">MapR DB destination</li>

                                    <li class="li">HDFS File Metadata executor</li>

                                    <li class="li">Map Reduce executor</li>

                                </ul>
</div>
</li>

                        <li class="li">Stage precondition property enhancement - Records that do not meet all
                            preconditions for a stage are now processed based on error handling
                            configured in the stage. Previously, they were processed based on error
                            handling configured for the pipeline. See <a class="xref" href="../Upgrade/PostUpgrade.html#concept_gk3_s5l_nz">Evaluate Precondition Error Handling</a> for information about
                            upgrading.</li>

                        <li class="li">XML parsing enhancements - You can include field XPath expressions and
                            namespaces in the record with the <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_w3k_1ch_qz">Include Field XPaths property</a>. And use the new <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_jll_4wh_qz">Output Field Attributes property</a> to write XML attributes and
                            namespace declarations to field attributes rather than including them in
                            the record as fields. </li>

                        <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">Wrap long lines in properties</a> - You can now configure <span class="ph">Data
                  Collector</span> to wrap long lines of text that you enter in properties, instead of
                            displaying the text with a scroll bar.</li>

                    </ul>

                </dd>

            
        </dl>

 </div>

</div>
<div class="topic concept nested1" id="concept_cf2_sdz_fz">
 <h2 class="title topictitle2">What's New in 2.4.1.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.4.1.0 includes the following new features and enhancements:<ul class="ul" id="concept_cf2_sdz_fz__ul_xdl_tdz_fz">
                <li class="li"><a class="xref" href="../Origins/Salesforce.html#concept_owv_nj5_2z">Salesforce origin enhancement</a> - When the origin processes existing
                    data and is not subscribed to notifications, it can now repeat the specified
                    query at regular intervals. The origin can repeat a full or incremental
                    query.</li>

                <li class="li"><a class="xref" href="../Administration/Administration_title.html#task_gbm_s3k_br" title="If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level.">Log data
                        display</a> - You can stop and restart the automatic display of the most
                    recent log data on the Data Collector Logs page.</li>

                <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">New
                        time function</a> - The <samp class="ph codeph">time:createDateFromStringTZ</samp>
                    function enables creating Date objects adjusted for time zones from string
                    datetime values.</li>

                <li class="li">New stage library stage-type icons - The stage library now displays icons to
                    differentiate between different stage types.</li>

            </ul>
<div class="note note"><span class="notetitle">Note:</span> The Hive Drift Solution is now known as the "Drift Synchronization Solution
                for Hive" in the documentation.</div>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_kzc_4sd_yy">
 <h2 class="title topictitle2">What's New in 2.4.0.0</h2>

 <div class="body conbody">
  <p class="p"><span class="ph">Data
                  Collector</span>
            version 2.4.0.0 includes the following new features and enhancements:</p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Pipeline Sharing and Permissions</dt>

                    <dd class="dd">Data Collector now provides pipeline-level permissions. Permissions
                        determine the access level that users and groups have on pipelines. To
                        create a multitenant environment, create groups of users and then share
                        pipelines with the groups to grant different levels of access.</dd>

                    <dd class="dd">With this change, only the pipeline owner and users with the Admin role can
                        view a pipeline by default. If upgrading from a previous version of Data
                        Collector, see the following post-upgrade task, <a class="xref" href="../Upgrade/PostUpgrade.html#concept_zbn_fpw_xy">Configure Pipeline Permissions</a>.</dd>

                    <dd class="dd">This feature includes the following components:<ul class="ul" id="concept_kzc_4sd_yy__ul_qhv_ds1_cz">
                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_i1p_hzd_yy">Pipeline permissions</a> - Pipelines now have read, write,
                                and execute permissions. Pipeline permissions overlay existing Data
                                Collector roles to provide greater security. For information, see
                                    <a class="xref" href="../Configuration/RolesandPermissions.html#concept_k1r_prc_yy">Roles and Permissions</a>.</li>

                            <li class="li"><a class="xref" href="../Pipeline_Maintenance/PipelineMaintenance_title.html#concept_jrg_1vy_wy">Pipeline sharing</a> - The pipeline owner and users with the
                                Admin role can configure pipeline permissions for users and
                                groups.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Data Collector pipeline access control property</a> - You
                                can enable and disable the use of pipeline permissions with the
                                pipeline.access.control.enabled configuration property. By default,
                                this property is enabled.</li>

                            <li class="li"><a class="xref" href="../Configuration/RolesandPermissions.html#concept_p11_msc_1z">Permissions transfer</a> - You can transfer all pipeline
                                permissions associated with a user or group to a different user or
                                group. Use pipeline transfer to easily migrate permissions after
                                registering with DPM or after a user or group becomes obsolete.</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Dataflow Performance Manager (DPM)</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_f3t_zs1_cz">
                            <li class="li"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#task_a4y_v1g_xw" title="You can register a Data Collector with DPM from the Data Collector UI.">Register Data Collectors with DPM</a> - If Data Collector
                                uses file-based authentication and if you register the Data
                                Collector from the Data Collector UI, you can now create DPM user
                                accounts and groups during the registration process.</li>

                            <li class="li"><a class="xref" href="../DPM/AggregatedStatistics.html#concept_c53_pzp_yy">Aggregated statistics for DPM</a> - When working with DPM,
                                you can now configure a pipeline to write aggregated statistics to
                                SDC RPC. Write statistics to SDC RPC for development purposes only.
                                For a production environment, use a Kafka cluster or Amazon Kinesis
                                Streams to aggregate statistics.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_ad3_kt1_cz">
                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev SDC RPC with Buffering origin</a> - A new development
                                stage that receives records from an SDC RPC destination, temporarily
                                buffering the records to disk before passing the records to the next
                                stage in the pipeline. Use as the origin in an SDC RPC destination
                                pipeline.</li>

                            <li class="li"><a class="xref" href="../Origins/AmazonS3.html#task_gfj_ssv_yq">Amazon S3 origin enhancement</a> - You can configure a new
                                File Pool Size property to determine the maximum number of files
                                that the origin stores in memory for processing after loading and
                                sorting all files present on S3.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Other</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_kzc_4sd_yy__ul_hsj_tt1_cz">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a> - This release supports the
                                following new stage libraries:<ul class="ul" id="concept_kzc_4sd_yy__ul_xrv_5t1_cz">
                                    <li dir="ltr" class="li">Kudu versions 1.1 and 1.2</li>

                                    <li class="li">
                                        <p dir="ltr" class="p">Cloudera CDH version 5.10 distribution of
                                            Hadoop</p>

                                    </li>

                                    <li dir="ltr" class="li">Cloudera version 5.10 distribution of Apache Kafka
                                        2.1</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/ExternalLibs.html#concept_pdv_qlw_ft" title="After you've set up the external directory, use the Package Manager within Data Collector to install external libraries.To manually install external libraries, use the required procedure for your installation type.">Install external libraries using the Data Collector user
                                    interface</a> - You can now use the Data Collector user
                                interface to install external libraries to make them available to
                                stages. For example, you can install JDBC drivers for stages that
                                use JDBC connections. Or, you can install external libraries to call
                                external Java code from the Groovy, Java, and Jython Evaluator
                                processors.</li>

                            <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">Custom header enhancement</a> - You can now use HTML in the
                                ui.header.title configuration property to configure a custom header
                                for the <span class="ph">Data
                  Collector</span> UI. This allows you to specify the look and feel for any text
                                that you use, and to include small images in the header. </li>

                            <li class="li"><a class="xref" href="../Processors/Groovy.html#task_asl_bpt_gv">Groovy enhancement</a> - You can configure the processor to
                                use the invokedynamic bytecode instruction.</li>

                            <li class="li">Pipeline renaming - You can now rename a pipeline by clicking
                                directly on the pipeline name when editing the pipeline, in addition
                                to editing the Title general pipeline property.</li>

                        </ul>

                    </dd>

                
            </dl>

        </div>

 </div>

</div>
<div class="topic concept nested1" id="concept_bml_dbt_wy">
 <h2 class="title topictitle2">What's New in 2.3.0.1</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.3.0.1 includes the following new features and enhancements:<ul class="ul" id="concept_bml_dbt_wy__ul_srs_hbt_wy">
                <li class="li"><a class="xref" href="../Origins/OracleCDC.html#concept_rs5_hjj_tw">Oracle CDC Client
                        origin enhancement</a> - The origin can now track and adapt to schema
                    changes when reading the dictionary from redo logs. When using the dictionary in
                    redo logs, the origin can also generate events for each DDL that it reads. </li>

                <li class="li"><a class="xref" href="../Configuration/DCConfig.html#task_lxk_kjw_1r" title="You can customize Data Collector by editing the Data Collector configuration file, sdc.properties.">New
                        Data Collector property</a> - The http.enable.forwarded.requests property
                    in the Data Collector configuration file enables handling X-Forwarded-For,
                    X-Forwarded-Proto, X-Forwarded-Port request headers issued by a reverse proxy or
                    load balancer.</li>

                <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_kx3_zrs_ns">MongoDB
                        origin enhancement</a> ­ The origin now supports using any string field
                    as the offset field. </li>

            </ul>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_yym_xqt_5y">
 <h2 class="title topictitle2">What's New in 2.3.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.3.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Multithreaded Pipelines</dt>

                    <dd class="dd">You can use a multithreaded origin to generate <a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_zpp_2xc_py">multithreaded pipelines</a> to perform parallel processing. <p dir="ltr" class="p">The new multithreaded framework includes the following
                            changes:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_slg_mrt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y" title="The HTTP Server origin is a multithreaded origin that listens on an HTTP endpoint and processes the contents of all authorized HTTP POST requests. Use the HTTP Server origin to read high volumes of HTTP POST requests using multiple threads.">HTTP
                                        Server origin</a> - Listens on an HTTP endpoint and
                                    processes the contents of all authorized HTTP POST requests. Use
                                    the HTTP Server origin to receive high volumes of HTTP POST
                                    requests using multiple threads.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Enhanced Dev Data Generator origin</a> - Can create
                                    multiple threads for testing multithreaded pipelines.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Multithreaded_Pipelines/MultithreadedPipelines.html#concept_tdn_vwy_ry">Enhanced runtime statistics</a> - Monitoring a pipeline
                                    displays aggregated runtime statistics for all threads in the
                                    pipeline. You can also view the number of runners, i.e. threads
                                    and pipeline instances, being used.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">CDC/CRUD Enhancements</dt>

                    <dd class="dd">With this release, certain Data Collector stages enable you to easily
                        process change data capture (CDC) or transactional data in a pipeline. The
                        sdc.operation.type record header attribute is now used by all CDC-enabled
                        origins and CRUD-enabled stages:<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_iws_mhd_ty">CDC-enabled origins</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_vb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB Oplog and Salesforce origins are now
                                    enabled for processing changed data by including the CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p">Though previously CDC-enabled, the Oracle CDC Client and JDBC
                                    Query Consumer for Microsoft SQL Server now include CRUD
                                    operation type in the sdc.operation.type record header
                                    attribute. </p>

                                <p class="p">Previous operation type header attributes are still supported for
                                    backward-compatibility. </p>

                            </li>

                        </ul>
<p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/CDC-Overview.html#concept_lfb_phd_ty">CRUD-enabled stages</a>:</p>
<ul class="ul" id="concept_yym_xqt_5y__ul_wb4_prt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The JDBC Tee processor and JDBC Producer can now
                                    process changed data based on CRUD operations in record headers.
                                    The stages also include a default operation and unsupported
                                    operation handling. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p">The MongoDB and Elasticsearch destinations now look for
                                    the CRUD operation in the sdc.operation.type record header
                                    attribute. The Elasticsearch destination includes a default
                                    operation and unsupported operation handling.</p>

                            </li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Multitable Copy</dt>

                    <dd class="dd">You can use the new <a class="xref" href="../Origins/MultiTableJDBCConsumer.html#concept_zp3_wnw_4y" title="You define the group of tables that the JDBC Multitable Consumer origin reads by defining a table name pattern for the table configuration. The origin reads all tables whose names match the pattern. The JDBC Multitable Consumer origin uses an offset column and initial offset value to determine where to start reading data within tables and partitions.The JDBC Multitable Consumer origin can read from views in addition to tables. You can define the initial order that the origin uses to read the tables.">JDBC
                            Multitable Consumer origin</a> when you need to copy multiple tables
                        to a destination system or for database replication. The JDBC Multitable
                        Consumer origin reads database data from multiple tables through a JDBC
                        connection. The origin generates SQL queries based on the table
                        configurations that you define.</dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_up5_ntt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Configuration/Authentication.html#concept_wgy_rxt_5x" title="If your organization does not use LDAP, configure Data Collector to use the default file-based authentication.">Groups for file-based authentication</a> - If you use
                                file-based authentication, you can now create groups of users when
                                multiple users use Data Collector. You configure groups in the
                                associated realm.properties file located in the Data Collector
                                configuration directory, $SDC_CONF. <p class="p">If you use file-based
                                    authentication, you can also now view all user accounts granted
                                    access to the Data Collector, including the roles and groups
                                    assigned to each user.</p>
</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/Authentication.html#concept_dns_dvg_h5" title="Data Collector can authenticate user accounts based on LDAP or files. Best practice is to use LDAP if your organization has it. By default, Data Collector uses file-based authentication.">LDAP authentication enhancements</a> - You can now
                                    configure Data Collector to use StartTLS to make secure
                                    connections to an LDAP server. You can also configure the
                                    userFilter property to define the LDAP user attribute used to
                                    log in to Data Collector. For example, a username, uid, or email
                                    address.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../DPM/RegisterSDCwithDPM.html#concept_dmr_df5_5y" title="You can configure each registered Data Collector to use an authenticated HTTP or HTTPS proxy server for outbound requests made to DPM. Define the proxy properties in the SDC_JAVA_OPTS environment variable in the Data Collector environment configuration file located in the $SDC_DIST/libexec directory.">Proxy
                                        configuration for outbound requests</a> - You can now
                                    configure Data Collector to use an authenticated HTTP proxy for
                                    outbound requests to Dataflow Performance Manager (DPM).</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector logging</a> - Data Collector now
                                    enables logging for the Java garbage collector by default. Logs
                                    are written to $SDC_LOG/gc.log. You can disable the logging if
                                    needed. </p>

                            </li>

                            <li dir="ltr" class="li">Heap dump for out of memory errors - Data Collector now
                                produces a heap dump file by default if it encounters an out of
                                memory error. You can configure the location of the heap dump file
                                or you can disable this default behavior. </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Administration/Administration_title.html#task_lkv_g2f_wy" title="If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level.">Modifying the log level</a> - You can now use the Data
                                Collector UI to modify the log level to display messages at another
                                severity level.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipelines</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_lzx_xtt_5y">
                            <li dir="ltr" class="li">Pipeline renaming - You can now rename pipelines by
                                editing the Title general pipeline property.</li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Pipeline_Design/FieldAttributes.html#concept_xfm_wtp_1z" title="Field attributes are attributes that provide additional information about each field that you can use in pipeline logic, as needed.">Field attributes</a> - Data Collector now supports
                                    field-level attributes. Use the Expression Evaluator to add
                                    field attributes.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_ljw_15t_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Origins/HTTPServer.html#concept_s2p_5hb_4y" title="The HTTP Server origin is a multithreaded origin that listens on an HTTP endpoint and processes the contents of all authorized HTTP POST requests. Use the HTTP Server origin to read high volumes of HTTP POST requests using multiple threads.">New HTTP Server origin</a> - A multithreaded origin that
                                listens on an HTTP endpoint and processes the contents of all
                                authorized HTTP POST requests. Use the HTTP Server origin to read
                                high volumes of HTTP POST requests using multiple threads. </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPtoKafka.html#concept_izh_mqd_dy">New
                                        HTTP to Kafka origin</a> - Listens on a HTTP endpoint and
                                    writes the contents of all authorized HTTP POST requests
                                    directly to Kafka. Use to read high volumes of HTTP POST
                                    requests and write them to Kafka. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MapRDBJSON.html#concept_ywh_k15_3y" title="The MapR DB JSON origin reads JSON documents from MapR DB JSON tables. The origin converts each document into a record.">New
                                        MapR DB JSON origin</a> - Reads JSON documents from MapR
                                    DB JSON tables.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/MongoDBOplog.html#concept_mjn_yqw_4y">New
                                        MongoDB Oplog origin</a> - Reads entries from a MongoDB
                                    Oplog. Use to process change information for data or database
                                    operations. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/Directory.html#concept_xd5_5z4_4y">Directory origin enhancement</a> - You can use regular
                                    expressions in addition to glob patterns to define the file name
                                    pattern to process files. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Origins/HTTPClient.html#concept_c13_zz1_5y" title="You can configure the HTTP Client origin to use the OAuth 2 protocol to connect to an HTTP service that uses basic, digest, or universal authentication, OAuth 2 client credentials, OAuth 2 username and password, or OAuth 2 JSON Web Tokens (JWT).">HTTP Client origin enhancement</a> - You can now
                                    configure the origin to use the OAuth 2 protocol to connect to
                                    an HTTP service.</p>

                            </li>

                            <li dir="ltr" class="li"><a class="xref" href="../Origins/JDBCConsumer.html#concept_qhf_hjr_bs" title="JDBC Query Consumer uses an offset column and initial offset value to determine where to start reading data within a table. Include both the offset column and the offset value in the WHERE clause of the SQL query. JDBC Query Consumer supports recovery after a deliberate or unexpected stop when it performs incremental queries. Recovery is not supported for full queries.When you define the SQL query for incremental mode, JDBC Query Consumer requires a WHERE and ORDER BY clause in the query. You can define any type of SQL query for full mode.">JDBC
                                    Query Consumer origin enhancements</a> - The JDBC Consumer
                                origin has been renamed to the JDBC Query Consumer origin. The
                                origin functions the same as in previous releases. It reads database
                                data using a user-defined SQL query through a JDBC connection. You
                                can also now configure the origin to enable auto-commit mode for the
                                JDBC connection and to disable validation of the SQL query.</li>

                            <li class="li"><a class="xref" href="../Origins/MongoDB.html#concept_bk4_2rs_ns">MongoDB
                                    origin enhancements</a> - You can now use a nested field as
                                the offset field. The origin supports reading the MongoDB BSON
                                timestamp for MongoDB versions 2.6 and later. And you can configure
                                the origin to connect to a single MongoDB server or node. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_z35_cwt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#task_g23_2tq_wq">Field Type Converter processor enhancement</a> - You can now
                                configure the processor to convert timestamp data in a long field to
                                a String data type. Previously, you had to use one Field Type
                                Converter processor to convert the long field to a datetime, and
                                then use another processor to convert the datetime field to a
                                string.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/HTTPClient.html#concept_ghx_ypr_fw">HTTP Client processor enhancements</a>  - You can now
                                    configure the processor to use the OAuth 2 protocol to connect
                                    to an HTTP service. You can also configure a rate limit for the
                                    processor, which defines the maximum number of requests to make
                                    per second.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/JDBCLookup.html#concept_ysc_ccy_hw" title="The JDBC Lookup processor uses a JDBC connection to perform lookups in a database table and pass the lookup values to fields. Use the JDBC Lookup to enrich records with additional data.">JDBC Lookup processor enhancements</a> - You can now
                                    configure the processor to enable auto-commit mode for the JDBC
                                    connection. You can also configure the processor to use a
                                    default value if the database does not return a lookup value for
                                    a column.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor enhancement</a> - You can
                                    now configure the processor to use a default value if Salesforce
                                    does not return a lookup value for a field.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5" title="Configure an XML Parser to parse XML data in a string field.">XML
                                        Parser enhancement</a> - A new Multiple Values Behavior
                                    property allows you to specify the behavior when you define a
                                    delimiter element and the document includes more than one value:
                                    Return the first value as a record, return one record with a
                                    list field for each value, or return all values as records.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destinations</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_uys_hwt_5y">
                            <li dir="ltr" class="li"><a class="xref" href="../Destinations/MapRDBJSON.html#concept_i4h_2kj_dy" title="MapR DB uses a row key to uniquely identify each row in a JSON table. The row key is defined by the _id field of the JSON document stored in the row.You configure the MapR DB JSON destination to process row keys as string or binary data. If necessary, the MapR DB JSON destination converts the data type of the row key field and then writes the converted value to the _id field in the JSON document.">New
                                    MapR DB JSON destination</a> - Writes data as JSON documents
                                to MapR DB JSON tables.</li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> enhancement - You
                                    can now use the destination in cluster batch pipelines. You can
                                    also process binary and protobuf data, use record header
                                    attributes to write records to files and roll files, and
                                    configure a file suffix and the maximum number of records that
                                    can be written to a file. </p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/Elasticsearch.html#concept_u5t_vpv_4r" title="The Elasticsearch destination writes data to an Elasticsearch cluster, including Elastic Cloud clusters (formerly Found clusters). The destination uses the Elasticsearch HTTP API to write each record to Elasticsearch as a document.">Elasticsearch destination enhancement</a> - The
                                    destination now uses the Elasticsearch HTTP API. With this API,
                                    the Elasticsearch version 5 stage library is compatible with all
                                    versions of Elasticsearch. Earlier stage library versions have
                                    been removed. Elasticsearch is no longer supported on Java 7.
                                    You’ll need to verify that Java 8 is installed on the Data
                                    Collector machine and remove this stage from the blacklist
                                    property in $SDC_CONF/sdc.properties before you can use it. </p>

                                <p class="p">You can also now configure the destination to perform any of the
                                    following CRUD operations: create, update, delete, or index.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HiveMetastore.html#concept_x4p_fyc_rx">Hive Metastore destination enhancement</a> - New table
                                    events now include information about columns and partitions in
                                    the table.</p>

                            </li>

                            <li dir="ltr" class="li">
                                <p class="p"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_uv2_vfb_vy">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_s5n_ggc_vy">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_vvr_ngc_vy">MapR FS</a> destination enhancement - The destinations
                                    now support recovery after an unexpected stop of the pipeline by
                                    renaming temporary files when the pipeline restarts.</p>

                            </li>

                            <li dir="ltr" class="li">Redis destination enhancement - You can now configure a
                                timeout for each key that the destination writes to Redis.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Executors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_zqk_4wt_5y">
                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive
                                        Query executor enhancements</a>: <ul class="ul" id="concept_yym_xqt_5y__ul_gf2_qwt_5y">
                                        <li class="li">The executor can now execute multiple queries for each
                                            event that it receives.</li>

                                        <li class="li">It can also generate event records each time it
                                            processes a query.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                        <ul class="ul" id="concept_yym_xqt_5y__ul_brk_4wt_5y">
                            <li dir="ltr" class="li">
                                <p dir="ltr" class="p"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC
                                        Query executor enhancement</a> - You can now configure
                                    the executor to enable auto-commit mode for the JDBC
                                    connection.</p>

                            </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hpw_zwt_5y">
                            <li class="li"><a class="xref" href="../Data_Formats/WholeFile.html#concept_prp_jzd_py">Whole File enhancement</a> - You can now specify a transfer
                                rate to help control the resources used to process whole files. You
                                can specify the rate limit in all origins that process whole
                                files.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_yym_xqt_5y__ul_hq3_2xt_5y">
                            <li class="li"><a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">New pipeline functions</a> - You can use the following new
                                pipeline functions to return pipeline information:<ul class="ul" id="concept_yym_xqt_5y__ul_mls_bp1_cz">
                                    <li class="li">pipeline:id() - Returns the pipeline ID, a UUID that is
                                        automatically generated and used by Data Collector to
                                        identify the pipeline. <div class="note note"><span class="notetitle">Note:</span> The existing pipeline:name()
                                            function now returns the pipeline ID instead of the
                                            pipeline name since pipeline ID is the correct way to
                                            identify a pipeline.</div>
</li>

                                    <li class="li">
                                        <p class="p">pipeline:title() - Returns the pipeline title or
                                            name.</p>

                                    </li>

                                </ul>
</li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_p1z_ggv_1r" title="Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record.">New record functions</a> - You can use the following new
                                    record functions to work with field attributes:<ul class="ul" id="concept_yym_xqt_5y__ul_k4h_kxt_5y">
                                        <li class="li">record:fieldAttribute (&lt;field path&gt;, &lt;attribute
                                            name&gt;) - Returns the value of the specified field
                                            attribute. </li>

                                        <li class="li">record:fieldAttributeOrDefault (&lt;field path&gt;,
                                            &lt;attribute name&gt;, &lt;default value&gt;) - Returns the
                                            value of the specified field attribute. Returns the
                                            default value if the attribute does not exist or
                                            contains no value.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ahp_f4v_1r">New string functions</a> - You can use the following new
                                    string functions to transform string data: <ul class="ul" id="concept_yym_xqt_5y__ul_knh_mxt_5y">
                                        <li class="li">str:urlEncode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            URL encoded string from a decoded string using the
                                            specified encoding format. </li>

                                        <li class="li">str:urlDecode (&lt;string&gt;, &lt;encoding&gt;) - Returns a
                                            decoded string from a URL encoded string using the
                                            specified encoding format.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">New time functions</a> - You can use the following new
                                    time functions to transform datetime data: <ul class="ul" id="concept_yym_xqt_5y__ul_yk5_pxt_5y">
                                        <li class="li">time:dateTimeToMilliseconds (&lt;Date object&gt;) -
                                            Converts a Date object to an epoch or UNIX time in
                                            milliseconds. </li>

                                        <li class="li">time:extractDateFromString(&lt;string&gt;, &lt;format
                                            string&gt;) - Extracts a Date object from a String, based
                                            on the specified date format. </li>

                                        <li class="li">time:extractStringFromDateTZ (&lt;Date object&gt;,
                                            &lt;timezone&gt;, &lt;format string&gt;) - Extracts a string
                                            value from a Date object based on the specified date
                                            format and time zone.</li>

                                    </ul>
</div>

                            </li>

                            <li dir="ltr" class="li">
                                <div class="p" dir="ltr"><a class="xref" href="../Expression_Language/Functions.html#concept_ddw_ld1_1s">New and enhanced miscellaneous functions</a> - You can
                                    use the following new and enhanced miscellaneous functions: <ul class="ul" id="concept_yym_xqt_5y__ul_m2x_nxt_5y">
                                        <li class="li">offset:column(&lt;position&gt;) - Returns the value of the
                                            positioned offset column for the current table.
                                            Available only in the additional offset column
                                            conditions of the JDBC Multitable Consumer origin. </li>

                                        <li class="li">every function - You can now use the function with the
                                            hh() datetime variable in directory templates. This
                                            allows you to create directories based on the specified
                                            interval for hours.</li>

                                    </ul>
</div>

                            </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
<div class="topic concept nested1" id="concept_wbf_dgk_fy">
 <h2 class="title topictitle2">What's New in 2.2.1.0</h2>

 <div class="body conbody">
        <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.2.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_y1c_1pm_3y">
                            <li class="li">New <a class="xref" href="../Processors/FieldZip.html#concept_o3b_t1k_yx">Field Zip processor</a> - Merges two List fields or two
                                List-Map fields in the same record.</li>

                            <li class="li">New <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor</a> - Performs lookups in a
                                Salesforce object and passes the lookup values to fields. Use the
                                Salesforce Lookup to enrich records with additional data.</li>

                            <li class="li"><a class="xref" href="../Processors/ValueReplacer.html#concept_ppg_ztk_3y">Value Replacer</a> enhancement - You can now replace field
                                values with nulls using a condition.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_vlq_dpm_3y">
                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_wsz_qj4_zx">Whole file support in the Azure Data Lake Store
                                    destination</a> - You can now use the whole file data format
                                to stream whole files to Azure Data Lake Store. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_oyv_zfk_fy">
 <h2 class="title topictitle2">What's New in 2.2.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span> version
            2.2.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Event Framework</dt>

                    <dd class="dd">The Data Collector event framework enables the pipeline to trigger tasks in
                        external systems based on actions that occur in the pipeline, such as
                        running a MapReduce job after the pipeline writes a file to HDFS. You can
                        also use the event framework to store event information, such as when an
                        origin starts or completes reading a file. </dd>

                    <dd class="dd">For details, see the <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Event Framework chapter</a>. </dd>

                    <dd class="dd">The event framework includes the following new features and enhancements:<ul class="ul" id="concept_oyv_zfk_fy__ul_ojf_xgk_fy">
                            <li class="li"><a class="xref" href="../Executors/Executors-overview.html#concept_stt_2lk_fx">New executor stages</a>. A new type of stage that performs
                                tasks in external systems upon receiving an event. This release
                                includes the following executors:<ul class="ul" id="concept_oyv_zfk_fy__ul_bn4_ygk_fy">
                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File Metadata executor</a> - Changes file
                                        metadata such as the name, location, permissions, and ACLs. </li>

                                    <li class="li"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive Query executor</a> - Runs a Hive or Impala
                                        query. </li>

                                    <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor</a> - Runs a SQL query. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce executor</a> - Runs a custom MapReduce job
                                        or an Avro to Parquet MapReduce job. </li>

                                </ul>
</li>

                            <li class="li">Event generation. The following stages now generate events that you
                                can use in a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_sxd_bhk_fy">
                                    <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory</a> and <a class="xref" href="../Origins/FileTail.html#concept_gwn_c32_px">File Tail</a> origins - Generate events when they
                                        start and complete reading a file.</li>

                                    <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#concept_aqq_tt2_px">Amazon S3 destination</a> - Generates events when it
                                        completes writing to an object or streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_bvb_rxj_px">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_in1_fcm_px">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_bqd_3qb_rx">MapR FS</a> destinations - Generate events when they
                                        close an output file or complete streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors - Can run scripts
                                        that generate events. </li>

                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_vhl_mfj_rx">HDFS File Metadata executor</a> - Generates events
                                        when it changes file metadata. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_e1s_sm5_sx">MapReduce executor</a> - Generates events when it
                                        starts a MapReduce job.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev stages</a>. You can use the following stages to develop
                                and test event handling: <ul class="ul" id="concept_oyv_zfk_fy__ul_ysc_2hk_fy">
                                    <li class="li">Dev Data Generator enhancement - You can now configure the
                                        Dev Data Generator to generate event records as well as data
                                        records. You can also specify the number of records in a
                                        batch. </li>

                                    <li class="li">To Event - Generates event records using the incoming record
                                        as the body of the event record.</li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_q3x_pvm_3y">
                            <li class="li"><a class="xref" href="../Installation/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Installation requirements</a>:<ul class="ul" id="concept_oyv_zfk_fy__ul_zh5_qvm_3y">
                                    <li class="li">Java requirement - Oracle Java 7 is supported but now
                                        deprecated. Oracle announced the end of public updates for
                                        Java 7 in April 2015. StreamSets recommends migrating to
                                        Java 8, as Java 7 support will be removed in a future Data
                                        Collector release. </li>

                                    <li class="li">File descriptors requirement - Data Collector now requires a
                                        minimum of 32,768 open file descriptors. </li>

                                </ul>
</li>

                        </ul>

                        <ul class="ul" id="concept_oyv_zfk_fy__ul_jtf_ghk_fy">
                            <li class="li"><a class="xref" href="../Installation/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space.">Core installation</a> includes the basic stage library only
                                - The core RPM and tarball installations now include the basic stage
                                library only, to allow Data Collector to use less disk space.
                                Install additional stage libraries using the Package Manager for
                                tarball installations or the command line for RPM and tarball
                                installations. <p class="p">Previously, the core installation also included
                                    the Groovy, Jython, and statistics stage libraries.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5w_mhk_fy">
                            <li class="li"><a class="xref" href="../Installation/AddtionalStageLibs.html#concept_evs_xkm_s5">New stage libraries</a>. Data Collector now supports the
                                following stage libraries: <ul class="ul" id="concept_oyv_zfk_fy__ul_oxv_nhk_fy">
                                    <li class="li">Apache Kudu version 1.0.x - Earlier Kudu versions are no
                                        longer supported. </li>

                                    <li class="li">Cloudera CDH version 5.9 distribution of Apache Hadoop. </li>

                                    <li class="li">Cloudera version 5.9 distribution of Apache Kafka 2.0. </li>

                                    <li class="li">Elasticsearch version 5.0.x. </li>

                                    <li class="li">Google Cloud Bigtable. </li>

                                    <li class="li">Hortonworks HDP version 2.5 distribution of Apache Hadoop. </li>

                                    <li class="li">MySQL Binary Log. </li>

                                    <li class="li">Salesforce. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Configuration/Authentication.html#concept_x2j_5ts_g5" title="If your organization uses LDAP, configure Data Collector to use LDAP authentication. After you configure LDAP authentication, users log in to Data Collector using their LDAP username and password.">LDAP authentication</a> - If you use LDAP authentication,
                                you can now configure Data Collector to connect to multiple LDAP
                                servers. You can also configure Data Collector to support an LDAP
                                deployment where members are defined by uid or by full DN. </li>

                            <li class="li"><a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector</a> - Data Collector now uses the
                                Concurrent Mark Sweep (CMS) garbage collector by default. You can
                                configure Data Collector to use a different garbage collector by
                                modifying Java configuration options in the Data Collector
                                environment configuration file.</li>

                            <li class="li">Environment variables for <a class="xref" href="../Configuration/DCEnvironmentConfig.html#concept_vrx_4fg_qr" title="Increase or decrease the Data Collector Java heap size as necessary, based on the resources available on the host machine. By default, the Java heap size is 1024 MB. You can enable remote debugging to debug a Data Collector instance running on a remote machine. You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java configuration options</a>. Data Collector now uses
                                three environment variables to define Java configuration options:
                                    <ul class="ul" id="concept_oyv_zfk_fy__ul_wym_thk_fy">
                                    <li class="li">SDC_JAVA_OPTS - Includes configuration options for all Java
                                        versions. SDC_JAVA7_OPTS - Includes configuration options
                                        used only when Data Collector is running Java 7.</li>

                                    <li class="li">SDC_JAVA8_OPTS - Includes configuration options used only
                                        when Data Collector is running Java 8. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">New time zone property</a> - You can configure the Data
                                Collector UI to use UTC, the browser time zone, or the Data
                                Collector time zone. The time zone property affects how dates and
                                times display in the UI. The default is the browser time zone.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_kq1_whk_fy">
                            <li class="li">New <a class="xref" href="../Origins/MySQLBinaryLog.html#concept_kqg_1yh_xx" title="The MySQL Binary Log origin can process binary logs from a MySQL server configured to use row-based logging.You can configure the origin to start reading the binary log file from the beginning of the file or from an initial offset in the file.The binary log file captures all changes made to the MySQL database. If you want the MySQL Binary Log origin to capture changes from a subset of tables, you can configure the origin to include changes from specific tables or to ignore changes from specific tables.">MySQL Binary Log origin</a> - Reads MySQL binary logs to
                                generate records with change data capture information. </li>

                            <li class="li">New <a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx" title="The Salesforce origin reads data from Salesforce.">Salesforce origin </a>- Reads data from Salesforce. The
                                origin can execute a SOQL query to read existing data from
                                Salesforce. The origin can also subscribe to the Force.com Streaming
                                API to receive notifications for changes to Salesforce data. </li>

                            <li class="li"><a class="xref" href="../Origins/Directory.html#concept_qpt_rg3_cy">Directory origin</a> enhancement - You can configure the
                                Directory origin to read files from all subdirectories when using
                                the last-modified timestamp for the read order. </li>

                            <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Query Consumer</a> and <a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client</a> origin enhancement - You can now
                                configure the transaction isolation level that the JDBC Query
                                Consumer and Oracle CDC Client origins use to connect to the
                                database. Previously, the origins used the default transaction
                                isolation level configured for the database.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_wfg_13k_fy">
                            <li class="li">New <a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx" title="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop.">Spark
                                    Evaluator processor</a> - Processes data based on a Spark
                                application that you develop. Use the Spark Evaluator processor to
                                develop a Spark application that performs custom processing within a
                                pipeline. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldFlattener.html#concept_vpx_zc1_xx">Field Flattener processor</a> enhancements - In addition to
                                flattening the entire record, you can also now use the Field
                                Flattener processor to flatten specific list or map fields in the
                                record. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#concept_sym_c4g_xx" title="You can use the Field Type Converter to change the scale of decimal fields. For example, you might have a decimal field with the value 12345.6789115, and you'd like to decrease the scale to 4 so that the value is 12345.6789.">Field Type Converter processor</a> enhancements - You can
                                now use the Field Type Converter processor to change the scale of a
                                decimal field. Or, if you convert a field with another data type to
                                the Decimal data type, you can configure the scale to use in the
                                conversion. </li>

                            <li class="li"><a class="xref" href="../Processors/ListPivoter.html#concept_ekg_313_qw" title="Configure a Field Pivoter to pivot data in a list, map, or list-map field and generate a record for each item in the field.">Field
                                    Pivoter processor</a> enhancements - The List Pivoter
                                processor has been renamed to the Field Pivoter processor. You can
                                now use the processor to pivot data in a list, map, or list-map
                                field. You can also use the processor to save the field name of the
                                first-level item in the pivoted field. </li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> and <a class="xref" href="../Processors/JDBCTee.html#task_qpj_ncy_hw">JDBC Tee</a> processor enhancement - You can now configure
                                the transaction isolation level that the JDBC Lookup and JDBC Tee
                                processors use to connect to the database. Previously, the origins
                                used the default transaction isolation level configured for the
                                database. </li>

                            <li class="li">Scripting processor enhancements - The <a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors can generate event records
                                and work with record header attributes. The sample scripts now
                                include examples of both and a new tip for generating unique record
                                IDs. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLFlattener.html#task_pmb_l55_sv" title="Configure an XML Flattener to flatten XML data embedded in a string field.">XML Flattener processor</a> enhancement - You can now
                                configure the XML Flattener processor to write the flattened data to
                                a new output field. Previously, the processor wrote the flattened
                                data to the same field. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5" title="Configure an XML Parser to parse XML data in a string field.">XML
                                    Parser processor</a> enhancement. You can now generate
                                records from XML documents using simplified XPath expressions. This
                                enables reading records from deeper within XML documents. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5j_l3k_fy">
                            <li class="li">New <a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> - Writes data to
                                Microsoft Azure Data Lake Store. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Bigtable.html#concept_pl5_tmq_tx" title="The Google Bigtable destination requires the BoringSSL library. You must download and install the external library so that the Google Bigtable destination can access it.Configure the Google Application Default Credentials that the Google Bigtable destination uses to connect to Google Cloud Bigtable. Each Google Cloud Bigtable table has one index, the row key. When you configure the Google Bigtable destination, you define which field or fields in the record to use as the row key. When you map record fields to Google Cloud Bigtable columns, you specify whether the Cloud Bigtable storage type is text or binary. The destination converts the data types of the record fields to the Cloud Bigtable storage types. When you configure the Google Bigtable destination, you map record fields to Google Cloud Bigtable columns. You define the Cloud Bigtable columns to write to by defining the column family and column qualifier.The time basis determines the timestamp value added for each column written to Google Cloud Bigtable.">Google Bigtable destination</a> - Writes data to Google
                                Cloud Bigtable. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">Salesforce destination</a> - Writes data to Salesforce. New
                                Wave Analytics destination. Writes data to Salesforce Wave
                                Analytics. The destination creates a dataset with external data. </li>

                            <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">Amazon S3 destination</a> change - The AWS KMS Key ID
                                property has been renamed AWS KMS Key ARN. Data Collector upgrades
                                existing pipelines seamlessly. </li>

                            <li class="li">File suffix enhancement. You can now configure a file suffix, such
                                as txt or json, for output files generated by <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_zvc_bv5_1r">Local
                                    FS</a>, <a class="xref" href="../Destinations/MapRFS.html#concept_spv_xlc_fv" title="The MapR FS destination writes files to MapR FS. You can write the data to MapR as flat files or Hadoop sequence files.">MapR
                                    FS</a>, and the <a class="xref" href="../Destinations/AmazonS3.html#concept_avx_bnq_rt" title="The Amazon S3 destination writes data to Amazon S3. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon Kinesis Streams, use the Kinesis Producer destination.">Amazon
                                    S3</a> destinations. </li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht" title="The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log.">JDBC Producer</a> destination enhancement - You can now
                                configure the transaction isolation level that the JDBC Producer
                                destination uses to connect to the database. Previously, the
                                destination used the default transaction isolation level configured
                                for the database. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#concept_dvg_vvj_wx">Kudu destination</a> enhancement - You can now configure the
                                destination to perform one of the following write operations:
                                insert, update, delete, or upsert.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_r3d_dkk_fy">
                            <li class="li"><a class="xref" href="../Data_Formats/XMLDFormat.html#concept_lty_42b_dy">XML processing</a> enhancement - You can now generate
                                records from XML documents using <a class="xref" href="../Data_Formats/XMLDFormat.html#concept_zw2_mfk_dy">simplified XPath expressions</a> with origins that process
                                XML data and the XML Parser processor. This enables reading records
                                from deeper within XML documents. </li>

                            <li class="li">Consolidated data format properties - You now configure the data
                                format and related properties on a new Data Format tab. Previously,
                                data formats had individual configuration tabs, e.g., Avro,
                                Delimited, Log. <p class="p">Related properties, such as Charset, Compression
                                    Format, and Ignore Control Characters now appear on the Data
                                    Format tab as well. </p>
</li>

                            <li class="li"><a class="xref" href="../Data_Formats/WholeFile.html#concept_ojv_sr4_vx">Checksum generation for whole files</a> - Destinations that
                                stream whole files can now generate checksums for the files so you
                                can confirm the accurate transmission of the file.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Maintenance</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_h24_3kk_fy">
                            <li class="li">Add labels to pipelines from the Home page - You can now add labels
                                to multiple pipelines from the Data Collector Home page. Use labels
                                to group similar pipelines. For example, you might want to group
                                pipelines by database schema or by the test or production
                                environment. </li>

                            <li class="li">Reset the origin for multiple pipelines from the Home page - You can
                                now reset the origin for multiple pipelines at the same time from
                                the Data Collector Home page.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Rules and Alerts</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_sfk_lkk_fy">
                            <li class="li"><a class="xref" href="../Alerts/RulesAlerts_title.html#concept_ky2_g4f_qv">Metric
                                    rules and alerts</a> enhancements - The gauge metric type can
                                now provide alerts based on the number of input, output, or error
                                records for the last processed batch.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language Functions</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_bwr_nkk_fy">
                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_kxj_nyl_5x">file functions </a>- You can use the following new file
                                functions to work with file paths:<ul class="ul" id="concept_oyv_zfk_fy__ul_xdq_4kk_fy">
                                    <li class="li">file:fileExtension(&lt;filepath&gt;) - Returns the file
                                        extension from a path. </li>

                                    <li class="li">file:fileName(&lt;filepath&gt;) - Returns a file name from a
                                        path. </li>

                                    <li class="li">file:parentPath(&lt;filepath&gt;) - Returns the parent path of
                                        the specified file or directory. </li>

                                    <li class="li">file:pathElement(&lt;filepath&gt;, &lt;integer&gt;) - Returns the
                                        portion of the file path specified by a positive or negative
                                        integer. </li>

                                    <li class="li">file:removeExtension(&lt;filepath&gt;) - Removes the file
                                        extension from a path. </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx">pipeline functions</a> - You can use the following new
                                pipeline functions to determine information about a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_lc5_rkk_fy">
                                    <li class="li">pipeline:name() - Returns the pipeline name. </li>

                                    <li class="li">pipeline:version() - Returns the pipeline version when the
                                        pipeline has been published to Dataflow Performance Manager
                                        (DPM). </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">time functions</a> - You can use the following new time
                                functions to transform datetime data:<ul class="ul" id="concept_oyv_zfk_fy__ul_znz_skk_fy">
                                    <li class="li"> time:extractLongFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a long value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:extractStringFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a string value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:millisecondsToDateTime(&lt;long&gt;) - Converts an epoch
                                        or UNIX time in milliseconds to a Date object. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navprev"><a class="link" href="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Getting Started</span></a></span>  
<span class="navnext"><a class="link" href="../Installation/Install_title.html" title="Installation"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Installation</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"></div>
</body>
</html>