
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Meet Dataflow Performance Manager" /><meta name="abstract" content="StreamSets Dataflow Performance Manager (DPMTM) is a management console for data in motion. DPM lets you map, measure, and master complex dataflows within your organization." /><meta name="description" content="StreamSets Dataflow Performance Manager (DPMTM) is a management console for data in motion. DPM lets you map, measure, and master complex dataflows within your organization." /><meta name="DC.Relation" scheme="URI" content="../DPM/DPM_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_l45_qwf_xw" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Meet Dataflow Performance Manager</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../DPM/DPM_title.html" title="Dataflow Performance Manager">Dataflow Performance Manager</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../DPM/DPM_title.html" title="Dataflow Performance Manager"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Dataflow Performance Manager</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_l45_qwf_xw">
 <h1 class="title topictitle1">Meet Dataflow Performance Manager</h1>

 
 <div class="body conbody"><p class="shortdesc">StreamSets <span class="ph">Dataflow Performance Manager</span> (<span class="ph">DPM</span><sup class="ph sup">TM</sup>) is a management console for data in motion. <span class="ph">DPM</span> lets
        you map, measure, and master complex dataflows within your organization. </p>

        <p class="p">Dataflow Performance
                ManagerWith <span class="ph">DPM</span>,
            you can map multiple dataflows in a single visual topology and track changes to the
            dataflows over time. You can view real-time statistics to measure dataflow performance
            across each topology, from end-to-end or point-to-point. You can master your day-to-day
            operations by performing release and configuration management, and monitoring alerts to
            ensure incoming data meets business requirements for availability and accuracy.</p>

        <p class="p">Multiple types of users within your organization can perform different roles in <span class="ph">DPM</span>.
            For example, a data architect typically creates a high-level design of how data needs to
            flow through multiple systems. A data engineer uses this high-level design to develop
            individual pipelines in a development <span class="ph">Data
                  Collector</span>.
            The data engineer then publishes the finished pipelines to <span class="ph">DPM</span>. </p>

        <p class="p">A DevOps or site reliability engineer adds published pipelines to jobs and then starts
            the jobs across multiple <span class="ph">Data
                  Collector</span>s,
            running a remote pipeline instance on each <span class="ph">Data
                  Collector</span>.
            Data architects map the related jobs into a single visual topology, and then use the
            topology to monitor and measure the complete dataflow. DevOps engineers create data SLAs
            (service level agreements) for topologies to define thresholds that the dataflows cannot
            exceed, ensuring that data is delivered in a timely manner.</p>

        <p class="p">Let’s take a closer look at what data architects, data engineers, and DevOps engineers
            can accomplish with <span class="ph">DPM</span>.</p>

    </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_knb_cjq_dx">
 <h2 class="title topictitle2">Design the Complete Data Architecture</h2>

 <div class="body conbody">
        <p class="p">As a data architect - the person responsible for defining how data is stored, consumed,
            and managed by different systems - you design the complete flow of data through multiple
            systems. You might architect the high-level design in a design document or diagram.
            Then, you work with your team to develop <span class="ph">Data
                  Collector</span>
            pipelines that meet those dataflow needs.</p>

        <p class="p">For example, you need to create a 360-degree view of your customers by collecting all
            customer data captured in your organization’s social feeds, enterprise data warehouse,
            and website logs. You need to send all of the data to the Hadoop Distributed File System
            (HDFS) for further analysis using tools such as Hive and Impala. You determine that the
            website logs must be written to Kafka as an intermediary system before being streamed to
            HDFS. </p>

        <p class="p">To address this need, you create the following high-level design of the complete data
            flow:</p>

        <p class="p"><img class="image" id="concept_knb_cjq_dx__image_w1d_k5r_dx" src="../Graphics/DPM_DesignArchitecture.png" height="266" width="644" /></p>

        <p class="p">Then the rest of your team uses this high-level design to develop the necessary
            pipelines, jobs, and topologies within <span class="ph">Data
                  Collector</span>
            and <span class="ph">DPM</span>.</p>

 </div>

</div>
<div class="topic concept nested1" id="concept_ung_wbs_bx">
 <h2 class="title topictitle2">Manage the Pipeline Repository</h2>

 
 <div class="body conbody"><p class="shortdesc">As a data engineer - the person responsible for making sure that data flows smoothly
        between systems - you use a development <span class="ph">Data
                  Collector</span>
        instance to develop the pipelines needed to implement the designed data architecture. When
        the pipelines are complete, you publish the pipelines to the pipeline repository in <span class="ph">DPM</span>. </p>

        <p class="p"><span class="ph">DPM</span>
            provides release management of pipelines. A typical pipeline development cycle involves
            iterative changes to the pipeline. <span class="ph">DPM</span>
            maintains the version history of each published pipeline. For example, as you design the
            Social Feeds Dataflows pipeline, you test the pipeline and then make changes to it. As a
            result, you will likely publish the pipeline to the <span class="ph">DPM</span>
            pipeline repository multiple times, as displayed in the following image of the pipeline
            history: </p>

        <p class="p"><img class="image" id="concept_ung_wbs_bx__image_llt_s4t_bx" src="../Graphics/DPM_ManagePipelineRepository.png" height="209" width="505" /></p>

        <p class="p">When viewing the pipeline history in <span class="ph">DPM</span>,
            you can view the configuration details for any pipeline version and can compare pipeline
            versions side-by-side. For example, if we click the <span class="ph uicontrol">Compare with Previous
                Version</span> icon <img class="image" id="concept_ung_wbs_bx__image_zqv_3st_bx" src="../Graphics/icon_DPM_ComparePreviousVersion.png" height="18" width="18" /> for version 3 in the image above, <span class="ph">DPM</span>
            displays version 2 and version 3 in the comparison window as follows: </p>

        <p class="p"><img class="image" id="concept_ung_wbs_bx__image_bzq_3d5_bx" src="../Graphics/DPM_ComparePipelineVersions.png" height="252" width="629" /></p>

        <p class="p">We can see that version 3 of the pipeline added an Expression Evaluator processor. We can
            drill into the details of each pipeline stage, and compare the configuration of each
            stage between the versions.</p>

        <p class="p">You can add tags to pipeline versions to mark release points or to separate development
            and production environments. For example, when you finish developing the Social Feeds
            Dataflows pipeline, you add a "Ready to Deploy" tag to the latest version. That tag
            informs your DevOps engineer which pipeline version is ready to be added to a job and
            run.</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_inh_ccs_bx">
 <h2 class="title topictitle2">Manage the Orchestration of Jobs</h2>

 
 <div class="body conbody"><p class="shortdesc">In <span class="ph">DPM</span>,
        pipelines are the design of the dataflow. Jobs are the execution of the dataflow. Data
        engineers design pipelines on an authoring <span class="ph">Data
                  Collector</span>.
        DevOps or site reliability engineers run jobs on groups of execution <span class="ph">Data
                  Collector</span>s.</p>

        <p class="p">
            
        </p>

        <p class="p">As a DevOps or site reliability engineer - the person responsible for ensuring that all
            services and systems are scalable and reliable - you register both authoring and
            execution <span class="ph">Data
                  Collector</span>s
            with <span class="ph">DPM</span>. You register <span class="ph">Data
                  Collector</span>s
            by manually administering them or by automatically provisioning them on a container
            orchestration framework such as Kubernetes. All registered <span class="ph">Data
                  Collector</span>s -
            either manually administered or automatically provisioned - function in the same
            way.</p>

        <p class="p">You create <span class="ph">DPM</span>
            jobs to run pipelines on groups of execution <span class="ph">Data
                  Collector</span>s.
            When you start a job on a group of <span class="ph">Data
                  Collector</span>s,
                <span class="ph">DPM</span>
            remotely runs the pipeline on the <span class="ph">Data
                  Collector</span>s
            in the group. This enables you to manage and orchestrate large scale dataflows run
            across multiple <span class="ph">Data
                  Collector</span>s.</p>

        <p class="p">You might organize jobs by project, geographic region, or department. For example, your
            data engineer has developed and published the WebLog Collection pipeline and the EDW
            Replication Flows pipeline. In your data center, you designate <span class="ph">Data
                  Collector</span>s
            that run on several of the servers as the web server group. You designate another group
            of <span class="ph">Data
                  Collector</span>s running on other servers as the data warehouse group. You then create a job to run
            the WebLog Collection pipeline on the group of web server <span class="ph">Data
                  Collector</span>s. </p>

        <p class="p">The following image displays how the WebLog Collection job runs a remote pipeline
            instance on each <span class="ph">Data
                  Collector</span> in
            the web server group. The job does not run pipelines on <span class="ph">Data
                  Collector</span>s
            in the data warehouse group, which are <span class="ph">Data
                  Collector</span>s
            that are reserved for pipelines that read from the enterprise data warehouse.</p>

        <p class="p"><img class="image" id="concept_inh_ccs_bx__image_fkl_vcc_fbb" src="../Graphics/DPM_ManageOrchestration.png" height="201" width="578" /></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_t34_2ds_bx">
 <h2 class="title topictitle2">Map Jobs into a Topology</h2>

 
 <div class="body conbody"><p class="shortdesc">In <span class="ph">Data
                  Collector</span>, you can monitor and view the details of a single pipeline. However, you typically run
        multiple intermediary pipelines, all of which work together to create a complete
        dataflow.</p>

        <p class="p">As a data architect, you create a
            topology in <span class="ph">DPM</span> to
            map multiple related jobs into a single view. A topology provides interactive end-to-end
            views of data as it traverses multiple pipelines. You can add any number of jobs to a
            topology.</p>

        <p class="p">To continue our Customer 360 example, after the WebLog Collection pipeline reads web
            server log files and writes the data to Kafka, another pipeline consumes the Kafka data,
            processes it, and streams the data to HDFS. Additional pipelines read from Twitter
            social feeds and from an enterprise data warehouse and also write the data to HDFS. In
                <span class="ph">DPM</span>, you can create a topology that includes jobs for all the pipelines, as follows: </p>

        <p class="p"><img class="image" id="concept_t34_2ds_bx__image_q5h_1dc_fbb" src="../Graphics/DPM_MapPipelinesTopology.png" height="366" width="742" /></p>

        <p class="p">From the topology, you can select each job and then drill into the configuration details
            of each pipeline. For example, if we select the Social Feeds Dataflows job in the
            topology canvas above, we can the three stages included in the pipeline in the detail
            pane on the right.</p>

    </div>

</div>
<div class="topic concept nested1" id="concept_sx5_zds_bx">
 <h2 class="title topictitle2">Measure Dataflow Quality</h2>

 
 <div class="body conbody"><p class="shortdesc"><span class="ph">DPM</span>
        provides the same level of detailed monitoring for topologies and jobs that you are
        accustomed to seeing for pipelines within <span class="ph">Data
                  Collector</span>. </p>

        <p class="p">As a data architect or as a DevOps
            or site reliability engineer, you can measure the health of the topology and the
            performance of all jobs and connecting systems included in the topology. <span class="ph">DPM</span>
            monitoring provides real-time statistics and error information about the running
            pipelines. </p>

        <p class="p">For example, the detail pane of the Customer 360 topology provides a single view into the
            record count and throughput for all running pipelines in the topology:</p>

        <p class="p"><img class="image" id="concept_sx5_zds_bx__image_t1k_s2c_fbb" src="../Graphics/DPM_MeasureQualityTopology.png" height="357" width="796" /></p>

        <p class="p">You can select a job or a connecting system within the topology to discover more detailed
            monitoring about the job or system. </p>

        <p class="p">When you start a job on a group of <span class="ph">Data
                  Collector</span>s,
                <span class="ph">DPM</span>
            provides a single view of the statistics for the complete job. From a job, you can view
            the statistics for a single pipeline or you can view the aggregated statistics across
            all remote pipeline instances that are run on a group of <span class="ph">Data
                  Collector</span>s.</p>

        <p class="p">For example, if we select the Social Feeds Dataflows job in the topology canvas, the
            detail pane displays metrics for the selected job:</p>

        <p class="p"><img class="image" id="concept_sx5_zds_bx__image_hrt_t2c_fbb" src="../Graphics/DPM_MeasureQualityJob.png" height="397" width="800" /></p>

    </div>

</div>
<div class="topic concept nested1" id="concept_xvp_x2x_fz">
 <h2 class="title topictitle2">Master Dataflow Operations</h2>

 
 <div class="body conbody"><p class="shortdesc">As a DevOps or site reliability engineer, you can master your day-to-day operations by
        defining data SLAs (service level agreements) to ensure that incoming data meets business
        requirements for availability and accuracy.</p>

  <p class="p">
            
        </p>

        <p class="p">In addition to measuring the health of a topology, you define data SLAs to define the
            expected thresholds of the data throughput rate or the error record rate. Data SLAs
            trigger an alert when the specified threshold is reached. Data SLA alerts provide
            immediate feedback on the data processing rates expected by your team. They enable you
            to master your dataflow operations by quickly investigating and resolving issues that
            arise.</p>

        <p class="p">For example, you have service level agreements with the operational analytics team to
            ensure that all of the data captured and processed in the Customer 360 topology is clean
            and available for immediate analysis. If any of the Customer 360 jobs encounter
            processing errors, you must immediately resolve those issues. You define and activate a
            data SLA that triggers an alert when a job in the topology encounters more than 100
            error records per second. </p>

        <p class="p">If the alert triggers, DPM notifies you with a red Notifications icon in the top toolbar:
                <img class="image" id="concept_xvp_x2x_fz__image_gk1_rfx_fz" src="../Graphics/icon_Notifications.png" height="16" width="14" />.
            You drill into the details of the data SLA to discover which threshold was reached and
            to investigate the issues that need to be resolved. The triggered data SLA displays a
            graph of the error record rate. The red line in the graph represents the defined
            threshold, as follows:</p>

        <p class="p"><img class="image" id="concept_xvp_x2x_fz__image_q5c_y2c_fbb" src="../Graphics/DPM_MasterOperations.png" height="443" width="538" /></p>

        <p class="p">We've seen how you can use <span class="ph">DPM</span> to
            turn a high-level architecture diagram of your dataflows into pipelines and jobs that
            you can then manage and measure from a single topology. Give it a try, and see for
            yourself how easy <span class="ph">DPM</span> has
            made it to map, measure, and master your data in motion.</p>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../DPM/DPM_title.html" title="Dataflow Performance Manager"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Dataflow Performance Manager</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"></div>
</body>
</html>