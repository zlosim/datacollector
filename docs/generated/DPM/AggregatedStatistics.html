
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Pipeline Statistics" /><meta name="abstract" content="A DPM job defines the pipeline to run and the Data Collectors that run the pipeline. When you start a job, DPM remotely runs the pipeline on the group of Data Collectors. To monitor the job statistics and metrics within DPM, you must configure the pipeline to write statistics to DPM or to another system." /><meta name="description" content="A DPM job defines the pipeline to run and the Data Collectors that run the pipeline. When you start a job, DPM remotely runs the pipeline on the group of Data Collectors. To monitor the job statistics and metrics within DPM, you must configure the pipeline to write statistics to DPM or to another system." /><meta name="DC.Relation" scheme="URI" content="../DPM/DPM_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_h2q_mb5_xw" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Pipeline Statistics</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../DPM/DPM_title.html" title="Dataflow Performance Manager">Dataflow Performance Manager</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../DPM/DPM_title.html" title="Dataflow Performance Manager"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Dataflow Performance Manager</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_h2q_mb5_xw">
 <h1 class="title topictitle1">Pipeline Statistics</h1>

 
 <div class="body conbody"><p class="shortdesc">A <span class="ph">DPM</span>
        job defines the pipeline to run and the <span class="ph">Data
                  Collector</span>s that
        run the pipeline. When you start a job, <span class="ph">DPM</span>
        remotely runs the pipeline on the group of <span class="ph">Data
                  Collector</span>s. To
        monitor the job statistics and metrics within <span class="ph">DPM</span>, you
        must configure the pipeline to write statistics to <span class="ph">DPM</span> or to
        another system.</p>

  <p class="p">When a job runs on a
            single <span class="ph">Data
                  Collector</span>,
            you can simply configure the pipeline to write the statistics directly to <span class="ph">DPM</span>. By
            default, pipelines are configured to write statistics directly to <span class="ph">DPM</span>.</p>

        <p class="p">When a job runs on multiple <span class="ph">Data
                  Collector</span>s,
            a remote pipeline instance runs on each of the <span class="ph">Data
                  Collector</span>s.
            To view aggregated statistics for the job within <span class="ph">DPM</span>,
            you must configure the pipeline to write the statistics to one of the following
            systems:</p>

        <ul class="ul" id="concept_h2q_mb5_xw__ul_e23_pxp_yy">
            <li class="li">SDC RPC</li>

            <li class="li">Kafka cluster</li>

            <li class="li">Amazon Kinesis Streams</li>

            <li class="li">MapR Streams</li>

        </ul>

        <p class="p">When you start a job that includes a pipeline configured to write to Kafka, Kinesis, MapR
            Streams, or SDC RPC, <span class="ph">DPM</span>
            automatically generates and runs a system pipeline for the job. The system pipeline
            reads the statistics written by each running pipeline instance to Kafka, Kinesis, MapR
            Streams, or SDC RPC. Then, the system pipeline aggregates and sends the statistics to
                <span class="ph">DPM</span>. </p>

        <div class="note important"><span class="importanttitle">Important:</span> For a production environment, use a Kafka
                  cluster, Amazon Kinesis Streams, or MapR Streams to aggregate statistics. Using
                  SDC RPC to aggregate statistics is not highly available and might cause the loss
                  of some data. It should be used for development purposes only.</div>

        <p class="p">When a <span class="ph">DPM</span> job includes a pipeline that is configured to write statistics, you can view the
            statistics and metrics when you monitor the job in <span class="ph">DPM</span>:</p>

        <p class="p"><img class="image" id="concept_h2q_mb5_xw__image_h2w_wxs_gx" src="../Graphics/DPM_MonitorJob.png" height="320" width="719" /></p>

        <p class="p">When a <span class="ph">DPM</span> job includes a pipeline that is configured to discard statistics, <span class="ph">DPM</span>
            cannot display statistics and metrics for the job. Instead, <span class="ph">DPM</span>
            displays the following warning message when you monitor the job:</p>

        <pre class="pre codeblock">Aggregated metrics for the job are not available as individual pipeline metrics are discarded.</pre>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_abc_1w1_c1b">
 <h2 class="title topictitle2">Write Statistics Directly to DPM</h2>

 
 <div class="body conbody"><p class="shortdesc">When you write statistics directly to <span class="ph">DPM</span>, <span class="ph">DPM</span> does
        not generate a system pipeline for the job. Instead, the <span class="ph">Data
                  Collector</span>
        directly sends the statistics to <span class="ph">DPM</span>.</p>

  <p class="p">Write statistics directly to <span class="ph">DPM</span>
            when the job runs on a single <span class="ph">Data
                  Collector</span>.
            If the job runs on multiple <span class="ph">Data
                  Collector</span>s,
                <span class="ph">DPM</span>
            can display the pipeline statistics for each individual <span class="ph">Data
                  Collector</span>.
            However, <span class="ph">DPM</span>
            cannot display an aggregated view of the statistics across all running pipeline
            instances.</p>

 </div>

</div>
<div class="topic concept nested1" id="concept_c53_pzp_yy">
 <h2 class="title topictitle2">Write Statistics to SDC RPC</h2>

 <div class="body conbody">
        <p class="p">When you write statistics to SDC RPC, <span class="ph">Data
                  Collector</span>
            effectively adds an SDC RPC destination to the pipeline that you are configuring. <span class="ph">DPM</span>
            automatically generates and runs a system pipeline for the job. The system pipeline is a
            pipeline with a Dev SDC RPC with Buffering origin that reads the statistics passed from
            the SDC RPC destination, and then aggregates and sends the statistics to <span class="ph">DPM</span>. </p>

        <p class="p">Write statistics to SDC RPC when the job runs on multiple <span class="ph">Data
                  Collector</span>s.</p>

        <div class="p">When you configure a pipeline to write statistics to an SDC RPC destination, you specify
            the following information: <ul class="ul" id="concept_c53_pzp_yy__ul_dzy_wm3_1z">
                <li class="li">SDC RPC connection - The host and port number of the <span class="ph">Data
                  Collector</span> machine where <span class="ph">DPM</span> starts the system pipeline. The host must be a <span class="ph">Data
                  Collector</span> machine registered with <span class="ph">DPM</span> that can run a pipeline for the job. A <span class="ph">Data
                  Collector</span> can run the pipeline when it has all labels associated with the job. <p class="p">For
                        example, if you associate the job with the WestCoast label, then the host
                        specified in the RPC connection must be a machine with a registered <span class="ph">Data
                  Collector</span> that also has the WestCoast label.</p>
</li>

                <li class="li">SDC RPC ID - A user-defined identifier that allows SDC RPC stages to recognize
                    each other. To avoid mixing statistics from different jobs, use a unique ID for
                    each job.</li>

            </ul>
</div>

        <p class="p">You can optionally enable encryption to pass data securely and define retry and timeout
            properties.</p>

  <div class="p">
            <div class="note important"><span class="importanttitle">Important:</span> For a production environment, use a Kafka
                  cluster, Amazon Kinesis Streams, or MapR Streams to aggregate statistics. Using
                  SDC RPC to aggregate statistics is not highly available and might cause the loss
                  of some data. It should be used for development purposes only.</div>

        </div>

        <p class="p">For more information about SDC RPC pipelines, see <a class="xref" href="../RPC_Pipelines/SDC_RPCpipelines_title.html#concept_lnh_z3z_bt" title="Data Collector Remote Protocol Call pipelines, a.k.a. SDC RPC pipelines, are a set of StreamSets pipelines that pass data from one pipeline to another without writing to an intermediary system.">SDC RPC Pipeline Overview</a>.</p>

 </div>

<div class="topic concept nested2" id="concept_e1d_nzr_yy">
 <h3 class="title topictitle3">Best Practices for SDC RPC</h3>

 <div class="body conbody">
  <div class="p">Consider the following best practices
            when you configure a pipeline to write statistics to an SDC RPC destination: <ul class="ul" id="concept_e1d_nzr_yy__ul_irc_szr_yy">
                <li class="li">To avoid mixing statistics from different jobs, use a unique SDC RPC ID for each
                    job. </li>

                <li class="li">Monitor the disk space where the Dev SDC RPC with Buffering origin in the system
                    pipeline temporarily buffers the records to disk before passing the records to
                    the next stage in the pipeline.<p class="p">The Dev SDC RPC with Buffering origin in the
                        system pipeline temporarily buffers the statistics to a queue on disk. If
                        the system pipeline slows, the temporary location on disk might become full.
                        The temporary statistics are written to the location specified in the
                            <samp class="ph codeph">java.io.tmpdir</samp> system property, to a file with the
                        following
                    name:</p>
<pre class="pre codeblock">sdc-fragments&lt;file ID&gt;.queueFile</pre>
</li>

            </ul>
</div>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_wmv_cbb_fx">
 <h2 class="title topictitle2">Write Statistics to Kafka</h2>

 
 <div class="body conbody"><p class="shortdesc">When you write statistics to a Kafka cluster, <span class="ph">Data
                  Collector</span>
        effectively adds a Kafka Producer destination to the pipeline that you are configuring. <span class="ph">DPM</span>
        automatically generates and runs a system pipeline for the job. The system pipeline reads
        the statistics from Kafka, and then aggregates and sends the statistics to <span class="ph">DPM</span>. </p>

        <p class="p">Write statistics to a Kafka cluster when the
            job runs on multiple <span class="ph">Data
                  Collector</span>s.</p>

        <p class="p">When you write statistics to a Kafka cluster, you define connection information and the
            topic to write to. </p>

        <p class="p">You also configure the partition strategy. The pipeline passes data to partitions in the
            Kafka topic based on the partition strategy that you choose. You can add additional
            Kafka configuration properties as needed. </p>

        <div class="note note"><span class="notetitle">Note:</span> At this time, you cannot configure the pipeline to connect securely through SSL/TLS or
            Kerberos when using Kafka.</div>

    </div>

<div class="topic concept nested2" id="concept_wph_j2b_fx">
 <h3 class="title topictitle3">Partition Strategy</h3>

 
 <div class="body conbody"><p class="shortdesc">The partition strategy determines how to write statistics to Kafka partitions. You can
        use a partition strategy to balance the work load or to write data semantically.</p>

        <p class="p">The pipeline can use one of the following partition strategies:</p>

        <p class="p">
            
        </p>

     <dl class="dl">
         
             <dt class="dt dlterm">Round-Robin</dt>

             <dd class="dd">Writes statistics to a different partition using a cyclical order. Use for load
                    balancing. </dd>

         
         
             <dt class="dt dlterm">Random</dt>

             <dd class="dd">Writes statistics to a different partition using a random order. Use for load
                    balancing.</dd>

         
         
                <dt class="dt dlterm">Expression</dt>

                <dd class="dd">Writes statistics to a partition based on the results of the partition
                    expression. Use to perform semantic partitioning. </dd>

                <dd class="dd">When you configure the partition expression, define the expression to evaluate
                    to the partition where you want statistics written. </dd>

            
            
                <dt class="dt dlterm">Default</dt>

                <dd class="dd">Writes statistics using the default partition strategy that Kafka provides.</dd>

            
     </dl>

    </div>

</div>
<div class="topic concept nested2" id="concept_bmq_ff5_xw">
 <h3 class="title topictitle3">Best Practices for a Kafka Cluster</h3>

 <div class="body conbody">
  <p class="p">Consider the following best practices
            when you configure a pipeline to write statistics to a Kafka cluster: </p>

        <ul class="ul" id="concept_bmq_ff5_xw__ul_vlq_jg5_xw">
            <li class="li">To avoid mixing statistics from different jobs, use a unique topic name for each
                job. </li>

            <li class="li">Consider the Kafka retention policy. <p class="p">Each running pipeline instance writes
                    statistics to Kafka, and then the system pipeline consumes the statistics from
                    Kafka. If the system pipeline unexpectedly shuts down, Kafka retains the
                    statistics for the amount of time determined by the Kafka retention policy. If
                    the system pipeline is down for longer than Kafka retains data, the statistics
                    are lost.</p>
</li>

        </ul>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_em4_2bb_fx">
 <h2 class="title topictitle2">Write Statistics to Kinesis Streams</h2>

 
 <div class="body conbody"><p class="shortdesc"> When you write statistics to Amazon Kinesis Streams, <span class="ph">Data
                  Collector</span>
        effectively adds a Kinesis Producer destination to the pipeline that you are configuring.
            <span class="ph">DPM</span>
        automatically generates and runs a system pipeline for the job. The system pipeline reads
        the statistics from Kinesis Streams, and then aggregates and sends the statistics to <span class="ph">DPM</span>. </p>

        <p class="p">Write statistics to Kinesis Streams when the job runs on multiple <span class="ph">Data
                  Collector</span>s.</p>

        <p class="p">When you write statistics to Kinesis Streams, you define connection information and the
            stream to write to.</p>

        <p class="p">You also configure the partition strategy. The pipeline passes data to partitions in
            Kinesis shards based on the partition strategy that you choose. You can add additional
            Kinesis configuration properties as needed. </p>

 </div>

<div class="topic concept nested2" id="concept_m3j_2kb_fx">
 <h3 class="title topictitle3">AWS Credentials</h3>

 
 <div class="body conbody"><p class="shortdesc">When the pipeline writes aggregated statistics to Amazon Kinesis Streams, it must pass
        credentials to Amazon Web Services. </p>

  <p class="p">Use one of the following methods to pass AWS
                credentials:</p>

     <dl class="dl">
         
             <dt class="dt dlterm">IAM roles</dt>

             <dd class="dd">When <span class="ph">Data
                  Collector</span> runs on an Amazon EC2 instance, you can use the AWS Management
                 Console to configure an IAM role for the EC2 instance. <span class="ph">Data
                  Collector</span> uses the IAM instance profile credentials to automatically connect
                 to AWS. </dd>

             <dd class="dd">When you use IAM roles, you do not need to specify the Access Key ID and Secret
                    Access Key properties when you configure statistics for the pipeline. </dd>

             <dd class="dd">For more information about assigning an IAM role to an EC2 instance, see
                 the Amazon EC2 documentation.</dd>

         
         
             <dt class="dt dlterm">AWS access key pairs</dt>

             <dd class="dd">
                 <p class="p">When <span class="ph">Data
                  Collector</span> does not run on an Amazon EC2 instance or when the EC2 instance doesn’t
                        have an IAM role, you must specify the <span class="ph uicontrol">Access Key ID</span>
                        and <span class="ph uicontrol">Secret Access Key</span> properties when you configure
                        statistics for the pipeline.</p>

             </dd>

         
     </dl>

 </div>

</div>
<div class="topic concept nested2" id="concept_fdy_5f5_xw">
 <h3 class="title topictitle3">Best Practices for Kinesis Streams</h3>

 <div class="body conbody">
  <p class="p">Consider the following best practices
            when you configure a pipeline to write statistics to Amazon Kinesis Streams: </p>

        <ul class="ul" id="concept_fdy_5f5_xw__ul_w44_3g5_xw">
            <li class="li">To avoid mixing statistics from different jobs, use a unique stream name for each
                job. </li>

            <li class="li">Consider the Kinesis Streams retention policy. <p class="p">Each running pipeline instance
                    writes statistics to Kinesis Streams, and then the system pipeline reads the
                    statistics from Kinesis Streams. If the system pipeline unexpectedly shuts down,
                    Kinesis Streams retains the statistics for the amount of time determined by the
                    Kinesis Streams retention policy. If the system pipeline is down for longer than
                    Kinesis Streams retains data, the statistics are lost.</p>
</li>

        </ul>

 </div>

</div>
</div>
<div class="topic concept nested1" id="concept_qh5_v5t_mbb">
 <h2 class="title topictitle2">Write Statistics to MapR Streams</h2>

 
 <div class="body conbody"><p class="shortdesc">When you write statistics to MapR Streams, <span class="ph">Data
                  Collector</span>
        effectively adds a MapR Streams Producer destination to the pipeline that you are
        configuring. <span class="ph">DPM</span>
        automatically generates and runs a system pipeline for the job. The system pipeline reads
        the statistics from MapR Streams, and then aggregates and sends the statistics to <span class="ph">DPM</span>. </p>

        <p class="p">Write statistics to MapR Streams when the job
            runs on multiple <span class="ph">Data
                  Collector</span>s.</p>

        <p class="p">When you write statistics to MapR Streams, you define the topic to write to. You also
            configure the partition strategy. The pipeline passes data to partitions in the MapR
            Streams topic based on the partition strategy that you choose. You can add additional
            MapR Streams configuration properties as needed. </p>

     <p class="p">Before you can write statistics to MapR Streams, you must
                  perform additional steps to enable <span class="ph">Data
                  Collector</span>
                  to process MapR data. <span class="ph">For more information, see <a class="xref" href="../Installation/MapR-Prerequisites.html#concept_jgs_qpg_2v" title="Due to licensing restrictions, StreamSets cannot distribute MapR libraries with Data Collector. As a result, you must perform additional steps to enable the Data Collector machine to connect to MapR. Data Collector does not display MapR origins and destinations in stage library lists nor the MapR Streams statistics aggregator in the pipeline properties until you perform these prerequisites.">MapR Prerequisites</a>.</span></p>

 </div>

<div class="topic concept nested2" id="concept_xsq_1wt_mbb">
 <h3 class="title topictitle3">Partition Strategy</h3>

 
 <div class="body conbody"><p class="shortdesc">The partition strategy determines how to write statistics to MapR Streams partitions.
        You can use a partition strategy to balance the work load or to write data
        semantically.</p>

        <p class="p">The pipeline can use one of the following partition strategies:</p>

        <p class="p">
            
        </p>

        <div class="p">
            <dl class="dl">
                
                    <dt class="dt dlterm">Round-Robin</dt>

                    <dd class="dd">Writes each record to a different partition using a cyclical order. Use for
                        load balancing.</dd>

                
                
                    <dt class="dt dlterm">Random</dt>

                    <dd class="dd">Writes each record to a different partition using a random order. Use for
                        load balancing.</dd>

                
                
                    <dt class="dt dlterm">Expression</dt>

                    <dd class="dd">Writes each record to a partition based on the results of the partition
                        expression. Use to perform semantic partitioning. </dd>

                    <dd class="dd">When you configure the partition expression, define the expression to
                        evaluate to the partition where you want each record written. The expression
                        must return a numeric value. </dd>

                
                
                    <dt class="dt dlterm">Default</dt>

                    <dd class="dd">Writes each record using the default partition strategy that MapR Streams
                        provides.</dd>

                
            </dl>

        </div>

 </div>

</div>
<div class="topic concept nested2" id="concept_owb_vxt_mbb">
 <h3 class="title topictitle3">Best Practices for MapR Streams</h3>

 <div class="body conbody">
        <p class="p">Consider the following best practices
            when you configure a pipeline to write statistics to MapR Streams: </p>

        <ul class="ul" id="concept_owb_vxt_mbb__ul_vlq_jg5_xw">
            <li class="li">To avoid mixing statistics from different jobs, use a unique topic name for each
                job. </li>

            <li class="li">Consider the MapR Streams retention policy. <p class="p">Each running pipeline instance writes
                    statistics to MapR Streams, and then the system pipeline consumes the statistics
                    from MapR Streams. If the system pipeline unexpectedly shuts down, MapR Streams
                    retains the statistics for the amount of time determined by the MapR Streams
                    retention policy. If the system pipeline is down for longer than MapR Streams
                    retains data, the statistics are lost.</p>
</li>

        </ul>

 </div>

</div>
</div>
<div class="topic task nested1" id="task_lcd_ng5_xw">
    <h2 class="title topictitle2">Configuring a Pipeline to Write Statistics</h2>

    
    <div class="body taskbody"><p class="shortdesc">You can configure a pipeline to write statistics after the <span class="ph">Data
                  Collector</span> has
        been registered with <span class="ph">DPM</span>.</p>

        <ol class="ol steps"><li class="li step stepexpand">
                <span class="ph cmd">Open the pipeline in <span class="ph">Data
                  Collector</span>. </span>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="ph uicontrol">Statistics</span> tab, select one of the following
                    options for the statistics aggregator:</span>
                <div class="itemgroup info">
                    <ul class="ul" id="task_lcd_ng5_xw__ul_ljj_brd_dx">
                        <li class="li">Discard - Discard the pipeline statistics. <span class="ph">DPM</span> cannot display statistics and metrics for the job.</li>

                        <li class="li">Write to <span class="ph">DPM</span> Directly - Write the pipeline statistics directly to <span class="ph">DPM</span>. Use when the job runs on a single <span class="ph">Data
                  Collector</span>.</li>

                        <li class="li">Write to SDC RPC - Write the pipeline statistics to an SDC RPC
                            destination. Use when the job runs on multiple <span class="ph">Data
                  Collector</span>s and for development purposes only.</li>

                        <li class="li">Write to Kafka - Write the pipeline statistics to a Kafka cluster. Use
                            when the job runs on multiple <span class="ph">Data
                  Collector</span>s.</li>

                        <li class="li">Write to Kinesis - Write the pipeline statistics to Amazon Kinesis
                            Streams. Use when the job runs on multiple <span class="ph">Data
                  Collector</span>s.</li>

                        <li class="li">Write to MapR Streams - Write the pipeline statistics to MapR Streams.
                            Use when the job runs on multiple <span class="ph">Data
                  Collector</span>s.</li>

                    </ul>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">To write statistics to an SDC RPC destination, on the <span class="ph uicontrol">Stats
                        Aggregator - Write to SDC RPC</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_lcd_ng5_xw__table_jxy_4sq_yy" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d56431e872">SDC RPC Properties</th>

                                    <th class="entry" valign="top" width="70%" id="d56431e875">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">SDC RPC Connection</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Host and port where the system pipeline runs. The host
                                        must be a machine with a registered <span class="ph">Data
                  Collector</span> that runs a pipeline instance for the job. <p class="p">Use the
                                            following format: <samp class="ph codeph">&lt;host&gt;:&lt;port&gt;</samp>.
                                        </p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Retries per Batch</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Number of times the SDC RPC destination tries to write a
                                        batch to the Dev SDC RPC with Buffering origin in the system
                                        pipeline. <p class="p">When the SDC RPC destination cannot write the
                                            batch within the configured number of retries, it fails
                                            the batch.</p>
<p class="p">Default is 3.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Back off Period</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Milliseconds to wait before retrying writing a batch to
                                        the Dev SDC RPC with Buffering origin in the system
                                            pipeline.<p class="p">The value that you enter increases
                                            exponentially after each retry. For example, if you set
                                            the back off period to 10, the SDC RPC destination
                                            attempts the first retry after waiting 10 milliseconds,
                                            attempts the second retry after waiting 100
                                            milliseconds, and attempts the third retry after waiting
                                            1,000 milliseconds. Set to 0 to retry
                                            immediately.</p>
<p class="p">Default is 0.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">SDC RPC ID</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">User-defined ID to allow the SDC RPC destination to pass
                                        statistics to the system pipeline. To avoid mixing
                                        statistics from different jobs, use a unique ID for each
                                            job.<p class="p">You cannot define an expression that evaluates to
                                            the ID. </p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Connection Timeout (ms)</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Milliseconds to establish a connection to the system
                                        pipeline. <p class="p">The SDC RPC destination retries the connection
                                            based on the Retries Per Batch property.</p>
<p class="p">Default
                                            is 5000 milliseconds.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">TLS Enabled</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Enables the secure transfer of data using TLS. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Truststore File</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Truststore file for TLS. Required if the keystore file is
                                        a self-signed certificate.<p class="p">Must be stored in the <span class="ph">Data
                  Collector</span> resources directory,
                                                <span class="ph filepath">$SDC_RESOURCES</span>, on each <span class="ph">Data
                  Collector</span> machine that runs a pipeline instance for the job.
                                        </p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Truststore Password</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Password for the truststore file.<div class="note tip"><span class="tiptitle">Tip:</span> To secure sensitive information such as
                  usernames and passwords, you can use <a class="xref" href="../Pipeline_Configuration/RuntimeValues.html#concept_bs4_5nm_2s" title="Similar to runtime properties, runtime resources are values that you define in a file local to the Data Collector and call from within a pipeline. But with runtime resources, you can restrict the permissions for the files to secure sensitive information. Use runtime resources to load sensitive information from files at runtime.">runtime resources</a> or <span class="ph"><a class="xref" href="../Configuration/CredentialStores.html#concept_bt1_bpj_r1b" title="Data Collector pipeline stages communicate with external systems to read and write data. Many of these external systems require credentials - user names or passwords - to access the data. When you configure pipeline stages for these external systems, you define the credentials that the stage uses to connect to the system.">credential stores.</a></span></div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Read Timeout (ms)</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Milliseconds to wait for the Dev SDC RPC with Buffering
                                        origin in the system pipeline to read data from a batch.
                                            <p class="p">The SDC RPC destination retries the write based on
                                            the Retries Per Batch property.</p>
<p class="p">Default is 2000
                                            milliseconds.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Use Compression</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Enables the SDC RPC destination to use compression to
                                        pass data to the Dev SDC RPC with Buffering origin in the
                                        system pipeline. Enabled by default. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e872 ">Verify Host in Server Certificate</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e875 ">Verifies the host in the keystore file on the <span class="ph">Data
                  Collector</span> machine that runs the system pipeline. </td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">To write statistics to Kafka, on the <span class="ph uicontrol">Stats Aggregator - Write to
                        Kafka</span> tab, configure the following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_lcd_ng5_xw__table_i2g_jwd_dx" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d56431e1070">Kafka Properties</th>

                                    <th class="entry" valign="top" width="70%" id="d56431e1073">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                        <td class="entry" valign="top" width="30%" headers="d56431e1070 ">Broker URI</td>

                                        <td class="entry" valign="top" width="70%" headers="d56431e1073 ">Connection string for the Kafka broker. Use the
                                            following format:
                                                <samp class="ph codeph">&lt;host&gt;:&lt;port&gt;</samp>.<p class="p">To ensure a
                                                connection, enter a comma-separated list of
                                                additional broker URI.</p>
</td>

                                    </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1070 ">Runtime Topic Resolution </td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1073 ">Do not use at this time. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1070 ">Topic</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1073 ">Topic to use. To avoid mixing statistics from different
                                        jobs, use a unique topic name for each job.<p class="p">You cannot
                                            define an expression that evaluates to the topic name.
                                        </p>
</td>

                                </tr>

                                <tr>
                                        <td class="entry" valign="top" width="30%" headers="d56431e1070 ">Partition Strategy <a class="xref" href="../Destinations/KProducer.html#concept_qpm_xp4_4r" title="The partition strategy determines how to write data to Kafka partitions. You can use a partition strategy to balance the work load or to write data semantically.">
                                                <img class="image" id="task_lcd_ng5_xw__d9895e8678" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                        <td class="entry" valign="top" width="70%" headers="d56431e1073 ">Strategy to use to write to partitions:<ul class="ul" id="task_lcd_ng5_xw__d9895e8682">
                                                <li class="li">Round Robin - Takes turns writing to different
                                                  partitions.</li>

                                                <li class="li">Random - Writes to partitions randomly.</li>

                                                <li class="li">Expression - Uses an expression to write data to
                                                  different partitions. Writes records to the
                                                  partitions specified by the results of the
                                                  expression.</li>

                                                <li class="li">Default - Uses an expression to extract a
                                                  partition key from the record. Writes records to
                                                  partitions based on a hash of the partition key.
                                                </li>

                                            </ul>
</td>

                                    </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1070 ">Partition Expression </td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1073 ">Expression to use when using the expression partition
                                            strategy.<p class="p">Define the expression to evaluate to the
                                            partition where you want statistics written. Partition
                                            numbers start with 0.</p>
<p class="p">Optionally, click
                                                <span class="ph uicontrol">Ctrl + Space Bar</span> for help
                                            with creating the expression.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1070 ">Kafka Configuration</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1073 ">Additional Kafka properties to use. Using <a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">simple or bulk edit mode</a>, click the
                                            <span class="ph uicontrol">Add</span> icon and define the Kafka
                                        property name and value.<p class="p">Use the property names and values
                                            as expected by Kafka. Do not use the broker.list
                                            property.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1070 ">ZooKeeper URI</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1073 ">Connection string for the ZooKeeper of the Kafka cluster.
                                        Use the following format:
                                            <samp class="ph codeph">&lt;host&gt;:&lt;port&gt;</samp>. <p class="p">To use a
                                            ZooKeeper quorum, enter a comma-separated list.
                                            </p>
<div class="p">To use a ZooKeeper chroot path, add the path at
                                            the end of the list as
                                            follows:<pre class="pre codeblock">&lt;host&gt;:&lt;port&gt;, &lt;host2&gt;:&lt;port2&gt;, .../&lt;chroot_path&gt;</pre>
</div>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">To write statistics to Amazon Kinesis Streams, on the <span class="ph uicontrol">Stats
                        Aggregator - Write to Kinesis</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_lcd_ng5_xw__table_kh3_kls_dx" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d56431e1224">Kinesis Properties</th>

                                    <th class="entry" valign="top" width="70%" id="d56431e1227">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Access Key ID <a class="xref" href="AggregatedStatistics.html#concept_m3j_2kb_fx" title="When the pipeline writes aggregated statistics to Amazon Kinesis Streams, it must pass credentials to Amazon Web Services."><img class="image" id="task_lcd_ng5_xw__image_ekt_x9g_cs" src="../Graphics/icon_moreInfo.png" height="12" width="12" />
                                        </a></td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1227 ">
        <p class="p">AWS access key ID.</p>

        <p class="p">Required when not using IAM roles with IAM instance profile credentials.</p>

       </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Secret Access Key <a class="xref" href="AggregatedStatistics.html#concept_m3j_2kb_fx" title="When the pipeline writes aggregated statistics to Amazon Kinesis Streams, it must pass credentials to Amazon Web Services."><img class="image" id="task_lcd_ng5_xw__image_ekt_x6g_cs" src="../Graphics/icon_moreInfo.png" height="12" width="12" />
                                        </a></td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1227 ">
        <p class="p">AWS secret access key. </p>

        <p class="p">Required when not using IAM roles with IAM instance profile credentials. </p>

       </td>

                                </tr>

                                <tr>
      <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Region</td>

      <td class="entry" valign="top" width="70%" headers="d56431e1227 ">Amazon Web Services region that hosts the Kinesis cluster.</td>

     </tr>

                                <tr>
      <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Endpoint</td>

      <td class="entry" valign="top" width="70%" headers="d56431e1227 ">Endpoint to connect to when you select Other for the region. Enter the endpoint
       name.</td>

     </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Stream Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1227 ">Kinesis stream name. To avoid mixing statistics from
                                        different jobs, use a unique stream name for each job.<p class="p">You
                                            cannot define an expression that evaluates to the stream
                                            name. </p>
</td>

                                </tr>

                                <tr>
      <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Partitioning Strategy</td>

      <td class="entry" valign="top" width="70%" headers="d56431e1227 ">Strategy to write data to Kinesis shards:<ul class="ul" id="task_lcd_ng5_xw__d9924e1343">
        <li class="li">Random - Generates a random partition key.</li>

        <li class="li">
         <p class="p">Expression - Uses the result of an expression as the partition key.</p>

        </li>

       </ul>
</td>

     </tr>

                                <tr>
      <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Partition Expression</td>

      <td class="entry" valign="top" width="70%" headers="d56431e1227 ">Expression to generate the partition key used to pass data to different shards. <p class="p">Use
        for the expression partition strategy. </p>
</td>

     </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1224 ">Kinesis Producer Configuration</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1227 ">Additional Kinesis properties.<p class="p">When you add a
                                            configuration property, enter the exact property name
                                            and the value. The pipeline does not validate the
                                            property names or values.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">To write statistics to MapR Streams, on the <span class="ph uicontrol">Stats Aggregator - Write
                        to MapR Streams</span> tab, configure the following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_lcd_ng5_xw__table_z2j_315_mbb" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d56431e1378">MapR Streams Properties</th>

                                    <th class="entry" valign="top" width="70%" id="d56431e1381">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1378 ">Runtime Topic Resolution </td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1381 ">Do not use at this time. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1378 ">Topic</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1381 ">Topic to use. To avoid mixing statistics from different
                                        jobs, use a unique topic name for each job.<p class="p">You cannot
                                            define an expression that evaluates to the topic name.
                                        </p>
</td>

                                </tr>

                                <tr>
                                        <td class="entry" valign="top" width="30%" headers="d56431e1378 ">Partition Strategy <a class="xref" href="../Destinations/KProducer.html#concept_qpm_xp4_4r" title="The partition strategy determines how to write data to Kafka partitions. You can use a partition strategy to balance the work load or to write data semantically.">
                                                <img class="image" id="task_lcd_ng5_xw__d9895e8678" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                        <td class="entry" valign="top" width="70%" headers="d56431e1381 ">Strategy to use to write to partitions:<ul class="ul" id="task_lcd_ng5_xw__d9895e8682">
                                                <li class="li">Round Robin - Takes turns writing to different
                                                  partitions.</li>

                                                <li class="li">Random - Writes to partitions randomly.</li>

                                                <li class="li">Expression - Uses an expression to write data to
                                                  different partitions. Writes records to the
                                                  partitions specified by the results of the
                                                  expression.</li>

                                                <li class="li">Default - Uses an expression to extract a
                                                  partition key from the record. Writes records to
                                                  partitions based on a hash of the partition key.
                                                </li>

                                            </ul>
</td>

                                    </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1378 ">Partition Expression </td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1381 ">Expression to use when using the expression partition
                                            strategy.<p class="p">Define the expression to evaluate to the
                                            partition where you want statistics written. Partition
                                            numbers start with 0.</p>
<p class="p">Optionally, click
                                                <span class="ph uicontrol">Ctrl + Space Bar</span> for help
                                            with creating the expression.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d56431e1378 ">MapR Streams Configuration</td>

                                    <td class="entry" valign="top" width="70%" headers="d56431e1381 ">Additional configuration properties to use. Using <a class="xref" href="../Pipeline_Configuration/SimpleBulkEdit.html#concept_alb_b3y_cbb">simple or bulk edit mode</a>, click the
                                            <span class="ph uicontrol">Add</span> icon and define the MapR
                                        Streams property name and value.<p class="p">Use the property names
                                            and values as expected by MapR Streams. You can use MapR
                                            Streams properties and the set of Kafka properties
                                            supported by MapR Streams. </p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
</ol>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../DPM/DPM_title.html" title="Dataflow Performance Manager"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Dataflow Performance Manager</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

--><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>